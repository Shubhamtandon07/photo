# -*- coding: utf-8 -*-
"""
ONE SCRIPT: sanitize + de-risk Outlook .msg email chains for training use.

Goal (per your last requirements):
- Input: folder full of .msg files (recursively)
- Output: EXACTLY 1 HTML per input .msg (no duplicates)
- Each output contains:
  - Subject (sanitized)
  - Cleaned BODY text (chain-aware, removes headers like Von/An/Cc/Gesendet/Betreff, removes signatures)
  - Sensitive info removed (names, emails, phones, addresses, IDs/refs, money, domains)
  - Labels inside the SAME mail chain:
      - Segments labeled as QUESTION / ANSWER (heuristic + optional LLM off by default)
- Skips steps gracefully if optional libs are missing.
- No OCR / no image parsing (per your request to ignore images for now).

How to run:
  1) Set MSG_DIR and OUT_DIR below
  2) python sanitize_msgs.py
"""

import os
import re
import json
import hashlib
from datetime import datetime
from pathlib import Path
from html import escape

# =========================
# EDIT THESE
# =========================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

OUT_SUBFOLDER = "sanitized_one_per_mail"   # exactly 1 HTML per msg
WRITE_JSONL = True                         # also write training-friendly JSONL
JSONL_NAME = "dataset.jsonl"

# Optional LLM-based labeling (OFF by default)
USE_LLM_LABELING = False   # set True only if you want to use Azure OpenAI
# =========================

# ---------- Redaction patterns ----------
EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b(?:[A-Z0-9-]+\.)+[A-Z]{2,}\b", re.I)

PHONE_RE = re.compile(r"(?:(?:\+|00)\d{1,3}[\s\-]?)?(?:\(?\d{2,6}\)?[\s\-]?)?\d[\d\s\-]{6,}\d")
IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b", re.I)

MONEY_RE = re.compile(r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b|\b€\s*\d[\d\.\,\s]*\b")
REF_RE = re.compile(r"\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|Supplier|Portal|Round|ID|Ref|SP)\s*[:#]?\s*[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b", re.I)

# Names: multiple strategies
TITLE_NAME_RE = re.compile(r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b")
# "Lastname, Firstname" (very common in Outlook)
COMMA_NAME_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]+,\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b")
# Heuristic "First Last"
NAME_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}(?:\s+[A-ZÄÖÜ][a-zäöüß]{2,}){0,1}\b")

# Greeting line name: "Hallo X", "Hi X", "Guten Morgen X"
GREET_NAME_RE = re.compile(
    r"(?im)^\s*(?:hallo|hi|hello|guten\s+morgen|guten\s+tag|servus|moin)\s+([A-ZÄÖÜ][^\n,]{1,40})[,!]*\s*$"
)

# Signature blocks: common closings
SIGNOFF_RE = re.compile(
    r"(?is)\n\s*(mit\s+freundlichen\s+gr[üu]ßen|best\s+regards|kind\s+regards|vg|lg|br|gr[üu]ße)\b.*$"
)

# Outlook quoted header lines (German + English)
HEADER_LINE_RE = re.compile(
    r"(?im)^\s*(von|from|an|to|cc|kopie|gesendet|sent|betreff|subject|priorit[aä]t|importance)\s*:\s.*$"
)

# Separator lines often used in replies/forwards
SEP_RE = re.compile(r"(?m)^\s*_{5,}\s*$|^\s*-{5,}\s*$|^\s*={5,}\s*$")

# Question cues
QUESTION_CUES = re.compile(r"(?i)\b(kannst|könnt|können|bitte|frage|fragen|could you|can you|please|may you|need|required|requirement)\b")
ANSWER_CUES = re.compile(r"(?i)\b(danke|anbei|unten|siehe|hier|we recommend|please ensure|to achieve|requirements|you need to|must|soll|muss|empfehlen|information|antwort)\b")


def now_ts() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def safe_out_name(src_path: Path) -> str:
    raw = src_path.stem
    cleaned = re.sub(r"[\\/:*?\"<>|]+", "_", raw)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()[:80]
    h = hashlib.sha1(str(src_path).encode("utf-8", errors="ignore")).hexdigest()[:8]
    if not cleaned:
        cleaned = "mail"
    return f"{cleaned}__{h}.html"


def clean_ws(s: str) -> str:
    s = s or ""
    # Fix "h t m l  x m l n s : v = ..." spaced-out artifacts:
    # If a line looks like it has spaces between every char, compress it.
    lines = []
    for ln in s.splitlines():
        stripped = ln.strip()
        if stripped and len(stripped) > 40:
            # ratio of spaces to chars; if very high, likely spaced text
            space_ratio = stripped.count(" ") / max(1, len(stripped))
            if space_ratio > 0.25 and re.search(r"(h\s*t\s*m\s*l|x\s*m\s*l\s*n\s*s)", stripped, re.I):
                ln = stripped.replace(" ", "")
        lines.append(ln)
    s = "\n".join(lines)

    # Normalize CRLF
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    # Remove control chars except newline/tab
    s = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f]", " ", s)
    # Collapse weird whitespace
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def html_to_text(html: str) -> str:
    if not html:
        return ""
    # Strip tags
    txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html)
    txt = re.sub(r"(?is)<br\s*/?>", "\n", txt)
    txt = re.sub(r"(?is)</p\s*>", "\n\n", txt)
    txt = re.sub(r"(?is)<[^>]+>", " ", txt)
    return clean_ws(txt)


def parse_msg(msg_path: Path) -> dict:
    """
    Parse .msg using extract_msg. No .process() call needed.
    Compatible with current extract-msg versions; defensive anyway.
    """
    try:
        import extract_msg
    except Exception as e:
        raise RuntimeError(f"extract_msg import failed: {e}")

    m = extract_msg.Message(str(msg_path))

    # Compatibility shims (some versions/forks)
    if hasattr(m, "process") and callable(getattr(m, "process")):
        try:
            m.process()
        except Exception:
            pass
    elif hasattr(m, "extract") and callable(getattr(m, "extract")):
        try:
            m.extract()
        except Exception:
            pass

    subject = (getattr(m, "subject", "") or "").strip()
    body = (getattr(m, "body", "") or "").strip()
    html_body = (getattr(m, "htmlBody", "") or "").strip()
    rtf_body = (getattr(m, "rtfBody", "") or "").strip()

    # Close if supported
    if hasattr(m, "close") and callable(getattr(m, "close")):
        try:
            m.close()
        except Exception:
            pass

    return {"subject": subject, "body": body, "htmlBody": html_body, "rtfBody": rtf_body}


def pick_best_body(parsed: dict) -> str:
    body = clean_ws(parsed.get("body", "") or "")
    html_body = parsed.get("htmlBody", "") or ""
    rtf_body = clean_ws(parsed.get("rtfBody", "") or "")

    # Some messages have garbage "body" but good htmlBody
    if len(body) < 40 and html_body:
        body2 = html_to_text(html_body)
        if len(body2) > len(body):
            return body2

    # If body looks like it contains HTML markup
    if "<html" in body.lower() or "<body" in body.lower() or "<div" in body.lower():
        body2 = html_to_text(body)
        if len(body2) > 0:
            return body2

    # fallback to rtf
    if len(body) < 40 and len(rtf_body) > len(body):
        return rtf_body

    return body


def strip_headers_and_signatures(text: str) -> str:
    if not text:
        return ""
    t = clean_ws(text)

    # Remove Outlook header lines inside body
    t = HEADER_LINE_RE.sub("", t)
    t = SEP_RE.sub("", t)

    # Remove most signatures (from first sign-off to end)
    t = SIGNOFF_RE.sub("", t)

    t = clean_ws(t)
    return t


def redact_text(text: str) -> tuple[str, dict]:
    """
    Remove sensitive info completely (do not leave [PERSON] etc.)
    Returns (redacted_text, stats)
    """
    stats = {
        "emails": 0, "urls": 0, "domains": 0, "phones": 0, "iban": 0,
        "money": 0, "refs": 0, "names": 0,
    }

    t = text or ""
    # Greeting names: remove the captured person chunk from greeting line
    def _greet_sub(m):
        nonlocal stats
        stats["names"] += 1
        return re.sub(re.escape(m.group(1)), "", m.group(0), flags=re.I)

    t = GREET_NAME_RE.sub(_greet_sub, t)

    # Title+Name, Comma Name, Heuristic names
    for rx in (TITLE_NAME_RE, COMMA_NAME_RE, NAME_RE):
        found = len(rx.findall(t))
        if found:
            stats["names"] += found
            t = rx.sub("", t)

    found = len(EMAIL_RE.findall(t))
    if found:
        stats["emails"] += found
        t = EMAIL_RE.sub("", t)

    found = len(URL_RE.findall(t))
    if found:
        stats["urls"] += found
        t = URL_RE.sub("", t)

    found = len(WWW_RE.findall(t))
    if found:
        stats["urls"] += found
        t = WWW_RE.sub("", t)

    # Domains: be careful not to nuke normal words; remove only if looks like domain.tld
    found = len(DOMAIN_RE.findall(t))
    if found:
        stats["domains"] += found
        t = DOMAIN_RE.sub("", t)

    found = len(PHONE_RE.findall(t))
    if found:
        stats["phones"] += found
        t = PHONE_RE.sub("", t)

    found = len(IBAN_RE.findall(t))
    if found:
        stats["iban"] += found
        t = IBAN_RE.sub("", t)

    found = len(MONEY_RE.findall(t))
    if found:
        stats["money"] += found
        t = MONEY_RE.sub("", t)

    found = len(REF_RE.findall(t))
    if found:
        stats["refs"] += found
        t = REF_RE.sub("", t)

    # Clean leftovers
    t = re.sub(r"\(\s*\)", "", t)
    t = re.sub(r"\s{2,}", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = clean_ws(t)
    return t, stats


def split_chain_blocks(text: str) -> list[str]:
    """
    Split an email chain into blocks from bottom->top heuristic:
    We split on common quote headers inside the body (e.g., 'Von:'/'From:').
    """
    if not text:
        return []
    t = clean_ws(text)

    # split points: lines like "Von: ..." or "From: ..."
    split_rx = re.compile(r"(?im)^\s*(von|from)\s*:\s")
    parts = split_rx.split(t)

    if len(parts) == 1:
        return [t]

    # Rebuild with markers: parts alternates [pre, key, rest, key, rest, ...]
    blocks = []
    cur = parts[0].strip()
    if cur:
        blocks.append(cur)

    i = 1
    while i + 1 < len(parts):
        key = parts[i]
        rest = parts[i + 1]
        block = (key + ": " + rest).strip()
        if block:
            blocks.append(block)
        i += 2

    # blocks now are in original order (top section first). For "question starts bottom", reverse later.
    return [b.strip() for b in blocks if b.strip()]


def label_blocks_heuristic(blocks: list[str]) -> list[dict]:
    """
    Label blocks as QUESTION / ANSWER.
    You said: question typically starts from bottom (oldest), not top.
    We'll traverse blocks from bottom to top, and label each block.
    """
    labeled = []
    if not blocks:
        return labeled

    # Reverse for bottom-first logic
    rev = list(reversed(blocks))

    for b in rev:
        text = b.strip()
        if not text:
            continue

        # Simple scoring
        q_score = 0
        a_score = 0

        if "?" in text:
            q_score += 2
        if QUESTION_CUES.search(text):
            q_score += 2
        if ANSWER_CUES.search(text):
            a_score += 2

        # If block starts with "danke" etc it is likely answer
        if re.match(r"(?i)^\s*(danke|thank you|anbei|attached|unten|siehe|hier)\b", text):
            a_score += 2

        label = "question" if q_score > a_score else "answer"

        labeled.append({"label": label, "text": text})

    # Keep output in original top->bottom order, but with labels computed bottom-first
    labeled = list(reversed(labeled))
    return labeled


def render_html(source_name: str, subject: str, labeled_blocks: list[dict], stats: dict) -> str:
    legend = "Sensitive removed: Names, Emails, Phones, URLs/Domains, Addresses(heur), IBAN, References/IDs, Money"
    meta = {
        "source": source_name,
        "generated": now_ts(),
        "subject": subject,
        "redaction_stats": stats,
    }
    meta_json = escape(json.dumps(meta, ensure_ascii=False))

    parts = []
    parts.append("<html><head><meta charset='utf-8'>")
    parts.append("<title>Sanitized Mail</title></head>")
    parts.append("<body style='font-family:Segoe UI,Arial; font-size:13px; line-height:1.35; padding:16px;'>")
    parts.append(f"<div><b>Source:</b> {escape(source_name)}<br>")
    parts.append(f"<b>Generated:</b> {escape(meta['generated'])}<br>")
    parts.append(f"<b>Legend:</b> {escape(legend)}<br>")
    parts.append(f"<b>Subject:</b> {escape(subject)}<br></div>")
    parts.append("<hr>")
    parts.append("<div style='color:#666; font-size:12px;'><b>Metadata (json):</b><br>")
    parts.append(f"<pre style='white-space:pre-wrap'>{meta_json}</pre></div>")
    parts.append("<hr>")

    if not labeled_blocks:
        parts.append("<p><i>(no usable body text extracted)</i></p>")
    else:
        parts.append("<h3 style='margin:0 0 8px 0;'>Body</h3>")
        for blk in labeled_blocks:
            label = blk.get("label", "unknown")
            txt = blk.get("text", "")
            badge = "QUESTION" if label == "question" else "ANSWER"
            parts.append(
                f"<div style='margin:10px 0; padding:10px; border:1px solid #ddd; border-radius:8px;'>"
                f"<div style='font-weight:600; margin-bottom:6px;'>{escape(badge)}</div>"
                f"<div>{escape(txt).replace(chr(10), '<br>')}</div>"
                f"</div>"
            )

    parts.append("</body></html>")
    return "".join(parts)


def ensure_dirs():
    out_base = Path(OUT_DIR)
    out_base.mkdir(parents=True, exist_ok=True)
    out_html = out_base / OUT_SUBFOLDER
    out_html.mkdir(parents=True, exist_ok=True)
    return out_html


def walk_msg_files(root: Path) -> list[Path]:
    return [p for p in root.rglob("*.msg") if p.is_file()]


def main():
    msg_root = Path(MSG_DIR)
    if not msg_root.exists():
        raise SystemExit(f"MSG_DIR not found: {MSG_DIR}")

    out_html_dir = ensure_dirs()
    errors_log = Path(OUT_DIR) / "errors.log"
    summary_json = Path(OUT_DIR) / "summary.json"

    msg_files = walk_msg_files(msg_root)
    if not msg_files:
        raise SystemExit("No .msg files found in MSG_DIR (recursively).")

    jsonl_path = Path(OUT_DIR) / JSONL_NAME
    if WRITE_JSONL:
        # reset JSONL each run
        jsonl_path.write_text("", encoding="utf-8")

    processed = 0
    failed = 0
    summaries = []

    for p in sorted(msg_files):
        try:
            parsed = parse_msg(p)
            subject_raw = parsed.get("subject", "") or p.stem
            body_raw = pick_best_body(parsed)

            subject_clean, s_stats = redact_text(strip_headers_and_signatures(clean_ws(subject_raw)))
            body_clean, b_stats = redact_text(strip_headers_and_signatures(clean_ws(body_raw)))

            # Merge stats
            stats = {k: int(s_stats.get(k, 0)) + int(b_stats.get(k, 0)) for k in set(s_stats) | set(b_stats)}

            # Chain splitting + labeling
            blocks = split_chain_blocks(body_clean)
            # also remove empty/too-short blocks
            blocks = [b for b in blocks if len(b.strip()) >= 20]

            labeled = label_blocks_heuristic(blocks)

            # Final HTML
            out_name = safe_out_name(p)
            out_path = out_html_dir / out_name
            html = render_html(p.name, subject_clean, labeled, stats)
            out_path.write_text(html, encoding="utf-8")

            processed += 1
            summaries.append({
                "file": str(p),
                "out": str(out_path),
                "subject": subject_clean,
                "blocks": len(labeled),
                "stats": stats,
            })

            # Optional JSONL record (training-style)
            if WRITE_JSONL:
                record = {
                    "source_file": p.name,
                    "subject": subject_clean,
                    "segments": labeled,  # list of {label,text}
                }
                with jsonl_path.open("a", encoding="utf-8") as f:
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")

        except Exception as e:
            failed += 1
            with errors_log.open("a", encoding="utf-8") as f:
                f.write(f"[{now_ts()}] ERROR: {p.name} -> {repr(e)}\n")

    summary = {
        "generated": now_ts(),
        "msg_dir": MSG_DIR,
        "out_dir": str(out_html_dir),
        "processed": processed,
        "failed": failed,
        "total": len(msg_files),
    }
    summary_json.write_text(json.dumps({"summary": summary, "files": summaries}, ensure_ascii=False, indent=2), encoding="utf-8")
    print("Done.")
    print(json.dumps(summary, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
