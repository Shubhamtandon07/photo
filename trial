# -*- coding: utf-8 -*-
"""
ONE OUTPUT PER INPUT MAIL (sanitized + labeled)

Reads (recursive):
- .msg (if extract_msg is installed)
- .html/.htm (mail exports)

Writes:
- exactly ONE .html per input mail to OUT_DIR/sanitized_one_per_mail/

Output contains ONLY:
Label: question|answer
Subject: <sanitized>
Body:
<sanitized body + (optional) attachment text, also sanitized>

No "From/Von/To/An/CC", no names, no emails, no phones, no addresses, no IDs, no money, etc.
No [PERSON] tokens (we delete).

If optional libraries are missing (pdf/docx/xlsx/ocr), that attachment type is skipped silently.
"""

import re
import os
import json
import time
import hashlib
import tempfile
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict, Optional
from html import escape

# =========================
# EDIT THESE
# =========================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

# Optional curated company/supplier list (one per line). If empty -> ignored.
COMPANY_LIST_TXT = r""  # e.g. r"C:\...\company_list.txt"

# OCR (images in attachments)
ENABLE_OCR_IMAGES = True
OCR_LANG = "deu+eng"
OCR_MAX_IMAGES_PER_MAIL = 4
# =========================

# ---------------------------------------------------------
# Output structure
# ---------------------------------------------------------
OUT_DIR = Path(OUT_DIR)
OUT_SUB = OUT_DIR / "sanitized_one_per_mail"
OUT_SUB.mkdir(parents=True, exist_ok=True)

RUN_LOG = OUT_DIR / "run.log"
ERROR_LOG = OUT_DIR / "errors.log"
STATS_JSON = OUT_DIR / "stats.json"

TMP_DIR = Path(tempfile.gettempdir()) / "mail_sanitize_tmp2"
TMP_DIR.mkdir(parents=True, exist_ok=True)

def log(msg: str):
    line = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}"
    print(line)
    with RUN_LOG.open("a", encoding="utf-8") as f:
        f.write(line + "\n")

def err(msg: str):
    line = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ERROR: {msg}"
    print(line)
    with ERROR_LOG.open("a", encoding="utf-8") as f:
        f.write(line + "\n")

# ---------------------------------------------------------
# Optional libs (graceful)
# ---------------------------------------------------------
HAS_EXTRACT_MSG = False
HAS_PYPDF = False
HAS_DOCX = False
HAS_OPENPYXL = False
HAS_PIL = False
HAS_TESS = False

try:
    import extract_msg  # type: ignore
    HAS_EXTRACT_MSG = True
except Exception:
    HAS_EXTRACT_MSG = False

try:
    from pypdf import PdfReader  # type: ignore
    HAS_PYPDF = True
except Exception:
    HAS_PYPDF = False

try:
    from docx import Document as DocxDocument  # type: ignore
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

try:
    import openpyxl  # type: ignore
    HAS_OPENPYXL = True
except Exception:
    HAS_OPENPYXL = False

try:
    from PIL import Image  # type: ignore
    HAS_PIL = True
except Exception:
    HAS_PIL = False

try:
    import pytesseract  # type: ignore
    HAS_TESS = True
except Exception:
    HAS_TESS = False

# ---------------------------------------------------------
# Text cleanup
# ---------------------------------------------------------
def clean_ws_keep_newlines(s: str) -> str:
    s = (s or "").replace("\r\n", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def de_space_single_chars(text: str) -> str:
    """
    Fix broken spaced tokens like: '< h t m l  x m l n s : v = ... >'
    Only merges runs of single-letter tokens (typical corruption).
    """
    if not text:
        return ""
    def repl(m: re.Match) -> str:
        return m.group(0).replace(" ", "")
    return re.sub(r"(?:\b\w\b\s+){3,}\b\w\b", repl, text)

def strip_html_to_text(html: str) -> str:
    html = html or ""
    html = html.replace("\r\n", "\n")
    html = re.sub(r"(?is)<(script|style)[^>]*>.*?</\1>", " ", html)
    html = re.sub(r"(?i)<\s*br\s*/?\s*>", "\n", html)
    html = re.sub(r"(?i)</\s*p\s*>", "\n\n", html)
    html = re.sub(r"(?i)</\s*div\s*>", "\n", html)
    html = re.sub(r"(?i)</\s*li\s*>", "\n", html)
    txt = re.sub(r"(?s)<[^>]+>", " ", html)
    txt = txt.replace("&nbsp;", " ").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
    txt = de_space_single_chars(txt)
    return clean_ws_keep_newlines(txt)

# ---------------------------------------------------------
# Sensitive patterns (delete)
# ---------------------------------------------------------
EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b(?:[A-Z0-9-]+\.)+(?:[A-Z]{2,})\b", re.I)

PHONE_RE = re.compile(
    r"(?<!\w)(?:\+?\d{1,3}[\s\-\/]?)?(?:\(?\d{2,5}\)?[\s\-\/]?)?\d{3,4}[\s\-\/]?\d{3,4}(?!\w)"
)

ADDRESS_RE = re.compile(
    r"\b([A-ZÄÖÜ][a-zäöüß]+(?:\s[A-ZÄÖÜ][a-zäöüß]+){0,3})\s"
    r"(Straße|Strasse|Str\.|Weg|Allee|Platz|Ring|Gasse|Damm|Ufer)\s"
    r"\d{1,5}[a-zA-Z]?\b"
)

IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b", re.I)

MONEY_RE = re.compile(
    r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b|\b€\s*\d[\d\.\,\s]*\b"
)

ID_REF_RE = re.compile(
    r"\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|Supplier|Portal|Round|ID|Ref|SP|V\d{2,}|SP\d+)\s*[:#]?\s*[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b",
    re.I,
)

TITLE_NAME_RE = re.compile(
    r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b"
)

NAME_COMMA_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,},\s*[A-ZÄÖÜ][a-zäöüß]{2,}\b")
NAME_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}\b")

# greeting line name extraction (we delete name token)
GREETING_NAME_LINE_RE = re.compile(
    r"(?im)^\s*(hallo|hi|hello|guten\s+morgen|guten\s+tag|guten\s+abend)\s+([A-ZÄÖÜ][a-zäöüß]{1,})(\s*[,\!])?\s*$"
)

# Header blocks in quoted threads
HEADER_LINE_RE = re.compile(
    r"(?im)^\s*(from|von|sent|gesendet|to|an|cc|betreff|subject|priorität|priority)\s*:\s*.*$"
)

# Quote cut cues (to keep only top-most new message)
QUOTE_CUT_PATTERNS = [
    r"(?im)^\s*from\s*:\s*.*$",
    r"(?im)^\s*von\s*:\s*.*$",
    r"(?im)^\s*sent\s*:\s*.*$",
    r"(?im)^\s*gesendet\s*:\s*.*$",
    r"(?im)^\s*to\s*:\s*.*$",
    r"(?im)^\s*an\s*:\s*.*$",
    r"(?im)^\s*cc\s*:\s*.*$",
    r"(?im)^\s*betreff\s*:\s*.*$",
    r"(?im)^\s*subject\s*:\s*.*$",
    r"(?im)^-+\s*original message\s*-+\s*$",
    r"(?im)^_{5,}\s*$",
]

SIGNOFF_LINE_RE = re.compile(
    r"(?im)^\s*(vg|lg|br|mfg|mit\s+freundlichen\s+grüßen|best\s+regards|kind\s+regards|with\s+best\s+regards)\b.*$"
)

# ---------------------------------------------------------
# Company list (optional)
# ---------------------------------------------------------
def load_company_terms() -> List[str]:
    if not COMPANY_LIST_TXT:
        return []
    p = Path(COMPANY_LIST_TXT)
    if not p.exists():
        return []
    try:
        lines = [ln.strip() for ln in p.read_text(encoding="utf-8", errors="ignore").splitlines()]
        return [ln for ln in lines if ln and not ln.startswith("#")]
    except Exception:
        return []

def build_company_re(terms: List[str]) -> Optional[re.Pattern]:
    terms = [t.strip() for t in terms if t.strip()]
    if not terms:
        return None
    terms.sort(key=len, reverse=True)
    pat = r"\b(" + "|".join(re.escape(t) for t in terms[:2000]) + r")\b"
    try:
        return re.compile(pat, re.I)
    except Exception:
        return None

COMPANY_RE: Optional[re.Pattern] = None

# ---------------------------------------------------------
# Keep only important top body
# ---------------------------------------------------------
def cut_quoted_history(text: str) -> str:
    if not text:
        return ""
    t = text.replace("\r\n", "\n")
    earliest = None
    for pat in QUOTE_CUT_PATTERNS:
        m = re.search(pat, t)
        if m:
            earliest = m.start() if earliest is None else min(earliest, m.start())
    if earliest is not None and earliest > 0:
        t = t[:earliest]
    return t.strip()

def drop_header_lines(text: str) -> str:
    if not text:
        return ""
    lines = text.replace("\r\n", "\n").split("\n")
    kept = []
    for ln in lines:
        if HEADER_LINE_RE.search(ln):
            continue
        if "mailto:" in ln.lower() or "<mailto:" in ln.lower():
            continue
        kept.append(ln)
    return "\n".join(kept).strip()

def extract_important_body(body: str) -> str:
    if not body:
        return ""
    t = de_space_single_chars(body)
    t = clean_ws_keep_newlines(t)
    t = cut_quoted_history(t)
    t = drop_header_lines(t)

    # reduce signature contact block (keep signoff line, drop following contact-ish lines)
    lines = t.split("\n")
    out = []
    drop_budget = 0
    for ln in lines:
        if SIGNOFF_LINE_RE.match(ln):
            out.append(ln.strip())
            drop_budget = 5
            continue
        if drop_budget > 0:
            if EMAIL_RE.search(ln) or PHONE_RE.search(ln) or ADDRESS_RE.search(ln) or IBAN_RE.search(ln):
                drop_budget -= 1
                continue
            if NAME_RE.search(ln) or NAME_COMMA_RE.search(ln) or TITLE_NAME_RE.search(ln):
                drop_budget -= 1
                continue
            if "tel" in ln.lower() or "telefon" in ln.lower() or "mobil" in ln.lower():
                drop_budget -= 1
                continue
            drop_budget = 0
        out.append(ln)
    return clean_ws_keep_newlines("\n".join(out))

# ---------------------------------------------------------
# Delete sensitive info completely (no tokens)
# ---------------------------------------------------------
def delete_sensitive(text: str) -> str:
    if not text:
        return ""
    t = de_space_single_chars(text)

    # remove mailto fragments and residual tags
    t = re.sub(r"(?i)<mailto:[^>]+>", " ", t)
    t = re.sub(r"(?i)mailto\s*:\s*\S+", " ", t)
    t = re.sub(r"<[^>]+>", " ", t)

    # delete header lines again (sometimes inside body)
    t = drop_header_lines(t)

    # delete greeting names (only the name token)
    t = GREETING_NAME_LINE_RE.sub(lambda m: f"{m.group(1)}{m.group(3) or ''}", t)

    # delete patterns
    t = EMAIL_RE.sub(" ", t)
    t = URL_RE.sub(" ", t)
    t = WWW_RE.sub(" ", t)
    t = DOMAIN_RE.sub(" ", t)
    t = PHONE_RE.sub(" ", t)
    t = ADDRESS_RE.sub(" ", t)
    t = IBAN_RE.sub(" ", t)
    t = ID_REF_RE.sub(" ", t)
    t = MONEY_RE.sub(" ", t)
    t = TITLE_NAME_RE.sub(" ", t)
    t = NAME_COMMA_RE.sub(" ", t)
    t = NAME_RE.sub(" ", t)

    if COMPANY_RE is not None:
        t = COMPANY_RE.sub(" ", t)

    # normalize
    t = t.replace("\r\n", "\n")
    t = re.sub(r"[ \t]{2,}", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"\s+([,.;:])", r"\1", t)
    t = re.sub(r"([,.;:]){2,}", r"\1", t)
    return t.strip()

# ---------------------------------------------------------
# Q/A label heuristic (simple but effective)
# ---------------------------------------------------------
REQUEST_CUES = re.compile(
    r"(?i)\b(bitte|kann\s+jemand|kann\s+einer|könnt\s+ihr|could\s+you|can\s+you|please|prüfen|check|draufschauen|feedback|rückmeldung|anmerkungen|freigabe|review)\b"
)
ANSWER_CUES = re.compile(
    r"(?i)\b(danke|unten\s+meine|hier\s+meine|anbei|following|wir\s+plädieren|wir\s+würden|unsere\s+meinung|meine\s+anmerkungen|ich\s+sehe|aus\s+unserer\s+sicht)\b"
)

def label_question_answer(subject: str, body: str) -> str:
    s = (subject or "").strip().lower()
    b = (body or "").strip()

    # typical reply prefixes => answer
    if s.startswith(("aw:", "re:", "fw:", "wg:", "sv:", "fwd:")):
        return "answer"

    # explicit request language => question
    if REQUEST_CUES.search(b):
        return "question"

    # explicit response language => answer
    if ANSWER_CUES.search(b):
        return "answer"

    # fallback: short mails without request cues => answer, otherwise question
    return "answer" if len(b) < 250 else "question"

# ---------------------------------------------------------
# Attachment text extraction (best-effort)
# ---------------------------------------------------------
def read_pdf(p: Path) -> str:
    if not HAS_PYPDF:
        return ""
    try:
        out = []
        r = PdfReader(str(p))
        for pg in r.pages:
            try:
                out.append(pg.extract_text() or "")
            except Exception:
                pass
        return "\n".join(out)
    except Exception:
        return ""

def read_docx(p: Path) -> str:
    if not HAS_DOCX:
        return ""
    try:
        doc = DocxDocument(str(p))
        paras = "\n".join(par.text for par in doc.paragraphs if par.text.strip())
        return clean_ws_keep_newlines(paras)
    except Exception:
        return ""

def read_xlsx(p: Path) -> str:
    if not HAS_OPENPYXL:
        return ""
    try:
        wb = openpyxl.load_workbook(str(p), data_only=True, read_only=True)
        out = []
        for ws in wb.worksheets:
            out.append(f"Sheet: {ws.title}")
            max_rows = min(ws.max_row or 0, 150)
            max_cols = min(ws.max_column or 0, 20)
            for r in range(1, max_rows + 1):
                row_vals = []
                for c in range(1, max_cols + 1):
                    v = ws.cell(row=r, column=c).value
                    row_vals.append("" if v is None else str(v))
                if any(x.strip() for x in row_vals):
                    out.append(" | ".join(x.strip() for x in row_vals))
        return "\n".join(out)
    except Exception:
        return ""

def ocr_image(p: Path) -> str:
    if not ENABLE_OCR_IMAGES:
        return ""
    if not (HAS_PIL and HAS_TESS):
        return ""
    try:
        img = Image.open(str(p))
        txt = pytesseract.image_to_string(img, lang=OCR_LANG)
        return txt or ""
    except Exception:
        return ""

def extract_text_from_attachment(p: Path) -> str:
    ext = p.suffix.lower()
    try:
        if ext == ".txt":
            return p.read_text(encoding="utf-8", errors="ignore")
        if ext in (".html", ".htm"):
            return strip_html_to_text(p.read_text(encoding="utf-8", errors="ignore"))
        if ext == ".pdf":
            return read_pdf(p)
        if ext == ".docx":
            return read_docx(p)
        if ext == ".xlsx":
            return read_xlsx(p)
        if ext in (".png", ".jpg", ".jpeg"):
            return ocr_image(p)
    except Exception:
        return ""
    return ""

# ---------------------------------------------------------
# MSG parsing (no msg.save() explosion; we extract only what we need)
# ---------------------------------------------------------
def parse_msg(p: Path) -> Tuple[str, str, List[Tuple[str, str]]]:
    """
    returns: subject, body_text, attachments_text_list[(name,text)]
    """
    if not HAS_EXTRACT_MSG:
        return (p.stem, "", [])

    try:
        m = extract_msg.Message(str(p))
        m.process()

        subject = (getattr(m, "subject", "") or "").strip()

        body = ""
        try:
            html_body = getattr(m, "htmlBody", None)
            if html_body:
                body = strip_html_to_text(html_body)
        except Exception:
            body = ""

        if not body:
            try:
                body = getattr(m, "body", "") or ""
                body = de_space_single_chars(body)
                body = clean_ws_keep_newlines(body)
            except Exception:
                body = ""

        attachments: List[Tuple[str, str]] = []
        img_count = 0

        # extract_msg attachments: best-effort save to temp one-by-one
        try:
            att_list = getattr(m, "attachments", []) or []
            for att in att_list:
                try:
                    fname = getattr(att, "longFilename", None) or getattr(att, "shortFilename", None) or "attachment"
                    ext = Path(fname).suffix.lower()

                    # OCR cap
                    if ext in (".png", ".jpg", ".jpeg"):
                        img_count += 1
                        if img_count > OCR_MAX_IMAGES_PER_MAIL:
                            continue

                    tmp_path = TMP_DIR / f"{hashlib.sha1((str(p)+fname).encode('utf-8')).hexdigest()[:10]}__{re.sub(r'[^A-Za-z0-9._-]+','_',fname)}"
                    try:
                        att.save(customPath=str(tmp_path.parent), customFilename=tmp_path.name)
                    except Exception:
                        continue

                    if tmp_path.exists():
                        txt = extract_text_from_attachment(tmp_path)
                        txt = clean_ws_keep_newlines(txt)
                        if txt:
                            attachments.append((fname, txt))

                        # cleanup temp file
                        try:
                            tmp_path.unlink()
                        except Exception:
                            pass

                except Exception:
                    continue
        except Exception:
            pass

        return (subject or p.stem, body or "", attachments)

    except Exception as e:
        err(f"MSG parse failed: {p.name} -> {e}")
        return (p.stem, "", [])

# ---------------------------------------------------------
# HTML mail export parsing
# ---------------------------------------------------------
def parse_html_mail(p: Path) -> Tuple[str, str, List[Tuple[str, str]]]:
    raw = ""
    try:
        raw = p.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        raw = ""
    body = strip_html_to_text(raw)
    # subject is unknown; use filename stem
    return (p.stem, body, [])

# ---------------------------------------------------------
# One output per input
# ---------------------------------------------------------
def safe_out_name(src: Path) -> str:
    # stable: file stem + hash of full path
    h = hashlib.sha1(str(src).encode("utf-8", errors="ignore")).hexdigest()[:8]
    base = re.sub(r"[\\/:*?\"<>|]+", "_", src.stem)
    base = re.sub(r"\s+", " ", base).strip()
    base = base[:90] if len(base) > 90 else base
    if not base:
        base = "mail"
    return f"{base}__{h}.html"

def build_output_html(label: str, subject: str, body: str) -> str:
    # simple readable HTML, no extra metadata, no legends
    subj_html = escape(subject)
    body_html = escape(body).replace("\n", "<br>\n")
    return f"""<html>
<head><meta charset="utf-8"><title>Sanitized Mail</title></head>
<body style="font-family:Segoe UI,Arial;font-size:13px;line-height:1.35;padding:16px;">
<div><b>Label:</b> {escape(label)}</div>
<div style="margin-top:6px;"><b>Subject:</b> {subj_html}</div>
<hr>
<div><b>Body:</b></div>
<div style="margin-top:6px;white-space:normal;">{body_html}</div>
</body>
</html>"""

def process_one(src: Path) -> bool:
    ext = src.suffix.lower()

    if ext == ".msg":
        subject, body, att = parse_msg(src)
    elif ext in (".html", ".htm"):
        subject, body, att = parse_html_mail(src)
    else:
        return False

    important = extract_important_body(body)

    # merge attachments into same mail (still one output)
    if att:
        important += "\n\nATTACHMENTS:\n"
        for name, txt in att:
            important += f"\n[{name}]\n{txt}\n"

    # sanitize subject+body
    subject_s = delete_sensitive(subject)
    body_s = delete_sensitive(important)

    # if body empty, keep at least sanitized subject
    if not body_s.strip():
        body_s = "(no usable body text extracted)"

    label = label_question_answer(subject, important)

    out_path = OUT_SUB / safe_out_name(src)
    out_path.write_text(build_output_html(label, subject_s, body_s), encoding="utf-8")
    return True

# ---------------------------------------------------------
# File discovery (prevents loops by skipping OUT_DIR subtree)
# ---------------------------------------------------------
def is_under(path: Path, parent: Path) -> bool:
    try:
        path.resolve().relative_to(parent.resolve())
        return True
    except Exception:
        return False

def main():
    # reset logs
    RUN_LOG.write_text("", encoding="utf-8")
    if ERROR_LOG.exists():
        try:
            ERROR_LOG.unlink()
        except Exception:
            pass

    global COMPANY_RE
    COMPANY_RE = build_company_re(load_company_terms())

    log(f"MSG_DIR={MSG_DIR}")
    log(f"OUT_DIR={OUT_DIR}")
    log(f"Writing ONE output per mail to: {OUT_SUB}")
    log(f"extract_msg={HAS_EXTRACT_MSG}, pypdf={HAS_PYPDF}, docx={HAS_DOCX}, openpyxl={HAS_OPENPYXL}, PIL={HAS_PIL}, pytesseract={HAS_TESS}")

    base = Path(MSG_DIR)
    if not base.exists():
        raise SystemExit(f"MSG_DIR not found: {MSG_DIR}")

    # discover sources, but NEVER scan inside OUT_DIR (prevents 3-4 outputs issue)
    sources: List[Path] = []
    for p in base.rglob("*"):
        if not p.is_file():
            continue
        if p.name.startswith("~$"):
            continue
        if is_under(p, OUT_DIR):
            continue
        ext = p.suffix.lower()
        if ext in (".msg", ".html", ".htm"):
            sources.append(p)

    if not sources:
        raise SystemExit("No .msg/.html/.htm found in MSG_DIR (recursive).")

    t0 = time.time()
    stats = {"found": len(sources), "processed": 0, "errors": 0, "seconds": 0.0}

    for src in sorted(sources):
        try:
            ok = process_one(src)
            if ok:
                stats["processed"] += 1
        except Exception as e:
            stats["errors"] += 1
            err(f"{src.name}: {e}")

    stats["seconds"] = round(time.time() - t0, 2)
    STATS_JSON.write_text(json.dumps(stats, indent=2), encoding="utf-8")
    log(f"Done. found={stats['found']} processed={stats['processed']} errors={stats['errors']} time={stats['seconds']}s")
    log(f"Stats written: {STATS_JSON}")

if __name__ == "__main__":
    main()
