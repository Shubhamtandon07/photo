# -*- coding: utf-8 -*-
"""
One output HTML per input file (MSG or HTML export).
- Outlook COM reads .msg (Python 3.14-friendly)
- Optional attachment text extraction (pdf/docx/xlsx/txt)
- Strong per-file reporting so nothing is silently ignored
"""

from __future__ import annotations

import os
import re
import json
import csv
import time
import hashlib
import tempfile
from datetime import datetime
from pathlib import Path
from html import escape
from typing import List, Dict, Tuple, Optional

# =========================
# EDIT THESE PATHS
# =========================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_BASE_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

RUN_TAG = "trial"
RECURSIVE = True
MAX_FILES: Optional[int] = None

# If some mails are already exported as HTML files, enable this:
INCLUDE_HTML_FILES = True

# Attachments
PROCESS_ATTACHMENTS = True
MAX_ATTACHMENTS_PER_MAIL = 12
MAX_ATTACHMENT_CHARS_TOTAL = 20000

# Limits
MAX_TURN_CHARS = 14000
MAX_TOTAL_BODY_CHARS = 80000
STARTUP_DELAY_SEC = 0

# =========================
# OPTIONAL LIBS (AUTO-SKIP)
# =========================
try:
    from bs4 import BeautifulSoup  # type: ignore
except Exception:
    BeautifulSoup = None

# PDF readers (try pdfplumber first if present)
try:
    import pdfplumber  # type: ignore
except Exception:
    pdfplumber = None

try:
    from pypdf import PdfReader  # type: ignore
except Exception:
    PdfReader = None

try:
    from docx import Document as DocxDocument  # type: ignore
except Exception:
    DocxDocument = None

try:
    import openpyxl  # type: ignore
except Exception:
    openpyxl = None

# =========================
# OUTLOOK COM (REQUIRED for .msg)
# =========================
try:
    import win32com.client as win32  # type: ignore
except Exception as e:
    raise SystemExit(
        "pywin32/win32com is required for .msg parsing via Outlook COM.\n"
        "Install: py -m pip install pywin32\n"
        f"Import error: {e}"
    )

# =========================
# OUTPUT FOLDER (UNIQUE PER RUN)
# =========================
def make_run_out_dir(base_dir: str, tag: str) -> Path:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_tag = re.sub(r"[^A-Za-z0-9._-]+", "_", tag.strip()) or "run"
    out = Path(base_dir) / f"run_{ts}_{safe_tag}"
    out.mkdir(parents=True, exist_ok=True)
    (out / "_logs").mkdir(parents=True, exist_ok=True)
    (out / "sanitized_one_per_mail").mkdir(parents=True, exist_ok=True)
    return out

OUT_DIR = make_run_out_dir(OUT_BASE_DIR, RUN_TAG)
OUT_SAN = OUT_DIR / "sanitized_one_per_mail"
OUT_LOG = OUT_DIR / "_logs"
ERR_LOG = OUT_LOG / "errors.log"
STATS_JSON = OUT_LOG / "stats.json"
REPORT_CSV = OUT_LOG / "run_report.csv"

ATTACH_TMP = Path(tempfile.gettempdir()) / "msg_sanitize_attach_tmp"
ATTACH_TMP.mkdir(parents=True, exist_ok=True)

# =========================
# LOGGING
# =========================
def log_error(msg: str) -> None:
    stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with ERR_LOG.open("a", encoding="utf-8", errors="ignore") as f:
        f.write(f"[{stamp}] ERROR: {msg}\n")

def clean_ws(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def safe_filename(stem: str, ext: str = ".html") -> str:
    raw = (stem or "mail").strip()
    raw = re.sub(r"[\\/:*?\"<>|]+", "_", raw)
    raw = re.sub(r"\s+", " ", raw).strip()
    raw = raw[:90] if len(raw) > 90 else raw
    h = hashlib.sha1(stem.encode("utf-8", errors="ignore")).hexdigest()[:8]
    return f"{raw}__{h}{ext}"

# =========================
# HTML -> TEXT
# =========================
def normalize_spaced_letters(text: str) -> str:
    if not text:
        return ""
    lines = text.splitlines()
    out = []
    for ln in lines:
        toks = ln.strip().split()
        if not toks:
            out.append("")
            continue
        singleish = sum(1 for t in toks if len(t) == 1)
        if len(toks) >= 8 and singleish / max(1, len(toks)) >= 0.6:
            out.append(ln.replace(" ", ""))
        else:
            out.append(ln)
    return "\n".join(out)

def html_to_text(html: str) -> str:
    html = html or ""
    if not html.strip():
        return ""
    if BeautifulSoup is None:
        txt = re.sub(r"<br\s*/?>", "\n", html, flags=re.I)
        txt = re.sub(r"</p\s*>", "\n\n", txt, flags=re.I)
        txt = re.sub(r"<[^>]+>", " ", txt)
        return normalize_spaced_letters(txt)
    soup = BeautifulSoup(html, "html.parser")
    for br in soup.find_all("br"):
        br.replace_with("\n")
    txt = soup.get_text("\n")
    return normalize_spaced_letters(txt)

# =========================
# THREAD SPLITTING
# =========================
REPLY_SEP_PATTERNS = [
    r"^\s*From\s*:\s+",
    r"^\s*Von\s*:\s+",
    r"^\s*Sent\s*:\s+",
    r"^\s*Gesendet\s*:\s+",
    r"^\s*-{2,}\s*Original Message\s*-{2,}\s*$",
    r"^\s*_{5,}\s*$",
]
REPLY_SEP_RE = re.compile("|".join(REPLY_SEP_PATTERNS), re.IGNORECASE | re.MULTILINE)

def split_into_turns(full_text: str) -> List[str]:
    t = (full_text or "").replace("\r\n", "\n").strip()
    if not t:
        return []
    idxs = [m.start() for m in REPLY_SEP_RE.finditer(t)]
    if not idxs:
        return [t[:MAX_TURN_CHARS]]

    chunks = []
    starts = sorted(set([0] + idxs))
    for i, s in enumerate(starts):
        e = starts[i + 1] if i + 1 < len(starts) else len(t)
        piece = t[s:e].strip()
        if piece:
            chunks.append(piece)

    # Oldest first (your requirement: question starts from bottom)
    chunks = list(reversed(chunks))
    return [c[:MAX_TURN_CHARS] for c in chunks]

# =========================
# SENSITIVE PATTERNS (DELETE)
# =========================
EMAIL_RE = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b")
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b|\bwww\.[^\s<>()]+\b", re.IGNORECASE)
DOMAIN_RE = re.compile(r"\b[a-z0-9.-]+\.(?:de|com|net|org|eu|io|co|ch|at|fr|it|es|nl|se|no|dk|pl|cz|hu|ro|sk|si|pt)\b", re.IGNORECASE)
PHONE_RE = re.compile(r"(?:(?:\+|00)\d{1,3}[\s\-]?)?(?:\(?\d{2,5}\)?[\s\-]?)?\d[\d\s\-]{6,}\d")
MONEY_RE = re.compile(r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b|\b€\s*\d[\d\.\,\s]*\b")
IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b")
REF_RE = re.compile(r"\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|VU|SP|ID|Ref|RFQ)\s*[:#]?\s*[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b", re.IGNORECASE)
STREET_RE = re.compile(
    r"\b[A-ZÄÖÜ][A-Za-zÄÖÜäöüß\-]+(?:\s+[A-ZÄÖÜ][A-Za-zÄÖÜäöüß\-]+){0,4}\s+"
    r"(Straße|Strasse|Str\.|Weg|Allee|Platz|Ring|Gasse|Damm|Ufer|Chaussee|Gürtel)\b"
    r"(?:\s+\d{1,5}[a-zA-Z]?)?",
    re.IGNORECASE
)
STREET_ONLY_RE = re.compile(r"\b[A-ZÄÖÜ][A-Za-zÄÖÜäöüß\-]{2,}(straße|strasse|weg|allee|platz|ring|gasse|damm|ufer)\b", re.IGNORECASE)

TITLE_NAME_RE = re.compile(r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b")
FIRST_LAST_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}\b")
LAST_FIRST_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,},\s*[A-ZÄÖÜ][a-zäöüß]{2,}\b")

GREET_NAME_RE = re.compile(r"^\s*(Hallo|Hi|Hey|Moin|Guten\s+Morgen|Guten\s+Tag|Guten\s+Abend|Dear|Hello)\s+(.{1,60}?)([,!:])\s*$", re.IGNORECASE | re.MULTILINE)
SIGNOFF_LINE_RE = re.compile(r"^\s*(VG|Lg|LG|BR|MfG|Mit\s+freundlichen\s+Grüßen|Best\s+regards|Kind\s+regards|Regards|Thanks|Thank\s+you)[\s,]*.*$", re.IGNORECASE | re.MULTILINE)

COMPANY_SUFFIX_RE = re.compile(r"\b[A-Z][A-Za-z0-9&\-. ]{1,80}\s+(GmbH|AG|KG|SE|Inc\.?|Ltd\.?|LLC|S\.p\.A\.|SARL|BV|NV)\b")
SUPPLIER_CUE_RE = re.compile(r"\b(Lieferant|Supplier|Firma|Company|Unternehmen)\b\s*[:\-]?\s*([A-Za-z0-9&\-. ]{2,80})", re.IGNORECASE)

HEADER_LINE_RE = re.compile(r"^\s*(Von|From|An|To|Cc|CC|Bcc|BCC|Betreff|Subject|Gesendet|Sent|Date|Datum|Priorität|Priority)\s*:\s*.*$", re.IGNORECASE | re.MULTILINE)
MAILTO_RE = re.compile(r"mailto\s*:\s*\S+", re.IGNORECASE)

def strip_signature_tail(text: str) -> str:
    if not text:
        return ""
    lines = text.splitlines()
    if len(lines) < 4:
        return text.strip()
    last_idx = None
    for i, ln in enumerate(lines):
        if SIGNOFF_LINE_RE.match(ln):
            last_idx = i
    if last_idx is None:
        return text.strip()
    if last_idx < int(0.6 * len(lines)):
        return text.strip()
    return "\n".join(lines[:last_idx]).strip()

def redact_delete(text: str) -> Tuple[str, Dict[str, int]]:
    stats = {"emails":0,"phones":0,"urls":0,"domains":0,"iban":0,"money":0,"refs":0,"addresses":0,"names":0,"companies":0}
    t = (text or "").replace("\r\n", "\n")

    t = HEADER_LINE_RE.sub("", t)
    t, n = MAILTO_RE.subn("", t); stats["urls"] += n

    def _greet_sub(m: re.Match) -> str:
        return f"{m.group(1)}{m.group(3)}"
    t, n = GREET_NAME_RE.subn(_greet_sub, t); stats["names"] += n

    t, n = URL_RE.subn("", t); stats["urls"] += n
    t, n = EMAIL_RE.subn("", t); stats["emails"] += n
    t, n = DOMAIN_RE.subn("", t); stats["domains"] += n

    t, n = IBAN_RE.subn("", t); stats["iban"] += n
    t, n = MONEY_RE.subn("", t); stats["money"] += n
    t, n = REF_RE.subn("", t); stats["refs"] += n
    t, n = PHONE_RE.subn("", t); stats["phones"] += n

    t, n = STREET_RE.subn("", t); stats["addresses"] += n
    t, n = STREET_ONLY_RE.subn("", t); stats["addresses"] += n

    t, n = COMPANY_SUFFIX_RE.subn("", t); stats["companies"] += n
    t, n = SUPPLIER_CUE_RE.subn(lambda m: m.group(1) + ":", t); stats["companies"] += n

    t, n = TITLE_NAME_RE.subn("", t); stats["names"] += n
    t, n = LAST_FIRST_RE.subn("", t); stats["names"] += n
    t, n = FIRST_LAST_RE.subn("", t); stats["names"] += n

    t = strip_signature_tail(t)

    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = "\n".join(line.strip() for line in t.splitlines())
    t = t.strip()
    return t, stats

# =========================
# QUESTION / ANSWER LABELS (PER TURN)
# =========================
QUESTION_CUES = re.compile(
    r"(\?|bitte|kannst\s+du|könnt\s+ihr|could\s+you|can\s+you|please\s+send|please\s+share|"
    r"wie\s+ist|was\s+ist|wann|wo|warum|wieso|benötigen\s+wir|brauchen\s+wir|"
    r"uns\s+bitte|bitte\s+um|request|anfrage|frage|fragen)",
    re.IGNORECASE
)
ANSWER_CUES = re.compile(
    r"\b(danke|anbei|im\s+Anhang|hier\s+die|hiermit|zurückmeldung|antwort|we\s+will|we\s+can|"
    r"as\s+discussed|following|below|siehe|siehe\s+unten|wir\s+werden|wir\s+können|"
    r"ich\s+finde|aus\s+unserer\s+sicht|vorschlag|empfehlen)\b",
    re.IGNORECASE
)

def label_turn_heuristic(turn_text: str) -> str:
    t = (turn_text or "").strip().lower()
    if not t:
        return "other"
    q = len(QUESTION_CUES.findall(t))
    a = len(ANSWER_CUES.findall(t))
    if q >= 2 and a == 0:
        return "question"
    if a >= 2 and q == 0:
        return "answer"
    if a > q and a >= 1:
        return "answer"
    if q > a and q >= 1:
        return "question"
    return "other"

# =========================
# ATTACHMENT TEXT EXTRACTION
# =========================
def extract_text_from_attachment_file(p: Path) -> str:
    ext = p.suffix.lower()
    try:
        if ext == ".txt":
            return p.read_text(encoding="utf-8", errors="ignore")

        if ext == ".pdf":
            # Prefer pdfplumber when present
            if pdfplumber is not None:
                out = []
                with pdfplumber.open(str(p)) as pdf:
                    for page in pdf.pages:
                        try:
                            out.append(page.extract_text() or "")
                        except Exception:
                            pass
                return "\n".join(out)

            if PdfReader is not None:
                r = PdfReader(str(p))
                parts = []
                for pg in r.pages:
                    try:
                        parts.append(pg.extract_text() or "")
                    except Exception:
                        pass
                return "\n".join(parts)
            return ""

        if ext == ".docx" and DocxDocument is not None:
            doc = DocxDocument(str(p))
            paras = [clean_ws(par.text) for par in doc.paragraphs if clean_ws(par.text)]
            return "\n".join(paras)

        if ext == ".xlsx" and openpyxl is not None:
            wb = openpyxl.load_workbook(str(p), data_only=True, read_only=True)
            out = []
            for ws in wb.worksheets[:6]:
                out.append(f"Sheet: {ws.title}")
                max_r = min(ws.max_row or 0, 120)
                max_c = min(ws.max_column or 0, 25)
                for r in range(1, max_r + 1):
                    row = []
                    for c in range(1, max_c + 1):
                        v = ws.cell(row=r, column=c).value
                        row.append("" if v is None else str(v))
                    if any(x.strip() for x in row):
                        out.append(" | ".join(clean_ws(x) for x in row))
            return "\n".join(out)

    except Exception:
        return ""
    return ""

def extract_attachments_text(mail_item) -> Tuple[str, Dict[str, int]]:
    """
    Returns (text, attachment_stats)
    attachment_stats includes saved_count, extracted_count, extracted_chars, skipped_count, reasons.
    """
    info = {
        "saved": 0, "extracted": 0, "extracted_chars": 0,
        "skipped": 0, "no_text": 0
    }
    if not PROCESS_ATTACHMENTS:
        return "", info

    try:
        atts = mail_item.Attachments
    except Exception:
        return "", info

    allowed_exts = {".txt", ".pdf", ".docx", ".xlsx"}
    collected = []
    total = 0

    for i in range(1, min(atts.Count, MAX_ATTACHMENTS_PER_MAIL) + 1):
        try:
            att = atts.Item(i)
            fname = getattr(att, "FileName", "") or f"attachment_{i}"
            ext = Path(fname).suffix.lower()
            if ext not in allowed_exts:
                info["skipped"] += 1
                continue

            safe = re.sub(r"[^A-Za-z0-9._-]+", "_", fname)
            tmp_path = ATTACH_TMP / f"{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}__{safe}"

            try:
                att.SaveAsFile(str(tmp_path))
                info["saved"] += 1
            except Exception:
                info["skipped"] += 1
                continue

            txt = extract_text_from_attachment_file(tmp_path)
            txt = normalize_spaced_letters(txt).strip()

            if not txt:
                info["no_text"] += 1
                continue

            txt_red, _ = redact_delete(txt)
            if not txt_red:
                info["no_text"] += 1
                continue

            block = f"\n\n[Attachment: {fname}]\n{txt_red}\n"
            if total + len(block) > MAX_ATTACHMENT_CHARS_TOTAL:
                break
            collected.append(block)
            total += len(block)
            info["extracted"] += 1
            info["extracted_chars"] += len(txt_red)

        except Exception:
            info["skipped"] += 1
            continue

    return "\n".join(collected).strip(), info

# =========================
# OUTLOOK .MSG PARSING
# =========================
def open_msg_via_outlook(ns, msg_path: Path):
    try:
        return ns.OpenSharedItem(str(msg_path))
    except Exception:
        return ns.Application.CreateItemFromTemplate(str(msg_path))

def extract_subject_and_body(ns, msg_path: Path) -> Tuple[str, str, Dict[str, int]]:
    item = open_msg_via_outlook(ns, msg_path)
    subj = getattr(item, "Subject", "") or ""
    html_body = getattr(item, "HTMLBody", "") or ""
    body = getattr(item, "Body", "") or ""

    if html_body and html_body.strip():
        text = html_to_text(html_body)
    else:
        text = normalize_spaced_letters(body)

    att_text, att_info = extract_attachments_text(item)
    if att_text:
        text = (text + "\n\n" + att_text).strip()

    return subj, text, att_info

# =========================
# HTML INPUT MODE (if .html files are “mails”)
# =========================
def extract_from_html_file(path: Path) -> Tuple[str, str]:
    raw = path.read_text(encoding="utf-8", errors="ignore")
    txt = html_to_text(raw)
    # best-effort subject from filename
    subj = path.stem
    return subj, txt

# =========================
# HTML OUTPUT
# =========================
def render_one_html(source_name: str, subject: str, turns: List[Tuple[str, str]],
                    stats: Dict[str, int], attach_info: Dict[str, int]) -> str:
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    subj_show = escape(subject.strip() or "(no subject)")

    stats_rows = ""
    for k in ["emails","phones","urls","domains","iban","money","refs","addresses","names","companies"]:
        stats_rows += f"<tr><td>{escape(k)}</td><td style='text-align:right'>{int(stats.get(k,0))}</td></tr>\n"

    attach_rows = ""
    for k in ["saved","extracted","extracted_chars","skipped","no_text"]:
        attach_rows += f"<tr><td>{escape(k)}</td><td style='text-align:right'>{int(attach_info.get(k,0))}</td></tr>\n"

    turn_blocks = []
    for idx, (lbl, txt) in enumerate(turns, start=1):
        safe_txt = escape(txt).replace("\n", "<br>\n")
        turn_blocks.append(
            f"<div style='margin:12px 0;padding:10px;border:1px solid #ddd;border-radius:8px;'>"
            f"<div style='font-weight:600;margin-bottom:6px;'>Turn {idx} — {escape(lbl)}</div>"
            f"<div style='line-height:1.35'>{safe_txt or '<i>(empty after sanitization)</i>'}</div>"
            f"</div>"
        )
    body = "\n".join(turn_blocks) if turn_blocks else "<i>(no usable body text extracted)</i>"

    return f"""<!doctype html>
<html>
<head><meta charset="utf-8"><title>Sanitized Mail</title></head>
<body style="font-family:Segoe UI,Arial,sans-serif;font-size:13px;padding:16px;">
  <div style="margin-bottom:10px;">
    <div><b>Source:</b> {escape(source_name)}</div>
    <div><b>Generated:</b> {escape(now)}</div>
    <div><b>Subject:</b> {subj_show}</div>
  </div>

  <div style="margin:12px 0;">
    <b>RedactionStats</b>
    <table style="border-collapse:collapse;margin-top:6px;">
      <thead>
        <tr><th style="text-align:left;padding:4px 8px;border-bottom:1px solid #ddd;">Type</th>
            <th style="text-align:right;padding:4px 8px;border-bottom:1px solid #ddd;">Count</th></tr>
      </thead>
      <tbody>{stats_rows}</tbody>
    </table>
  </div>

  <div style="margin:12px 0;">
    <b>AttachmentStats</b>
    <div style="margin:6px 0;color:#555;">
      PDF readers: pdfplumber={'yes' if pdfplumber else 'no'}, pypdf={'yes' if PdfReader else 'no'}
    </div>
    <table style="border-collapse:collapse;margin-top:6px;">
      <thead>
        <tr><th style="text-align:left;padding:4px 8px;border-bottom:1px solid #ddd;">Key</th>
            <th style="text-align:right;padding:4px 8px;border-bottom:1px solid #ddd;">Value</th></tr>
      </thead>
      <tbody>{attach_rows}</tbody>
    </table>
  </div>

  <hr style="margin:14px 0;">
  {body}
</body>
</html>"""

# =========================
# INPUT DISCOVERY
# =========================
def gather_inputs() -> List[Path]:
    base = Path(MSG_DIR)
    if not base.exists():
        raise SystemExit(f"MSG_DIR not found: {MSG_DIR}")

    patterns = ["*.msg"]
    if INCLUDE_HTML_FILES:
        patterns += ["*.html", "*.htm"]

    files: List[Path] = []
    for pat in patterns:
        files.extend(base.rglob(pat) if RECURSIVE else base.glob(pat))

    files = [p for p in files if p.is_file() and not p.name.startswith("~$")]
    files.sort(key=lambda p: p.name.lower())
    if MAX_FILES is not None:
        files = files[:MAX_FILES]
    return files

# =========================
# MAIN PROCESSORS
# =========================
def process_one(ns, path: Path) -> Tuple[str, str, int, int, Dict[str, int], Dict[str, int]]:
    """
    Returns tuple for CSV:
    status, reason, body_chars_in, body_chars_out, redaction_stats, attach_info
    """
    try:
        if path.suffix.lower() in {".html", ".htm"}:
            subj, full_text = extract_from_html_file(path)
            attach_info = {"saved":0,"extracted":0,"extracted_chars":0,"skipped":0,"no_text":0}
        else:
            subj, full_text, attach_info = extract_subject_and_body(ns, path)

        body_in_len = len(full_text or "")
        if body_in_len == 0 and (subj or "").strip():
            # still output a file (your requirement: 1 file per input)
            full_text = ""
        elif body_in_len > MAX_TOTAL_BODY_CHARS:
            full_text = full_text[:MAX_TOTAL_BODY_CHARS]

        turns_raw = split_into_turns(full_text) if full_text.strip() else []
        if not turns_raw and (full_text or "").strip():
            turns_raw = [full_text[:MAX_TURN_CHARS]]

        total_stats = {"emails":0,"phones":0,"urls":0,"domains":0,"iban":0,"money":0,"refs":0,"addresses":0,"names":0,"companies":0}

        subj_clean, subj_stats = redact_delete(subj)
        for k,v in subj_stats.items():
            total_stats[k] += int(v)

        turns_out: List[Tuple[str, str]] = []
        for turn in turns_raw:
            t_clean, st = redact_delete(turn)
            for k,v in st.items():
                total_stats[k] += int(v)
            lbl = label_turn_heuristic(t_clean)
            turns_out.append((lbl, t_clean))

        body_out_len = sum(len(t) for _, t in turns_out)

        out_name = safe_filename(path.stem)
        out_path = OUT_SAN / out_name
        html = render_one_html(path.name, subj_clean, turns_out, total_stats, attach_info)
        out_path.write_text(html, encoding="utf-8", errors="ignore")

        return "OK", out_name, body_in_len, body_out_len, total_stats, attach_info

    except Exception as e:
        log_error(f"{path.name} -> {repr(e)}")
        return "SKIP", repr(e), 0, 0, {}, {}

def main():
    time.sleep(STARTUP_DELAY_SEC)
    inputs = gather_inputs()
    if not inputs:
        raise SystemExit(f"No .msg files found in: {MSG_DIR}")

    ns = win32.Dispatch("Outlook.Application").GetNamespace("MAPI")

    # CSV report
    with REPORT_CSV.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow([
            "input_file","status","reason_or_output","body_chars_in","body_chars_out",
            "att_saved","att_extracted","att_extracted_chars","att_skipped","att_no_text"
        ])

        ok = 0
        fail = 0
        outputs = []
        failed = []

        for p in inputs:
            status, info, in_len, out_len, rstats, ainfo = process_one(ns, p)
            if status == "OK":
                ok += 1
                outputs.append({"input": p.name, "output": info})
            else:
                fail += 1
                failed.append({"input": p.name, "error": info})

            w.writerow([
                p.name, status, info, in_len, out_len,
                ainfo.get("saved",0), ainfo.get("extracted",0), ainfo.get("extracted_chars",0),
                ainfo.get("skipped",0), ainfo.get("no_text",0)
            ])

    stats = {
        "run_out_dir": str(OUT_DIR),
        "input_dir": MSG_DIR,
        "count_inputs": len(inputs),
        "count_ok": ok,
        "count_failed": fail,
        "include_html_files": INCLUDE_HTML_FILES,
        "process_attachments": PROCESS_ATTACHMENTS,
        "optional_libs": {
            "beautifulsoup4": bool(BeautifulSoup),
            "pdfplumber": bool(pdfplumber),
            "pypdf": bool(PdfReader),
            "python_docx": bool(DocxDocument),
            "openpyxl": bool(openpyxl),
        },
        "outputs": outputs,
        "failed": failed,
        "report_csv": str(REPORT_CSV),
        "errors_log": str(ERR_LOG),
    }
    STATS_JSON.write_text(json.dumps(stats, indent=2, ensure_ascii=False), encoding="utf-8")

    print("DONE")
    print("Run output folder:", OUT_DIR)
    print("Sanitized HTML:", OUT_SAN)
    print("Report CSV:", REPORT_CSV)
    print("Errors log:", ERR_LOG)
    print("OK:", ok, "FAILED:", fail)

if __name__ == "__main__":
    main()
