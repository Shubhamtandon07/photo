# -*- coding: utf-8 -*-
"""
DETERMINISTIC ANALYSE (NO LLM)
reads:  C:/Users/SHTANDO/Desktop/KI Risko/gescrapte_Artikel-Links/gescrapte_links_<rohst>.csv
writes: C:/Users/SHTANDO/Desktop/KI Risko/Analyse_Tabelle/analyse_tabelle_<rohst>.csv
"""

import csv
import re
import time
import random
from pathlib import Path
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup

# Selenium fallback (only if needed)
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# --------------------------------------------------
# 0) Paths
# --------------------------------------------------
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_für_Einstellungen_der_Suche"
SCRAPED_DIR = PROJECT_ROOT / "gescrapte_Artikel-Links"
OUTPUT_DIR = PROJECT_ROOT / "Analyse_Tabelle"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

WRITE_DEBUG = True

# --------------------------------------------------
# 1) Read settings
# --------------------------------------------------
rohst = rawm = max_datum = None

def lese_einstellungen():
    global rohst, rawm, max_datum
    settings_path = SETTINGS_DIR / "Einstellungen_Analyse.csv"
    with settings_path.open("r", encoding="utf-8-sig") as f:
        rows = list(csv.reader(f, delimiter=","))
    def get_row(idx, default=""):
        return rows[idx][0].strip() if len(rows) > idx and len(rows[idx]) > 0 else default
    rohst     = get_row(5, "Stahl")
    rawm      = get_row(8, "Steel")
    max_datum = get_row(11, "20.11.2020")

# --------------------------------------------------
# 2) Link loader
# --------------------------------------------------
def lese_links_aus_scrape_csv(rohstoff: str) -> list[str]:
    fpath = SCRAPED_DIR / f"gescrapte_links_{rohstoff}.csv"
    if not fpath.exists():
        raise FileNotFoundError(f"Scrape CSV not found: {fpath}")
    links = []
    with fpath.open("r", encoding="utf-8-sig") as f:
        r = csv.reader(f, delimiter=";")
        next(r, None)
        for row in r:
            if row and row[0].strip():
                links.append(row[0].strip())
    # dedupe keep order
    seen = set()
    out = []
    for u in links:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# --------------------------------------------------
# 3) Fetch helpers (requests -> jina -> selenium)
# --------------------------------------------------
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15",
]

def _looks_like_block(html_text: str) -> bool:
    if not html_text:
        return True
    t = html_text.lower()
    block_signals = [
        "captcha", "cloudflare", "attention required", "access denied",
        "bot detection", "verify you are human", "unusual traffic",
        "checking your browser", "ddos protection",
    ]
    return any(s in t for s in block_signals)

def fetch_requests(url: str, timeout: int = 20) -> tuple[bool, str, str]:
    """Returns (ok, text, reason)"""
    try:
        headers = {"User-Agent": random.choice(UA_LIST), "Accept-Language": "en,de;q=0.9"}
        r = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        if r.status_code >= 400:
            return False, "", f"http_{r.status_code}"
        if _looks_like_block(r.text):
            return False, r.text, "blocked_or_captcha"
        return True, r.text, "ok"
    except Exception as e:
        return False, "", f"requests_error:{e}"

def fetch_jina_text(url: str, timeout: int = 25) -> tuple[bool, str, str]:
    """
    Jina AI text mirror: https://r.jina.ai/http(s)://...
    Often bypasses heavy JS/CF pages and returns readable text.
    """
    try:
        mirror = "https://r.jina.ai/" + url
        headers = {"User-Agent": random.choice(UA_LIST)}
        r = requests.get(mirror, headers=headers, timeout=timeout)
        if r.status_code >= 400:
            return False, "", f"jina_http_{r.status_code}"
        txt = r.text
        if not txt or len(txt) < 200:
            return False, txt, "jina_too_short"
        # Mirror returns plain text; still check for bot blocks
        if _looks_like_block(txt):
            return False, txt, "blocked_or_captcha"
        return True, txt, "ok_jina"
    except Exception as e:
        return False, "", f"jina_error:{e}"

def make_driver():
    opts = Options()
    # IMPORTANT: try non-headless first for fewer blocks
    # opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--start-maximized")
    opts.add_argument(f"user-agent={random.choice(UA_LIST)}")
    return webdriver.Chrome(options=opts)

def fetch_selenium(url: str, timeout: int = 20) -> tuple[bool, str, str]:
    d = make_driver()
    try:
        d.set_page_load_timeout(timeout)
        d.get(url)
        time.sleep(3)
        html = d.page_source or ""
        if _looks_like_block(html):
            return False, html, "blocked_or_captcha"
        return True, html, "ok_selenium"
    except Exception as e:
        return False, "", f"selenium_error:{e}"
    finally:
        try:
            d.quit()
        except Exception:
            pass

def url_to_html(url: str) -> tuple[bool, str, str]:
    # 1) requests
    ok, html, reason = fetch_requests(url)
    if ok:
        return True, html, reason

    # 2) jina mirror fallback
    ok2, txt2, reason2 = fetch_jina_text(url)
    if ok2:
        # already text, not html; we treat it as "html-like" container
        return True, txt2, reason2

    # 3) selenium fallback
    ok3, html3, reason3 = fetch_selenium(url)
    return ok3, html3, reason3

# --------------------------------------------------
# 4) Text extraction (deterministic)
# --------------------------------------------------
def extract_text_from_html(html_or_text: str) -> str:
    # If it's already plain text (jina), keep it
    if "<html" not in (html_or_text[:500].lower()):
        return re.sub(r"\n{3,}", "\n\n", html_or_text).strip()

    soup = BeautifulSoup(html_or_text, "html.parser")

    # remove junk
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "aside"]):
        tag.decompose()

    article = soup.find("article")
    if article:
        text = article.get_text("\n", strip=True)
        return text

    body = soup.find("body")
    if body:
        text = body.get_text("\n", strip=True)
        return text

    return soup.get_text("\n", strip=True)

# --------------------------------------------------
# 5) Deterministic risk analysis
# --------------------------------------------------
RISIKOKATEGORIEN = {
    "A": ("Working conditions, including occupational health and safety",
          [r"unsafe working", r"occupational", r"injur", r"fatalit", r"accident", r"hazard", r"explosion"]),
    "B": ("Child labour",
          [r"child labour", r"child labor", r"underage", r"minor[s]?\b", r"children working"]),
    "C": ("Modern slavery, including forced labour",
          [r"forced labour", r"forced labor", r"modern slavery", r"traffick", r"bonded labour", r"debt bondage"]),
    "D": ("Community and indigenous peoples’ rights",
          [r"indigenous", r"displacement", r"evict", r"land grab", r"community (?:conflict|protest)", r"resettlement"]),
    "E": ("Excessive violence by private and public security forces",
          [r"shoot", r"kill", r"violence", r"security forces", r"military", r"police brutality"]),
    "F": ("Environmental risks with impact on human rights",
          [r"pollution", r"contaminat", r"toxic", r"spill", r"tailings", r"waste", r"poison", r"radiation"]),
    "G": ("Business conduct in Conflict and High Risk Areas",
          [r"conflict mineral", r"armed group", r"war", r"sanction", r"high risk area"]),
    "H": ("Serious human rights abuses",
          [r"human rights abuse", r"gross violation", r"crimes against humanity", r"torture"]),
    "I": ("Biodiversity",
          [r"biodiversity", r"habitat", r"endangered", r"wildlife", r"protected area"]),
    "J": ("Water",
          [r"water pollution", r"river", r"groundwater", r"drinking water", r"water contamination"]),
    "K": ("Air",
          [r"air pollution", r"emission", r"smog", r"dust", r"sulfur dioxide", r"so2\b"]),
    "L": ("Soil",
          [r"soil contamination", r"soil pollution", r"land contamination"]),
    "M": ("Waste, hazardous substances, and plant safety",
          [r"hazardous", r"hazmat", r"cyanide", r"mercury", r"arsenic", r"chemical leak"]),
}

# Add your requested extra keywords conceptually (no LLM needed)
DISCRIMINATION_PATTERNS = [r"discrimination", r"racial", r"religion", r"religious", r"age discrimination"]
UNION_PATTERNS = [r"trade union", r"union rights", r"union-bust", r"collective bargaining", r"strike", r"labor strike"]

def analyse_text(rohstoff_name: str, text: str) -> list[str]:
    t = (text or "").lower()

    # Basic gates
    years = sorted(set(re.findall(r"\b(19\d{2}|20\d{2})\b", t)))

    # crude countries: just detect common “in <Country>” style tokens if present
    # (You can later replace this with spaCy when you move to Python 3.12)
    country_hits = []
    for c in ["china","india","brazil","indonesia","peru","mexico","canada","australia",
              "vietnam","philippines","south africa","zimbabwe","congo","russia","ukraine","germany"]:
        if re.search(rf"\b{re.escape(c)}\b", t):
            country_hits.append(c.title())

    # Categories
    matched_cats = []
    harms = []

    for code, (label, pats) in RISIKOKATEGORIEN.items():
        if any(re.search(p, t) for p in pats):
            matched_cats.append(f"{code} ({label})")

    # Discrimination + union freedoms (map them into A/H/D depending on your taxonomy)
    if any(re.search(p, t) for p in DISCRIMINATION_PATTERNS):
        harms.append("Discrimination (e.g., age/race/religion)")
        # often fits under H or A depending on your scheme; keep as harm text
        if not any(x.startswith("H ") for x in matched_cats):
            matched_cats.append("H (Serious human rights abuses)")

    if any(re.search(p, t) for p in UNION_PATTERNS):
        harms.append("Trade union freedoms / union rights issues")
        if not any(x.startswith("A ") for x in matched_cats):
            matched_cats.append("A (Working conditions, including occupational health and safety)")

    # Harms (simple)
    harm_terms = [
        ("child labour", [r"child labour", r"child labor", r"underage"]),
        ("forced labour", [r"forced labour", r"forced labor", r"modern slavery", r"traffick"]),
        ("unsafe working conditions", [r"unsafe", r"fatal", r"injur", r"accident", r"hazard"]),
        ("pollution / contamination", [r"pollution", r"contaminat", r"toxic", r"spill", r"tailings"]),
        ("community conflict", [r"community conflict", r"protest", r"displacement", r"evict"]),
    ]
    for name, pats in harm_terms:
        if any(re.search(p, t) for p in pats):
            harms.append(name)

    harms = list(dict.fromkeys([h for h in harms if h]))

    # severity heuristic
    severity = "minor"
    if re.search(r"\b(killed|death|deaths|fatal)\b", t) or re.search(r"\bmajor spill\b", t):
        severity = "major"
    if re.search(r"\b(critical|catastroph)\b", t) or re.search(r"\bmass (?:death|deaths)\b", t):
        severity = "critical"
    if re.search(r"\b(lawsuit|prosecution|criminal)\b", t) and severity == "minor":
        severity = "moderate"

    # minimal structured output (13 fields like your table)
    out = [
        ", ".join(matched_cats) if matched_cats else "Not in Text",
        "; ".join(harms) if harms else "Not in Text",
        "Deterministic keyword-based classification (no LLM).",
        ", ".join(years) if years else "Not in Text",
        ", ".join(country_hits) if country_hits else "Not in Text",
        "Not in Text",  # Orte
        "Not in Text",  # Stufe
        "Not in Text",  # Unternehmen
        "Not in Text",  # Gruppen/Öko
        "Not in Text",  # Mindestzahl
        "Not in Text",  # Begründung Mindestzahl
        severity,
        "Heuristic severity based on presence of deaths/major incidents/legal action keywords.",
    ]
    return out

# --------------------------------------------------
# 6) Write analyse CSV
# --------------------------------------------------
def schreibe_analyse_csv(rohstoff: str, daten_zeilen: list[list[str]]):
    out_path = OUTPUT_DIR / f"analyse_tabelle_{rohstoff}.csv"
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow([
            "link",
            "Risikokategorien (A–M)",
            "Arten der Menschenrechts-/Umweltschädigungen",
            "Begründung",
            "Jahr(e)",
            "Land/Länder",
            "Ort(e)",
            "Stufe der Rohstoffgewinnung",
            "Involvierte Unternehmen",
            "Betroffene Personengruppen/Ökosysteme",
            "Mindestzahl der betroffenen Personen",
            "Begründung Mindestzahl",
            "Schwere",
            "Begründung Schwere",
            "fetch_reason",
        ])
        for row in daten_zeilen:
            if not WRITE_DEBUG:
                all_not = all(isinstance(x, str) and x.strip().lower() == "not in text" for x in row[1:14])
                if all_not:
                    continue
            w.writerow(row)
    print(f"✅ Analyse geschrieben nach: {out_path}")

# --------------------------------------------------
# 7) Main
# --------------------------------------------------
if __name__ == "__main__":
    lese_einstellungen()
    print(f"SETTINGS:\n  rohst     = {rohst}\n  rawm      = {rawm}\n  max_datum = {max_datum}")

    links = lese_links_aus_scrape_csv(rohst)
    print("Links gefunden:", len(links))

    rows, failed = [], []

    for i, link in enumerate(links, start=1):
        print(f"[{i}/{len(links)}] analysiere: {link}")
        ok, html_or_text, fetch_reason = url_to_html(link)
        if not ok:
            print(f"  ❌ error: {fetch_reason}")
            failed.append((link, fetch_reason))
            continue

        text = extract_text_from_html(html_or_text)
        if not text or len(text) < 400:
            print("  ❌ error: too_little_text")
            failed.append((link, "too_little_text"))
            continue

        analysed = analyse_text(rawm, text)
        rows.append([link] + analysed + [fetch_reason])
        print(f"  ✅ done | fetch={fetch_reason} | text_len={len(text)}")

        time.sleep(1.5)

    schreibe_analyse_csv(rohst, rows)

    if failed:
        fail_path = OUTPUT_DIR / f"fehlgeschlagene_urls_analyse_{rohst}.csv"
        with fail_path.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f, delimiter=";")
            w.writerow(["link", "reason"])
            for u, r in failed:
                w.writerow([u, r])
        print(f"⚠️ Fehlgeschlagen: {len(failed)} → {fail_path}")
