from pathlib import Path
from datetime import datetime
import re
import pdfplumber

from docx import Document
from docx.enum.text import WD_ALIGN_PARAGRAPH


# =========================
# CONFIG
# =========================
RUN_DIR = Path(r"C:\Users\SHTANDO\Desktop\chatbot\Mails Rasmus\_exports\collect_att_20260219_192712")
PDF_DIR = RUN_DIR / "pdfs"

OUT_DIR = RUN_DIR / f"docx_extracted_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
OUT_PDF = OUT_DIR / "from_pdfs"
OUT_PDF.mkdir(parents=True, exist_ok=True)


# =========================
# HELPERS
# =========================
def safe_filename(name: str, max_len: int = 150) -> str:
    name = (name or "").strip()
    name = re.sub(r"[<>:\"/\\|?*\x00-\x1F]", "_", name)
    name = re.sub(r"\s+", " ", name)
    if len(name) <= max_len:
        return name
    base, dot, ext = name.rpartition(".")
    if dot:
        base = base[: max_len - (len(ext) + 1)]
        return f"{base}.{ext}"
    return name[:max_len]

def normalize_spaces(s: str) -> str:
    s = (s or "")
    s = s.replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def add_heading_meta(doc: Document, title: str, source_path: Path):
    h = doc.add_heading(title, level=1)
    h.alignment = WD_ALIGN_PARAGRAPH.LEFT
    p = doc.add_paragraph()
    p.add_run("Source: ").bold = True
    p.add_run(str(source_path))
    doc.add_paragraph("")

def add_paragraphs(doc: Document, lines):
    for ln in lines:
        ln = normalize_spaces(ln)
        if ln:
            doc.add_paragraph(ln)

def add_word_table(doc: Document, rows, cols, style="Table Grid"):
    t = doc.add_table(rows=rows, cols=cols)
    t.style = style
    return t

def cluster_words_into_lines(words, y_tol=3.0):
    if not words:
        return []
    words = sorted(words, key=lambda w: (w["top"], w["x0"]))
    lines = []
    cur = [words[0]]
    cur_y = words[0]["top"]
    for w in words[1:]:
        if abs(w["top"] - cur_y) <= y_tol:
            cur.append(w)
        else:
            cur = sorted(cur, key=lambda z: z["x0"])
            lines.append(cur)
            cur = [w]
            cur_y = w["top"]
    cur = sorted(cur, key=lambda z: z["x0"])
    lines.append(cur)
    return lines

def detect_two_column_split_candidate(words, page_width, min_gap=45, min_share_each_side=0.30):
    """
    Returns candidate split_x or None.
    Stricter thresholds than before to reduce false positives.
    """
    if not words:
        return None
    boxes = [(w["x0"], w["x1"]) for w in words
             if isinstance(w.get("x0"), (int,float)) and isinstance(w.get("x1"), (int,float))]
    if len(boxes) < 60:   # need more evidence
        return None

    mid = page_width / 2.0
    candidates = [mid + d for d in (-100, -60, -30, 0, 30, 60, 100)]

    best = None
    best_score = 0.0

    for s in candidates:
        left = right = crossing = 0
        for x0, x1 in boxes:
            if x1 < s:
                left += 1
            elif x0 > s:
                right += 1
            else:
                crossing += 1

        total = left + right + crossing
        if total == 0:
            continue

        if left/total < min_share_each_side or right/total < min_share_each_side:
            continue

        # check empty band around split
        band_left = s - min_gap/2
        band_right = s + min_gap/2
        in_band = 0
        for x0, x1 in boxes:
            if not (x1 < band_left or x0 > band_right):
                in_band += 1

        # prefer fewer crossing and fewer in band
        score = (1 / (1 + in_band)) * (1 / (1 + crossing))
        if score > best_score:
            best_score = score
            best = s

    return best

def build_lines_two_columns(lines, split_x):
    out = []
    for line in lines:
        left_words = []
        right_words = []
        for w in line:
            x0, x1 = w["x0"], w["x1"]
            txt = w.get("text", "")
            if x1 < split_x:
                left_words.append((x0, txt))
            elif x0 > split_x:
                right_words.append((x0, txt))
            else:
                cx = (x0 + x1) / 2
                (left_words if cx < split_x else right_words).append((x0, txt))

        left_words.sort(key=lambda t: t[0])
        right_words.sort(key=lambda t: t[0])

        lt = normalize_spaces(" ".join(t[1] for t in left_words))
        rt = normalize_spaces(" ".join(t[1] for t in right_words))

        if lt or rt:
            out.append((lt, rt))
    return out

def is_probably_bilingual(pairs, min_both_rows=8, min_both_ratio=0.40, min_total_rows=20):
    """
    Decide if left/right are BOTH meaningful on many rows -> bilingual layout.
    """
    if not pairs:
        return False
    if len(pairs) < min_total_rows:
        return False

    both = [(l, r) for (l, r) in pairs if l and r]
    both_ratio = len(both) / max(1, len(pairs))

    # also require that both sides have non-trivial total length
    left_chars = sum(len(l) for (l, _) in pairs if l)
    right_chars = sum(len(r) for (_, r) in pairs if r)

    if len(both) >= min_both_rows and both_ratio >= min_both_ratio and left_chars > 300 and right_chars > 300:
        return True
    return False

def extract_tables_strict(page):
    """
    Keep only credible grid tables. If tables are image-like, this will return nothing (expected).
    """
    try:
        tables = page.extract_tables(
            table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_tolerance": 5,
                "snap_tolerance": 3,
                "join_tolerance": 3,
                "edge_min_length": 3,
                "min_words_vertical": 3,
                "min_words_horizontal": 1,
            }
        ) or []
    except Exception:
        return []

    clean = []
    for t in tables:
        if not t or not isinstance(t, list):
            continue
        rows = [r for r in t if isinstance(r, list)]
        if len(rows) < 2:
            continue
        max_cols = max((len(r) for r in rows), default=0)
        if max_cols < 2:
            continue
        cells = [str(c).strip() for r in rows for c in r if c is not None]
        nonempty = sum(1 for c in cells if c)
        if nonempty < 6:
            continue
        clean.append(rows)
    return clean

def add_table(doc, table_data):
    if not table_data:
        return
    max_cols = max((len(r) for r in table_data if isinstance(r, list)), default=0)
    if max_cols < 1:
        return
    norm = []
    for r in table_data:
        r2 = [(c if c is not None else "") for c in r]
        if len(r2) < max_cols:
            r2 += [""] * (max_cols - len(r2))
        norm.append([normalize_spaces(str(c)) for c in r2])

    t = doc.add_table(rows=len(norm), cols=max_cols)
    t.style = "Table Grid"
    for i, row in enumerate(norm):
        for j, cell in enumerate(row):
            t.cell(i, j).text = cell
    doc.add_paragraph("")


# =========================
# CORE: PDF -> DOCX
# =========================
def extract_pdf_to_docx(pdf_path: Path, out_docx: Path):
    doc = Document()
    add_heading_meta(doc, f"PDF Extraction: {pdf_path.name}", pdf_path)

    with pdfplumber.open(str(pdf_path)) as pdf:
        for page_idx, page in enumerate(pdf.pages, start=1):
            doc.add_heading(f"Page {page_idx}", level=2)

            # A) Real tables (grid)
            tables = extract_tables_strict(page)
            if tables:
                doc.add_paragraph("Tables:")
                for ti, tbl in enumerate(tables, start=1):
                    doc.add_paragraph(f"Table {ti}:")
                    add_table(doc, tbl)

            # B) Words with coordinates
            try:
                words = page.extract_words(
                    keep_blank_chars=False,
                    use_text_flow=False,
                    extra_attrs=["fontname", "size"]
                )
            except Exception:
                words = []

            if not words:
                doc.add_paragraph("(No readable text found.)")
                doc.add_page_break()
                continue

            lines = cluster_words_into_lines(words, y_tol=3.0)

            # Candidate split (stricter)
            split_x = detect_two_column_split_candidate(words, page.width)

            # If split candidate exists, validate bilingual-ness
            if split_x is not None:
                pairs = build_lines_two_columns(lines, split_x)

                if is_probably_bilingual(pairs):
                    # ✅ bilingual: make a 2-col table (this is the ONLY time we do it)
                    t = add_word_table(doc, rows=len(pairs) + 1, cols=2, style="Table Grid")
                    t.cell(0, 0).text = "Deutsch (links)"
                    t.cell(0, 1).text = "English (right)"
                    for i, (lt, rt) in enumerate(pairs, start=1):
                        t.cell(i, 0).text = lt
                        t.cell(i, 1).text = rt
                    doc.add_paragraph("")
                else:
                    # ❌ not bilingual: treat as normal two-column reading order
                    left_only = [l for (l, r) in pairs if l]
                    right_only = [r for (l, r) in pairs if r]

                    doc.add_paragraph("Text:")
                    # left column first, then right column
                    add_paragraphs(doc, left_only)
                    if right_only:
                        doc.add_paragraph("")  # spacer
                        add_paragraphs(doc, right_only)

            else:
                # Single-column: paragraphs
                doc.add_paragraph("Text:")
                text_lines = []
                for line in lines:
                    text_lines.append(normalize_spaces(" ".join(w.get("text", "") for w in line)))
                text_lines = [ln for ln in text_lines if ln]
                add_paragraphs(doc, text_lines)

            doc.add_page_break()

    doc.save(str(out_docx))


# =========================
# RUN
# =========================
pdfs = sorted([p for p in PDF_DIR.glob("*.pdf") if p.is_file()])
print("PDFs found:", len(pdfs))
print("Output folder:", OUT_PDF)

ok = fail = 0
for p in pdfs:
    out = OUT_PDF / safe_filename(p.stem + ".docx")
    try:
        extract_pdf_to_docx(p, out)
        ok += 1
    except Exception as e:
        fail += 1
        print("FAIL:", p.name, "->", repr(e))

print(f"Done. ok={ok} fail={fail}")
print("DOCX output:", OUT_PDF)
