# -*- coding: utf-8 -*-
"""
ANALYSE-SKRIPT mit Risikokategorien (A–M)
reads:  C:/Users/SHTANDO/Desktop/KI Risko/gescrapte_Artikel-Links/gescrapte_links_<rohst>.csv
writes: C:/Users/SHTANDO/Desktop/KI Risko/Analyse_Tabelle/analyse_tabelle_<rohst>.csv
"""

import os
import csv
import random
from pathlib import Path
import openai
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --------------------------------------------------
# 0) Paths
# --------------------------------------------------
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_für_Einstellungen_der_Suche"
SCRAPED_DIR = PROJECT_ROOT / "gescrapte_Artikel-Links"
OUTPUT_DIR = PROJECT_ROOT / "Analyse_Tabelle"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

WRITE_DEBUG = True  # keep rows even if all “Not in Text”

# --------------------------------------------------
# 1) OpenAI setup
# --------------------------------------------------
client = openai.AzureOpenAI(
    api_version="2024-06-01",
    azure_endpoint="https://genai-nexus.api.corpinter.net/apikey/",
    api_key=os.getenv("OPENAI_API_KEY"),
)

# --------------------------------------------------
# 2) Read settings
# --------------------------------------------------
rohst = rawm = max_datum = None
seitenumfang = 3

def lese_einstellungen():
    global rohst, rawm, max_datum, seitenumfang
    settings_path = SETTINGS_DIR / "Einstellungen_Analyse.csv"
    with settings_path.open("r", encoding="utf-8-sig") as f:
        rows = list(csv.reader(f, delimiter=","))
    def get_row(idx, default=""):
        return rows[idx][0].strip() if len(rows) > idx and len(rows[idx]) > 0 else default
    rohst        = get_row(5, "Stahl")
    rawm         = get_row(8, "Steel")
    max_datum    = get_row(11, "20.11.2020")
    seitenumfang = int(get_row(14, "3") or "3")

# --------------------------------------------------
# 3) Chrome driver
# --------------------------------------------------
def make_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    try:
        return webdriver.Chrome(options=opts)
    except Exception:
        opts.add_argument("--headless")
        return webdriver.Chrome(options=opts)

# --------------------------------------------------
# 4) Extract article text
# --------------------------------------------------
def extract_with_gpt(full_text: str, fallback: str = "") -> str:
    prompt = (
        "Extract only the main news/article body from this webpage text. "
        "Ignore navigation, ads, cookie banners, and unrelated content.\n\n" + full_text
    )
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You extract clean article text from messy HTML."},
                {"role": "user", "content": prompt},
            ],
            temperature=0
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return fallback or full_text

def url_zu_artikeltext(url: str) -> str:
    d = make_driver()
    try:
        d.get(url)
        try:
            WebDriverWait(d, 5).until(EC.presence_of_element_located((By.TAG_NAME, "article")))
        except Exception:
            pass
        html = d.page_source
    finally:
        try: d.quit()
        except Exception: pass

    soup = BeautifulSoup(html, "html.parser")
    art = soup.find("article")
    if art:
        return art.get_text(separator="\n", strip=True)

    body = soup.find("body")
    if body:
        text = body.get_text(separator="\n", strip=True)
        return extract_with_gpt(text, text)

    text = soup.get_text(separator="\n", strip=True)
    return extract_with_gpt(text, text)

# --------------------------------------------------
# 5) GPT analysis with category codes (with labels)
# --------------------------------------------------
RISIKOKATEGORIEN = {
    "A": "Working conditions, including occupational health and safety",
    "B": "Child labour",
    "C": "Modern slavery, including forced labour",
    "D": "Community and indigenous peoples’ rights",
    "E": "Excessive violence by private and public security forces",
    "F": "Environmental risks with impact on human rights",
    "G": "Business conduct in Conflict and High Risk Areas",
    "H": "Serious human rights abuses",
    "I": "Biodiversity",
    "J": "Water",
    "K": "Air",
    "L": "Soil",
    "M": "Waste, hazardous substances, and plant safety",
}

def expand_categories(raw_cats: str) -> str:
    if not raw_cats or raw_cats.lower().startswith("not in text"):
        return raw_cats

    parts = [p.strip().upper() for p in raw_cats.replace(";", ",").split(",") if p.strip()]
    seen, expanded = set(), []
    for p in parts:
        if p not in seen:
            seen.add(p)
            label = RISIKOKATEGORIEN.get(p, "")
            if label:
                expanded.append(f"{p} ({label})")
            else:
                expanded.append(p)
    return ", ".join(expanded)

def analyse_artikel(rohstoff_name: str, artikeltext: str, temperature: float = 0.1) -> list[str]:
    prompt = f"""
Analysiere den folgenden Zeitungsartikel über den Rohstoff "{rohstoff_name}" 
und beantworte GENAU 13 Punkte. Wenn eine Information fehlt, schreibe "Not in Text".

1) Zutreffende Risikokategorien (Buchstaben A–M, z. B. "A, F, J")
2) Arten der Menschenrechts-/Umweltschädigungen
3) Kurze Begründung (max. 4 Sätze)
4) Jahr(e)
5) Land/Länder
6) Ort(e)
7) Stufe der Rohstoffgewinnung
8) Involvierte Unternehmen
9) Betroffene Personengruppen/Ökosysteme
10) Mindestzahl der betroffenen Personen (Integer)
11) Begründung für die Mindestzahl (1–2 Sätze)
12) Schwere (minor/moderate/major/critical)
13) Kurze Begründung für die Schwere

Artikel:
{artikeltext}
"""
    resp = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "Du bist ein Menschenrechts- und Nachhaltigkeitsexperte."},
            {"role": "user", "content": prompt},
        ],
        temperature=temperature
    )
    txt = resp.choices[0].message.content.strip()
    lines = [ln.strip() for ln in txt.split("\n") if ln.strip()]

    if len(lines) < 13:
        lines += ["Not in Text"] * (13 - len(lines))
    lines = lines[:13]

    lines[0] = expand_categories(lines[0])
    return lines

# --------------------------------------------------
# 6) CSV helpers
# --------------------------------------------------
def lese_links_aus_scrape_csv(rohstoff: str) -> list[str]:
    fpath = SCRAPED_DIR / f"gescrapte_links_{rohstoff}.csv"
    if not fpath.exists():
        raise FileNotFoundError(f"Scrape CSV not found: {fpath}")
    links = []
    with fpath.open("r", encoding="utf-8-sig") as f:
        r = csv.reader(f, delimiter=";")
        next(r, None)
        for row in r:
            if row and row[0].strip():
                links.append(row[0].strip())
    return list(dict.fromkeys(links))

def schreibe_analyse_csv(rohstoff: str, daten_zeilen: list[list[str]]):
    out_path = OUTPUT_DIR / f"analyse_tabelle_{rohstoff}.csv"
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow([
            "link",
            "Risikokategorien (A–M)",
            "Arten der Menschenrechts-/Umweltschädigungen",
            "Begründung",
            "Jahr(e)",
            "Land/Länder",
            "Ort(e)",
            "Stufe der Rohstoffgewinnung",
            "Involvierte Unternehmen",
            "Betroffene Personengruppen/Ökosysteme",
            "Mindestzahl der betroffenen Personen",
            "Begründung Mindestzahl",
            "Schwere",
            "Begründung Schwere",
        ])
        for row in daten_zeilen:
            if not WRITE_DEBUG:
                all_not = all(
                    (isinstance(x, str) and x.strip().lower() == "not in text")
                    for x in row[1:]
                )
                if all_not:
                    continue
            w.writerow(row)
    print(f"✅ Analyse geschrieben nach: {out_path}")

# --------------------------------------------------
# 7) Main process
# --------------------------------------------------
if __name__ == "__main__":
    lese_einstellungen()
    print(f"SETTINGS:\n  rohst     = {rohst}\n  rawm      = {rawm}\n  max_datum = {max_datum}")

    links = lese_links_aus_scrape_csv(rohst)
    print("Links gefunden:", len(links))

    # Analyse ALL links (no sampling)
    test_links = links
    print(f"Wir analysieren ALLE {len(test_links)} Links.")

    rows, failed = [], []

    for i, link in enumerate(test_links, start=1):
        print(f"[{i}/{len(test_links)}] analysiere: {link}")
        try:
            artikel = url_zu_artikeltext(link)

            # ✅ MANDATORY MATERIAL GATE
            if rawm and rawm.strip() and (rawm.lower() not in (artikel or "").lower()):
                print("  ⛔ skipped (required material not in body text)")
                continue

            analysed = analyse_artikel(rawm, artikel, temperature=0.1)
            cleaned = [x.replace("\n", " ").replace("\r", " ").strip(" -") for x in analysed]
            rows.append([link] + cleaned)
            print("  ✅ done")
        except Exception as e:
            print("  ❌ error:", e)
            failed.append(link)

    schreibe_analyse_csv(rohst, rows)

    if failed:
        fail_path = OUTPUT_DIR / f"fehlgeschlagene_urls_analyse_{rohst}.csv"
        with fail_path.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f, delimiter=";")
            w.writerow(["link"])
            for u in failed:
                w.writerow([u])
        print(f"⚠️ Fehlgeschlagen: {len(failed)} → {fail_path}")
