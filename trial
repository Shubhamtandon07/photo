# -*- coding: utf-8 -*-
# ============================================================
# KI Risiko ‚Äì STRICT material-based HR / ENV scraper (v3)
# Changes requested:
#   1) Live progress: show TOTAL accepted so far while running
#   2) Fallback: if after (user+extra sites) total accepted < 5,
#      then search the open web (no site:) until TOTAL accepted reaches 10,
#      then stop.
#
# IMPORTANT:
# - No per-site limit anymore for the site-based phase (you asked: "no limit").
# - Still keeps strict filters: required material + risk terms + noise filters.
# ============================================================

import os
import csv
import time
import re
from pathlib import Path
from urllib.parse import quote_plus, urlparse

import dateparser
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# 1) PATHS / CONFIG
# ============================================================

PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")

SETTINGS_DIR = PROJECT_ROOT / "Textdoks_f√ºr_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV   = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV      = SETTINGS_DIR / "Schlagw√∂rter.csv"

# DuckDuckGo paging per query
MAX_DDG_PAGES = 2
HEADLESS = False

# How many keywords from Schlagw√∂rter.csv to use per site
MAX_KEYWORDS_PER_SITE = 6

# Fallback logic
FALLBACK_TRIGGER_MIN_ACCEPTED = 5     # if total accepted after site phase < 5
FALLBACK_STOP_AT_TOTAL = 10           # stop once total accepted reaches 10


# ============================================================
# 2) RISK TERMS (EXTENDED)
# ============================================================

RISK_TERMS_BASE = [
    # general HR
    "human rights", "violation", "abuse",
    "forced labour", "forced labor",
    "child labour", "child labor",
    "underage", "modern slavery",
    "unsafe working conditions", "hazardous work", "worker exploitation",
    "wage theft", "trafficking",

    # discrimination
    "discrimination", "age discrimination", "racial discrimination", "religious discrimination",
    "race", "religion", "age",

    # trade union freedoms
    "trade union", "union rights", "union busting", "collective bargaining", "right to strike",

    # environment / env-human-rights intersection
    "pollution", "environmental damage", "contamination", "toxic waste", "spill",
    "tailings", "tailings dam", "deforestation",
    "community conflict", "protest", "displacement",
]

BAD_DOMAIN_SUBSTRINGS = [
    "facebook.com", "m.facebook.com",
    "youtube.com", "youtu.be",
    "instagram.com",
    "tiktok.com",
    "twitter.com", "x.com",
    "pinterest.",
]

BAD_URL_HINTS = [
    "/jobs", "/job", "/careers", "/career", "/karriere", "/stellen", "/vacancies",
    "/investor", "/investors", "/ir", "/investor-relations",
    "/stock", "/shares", "/quote", "/pricing",
    "/privacy", "/cookie", "/cookies", "/terms",
]

BAD_FILE_EXTS = (".pdf", ".doc", ".docx", ".ppt", ".pptx")


# ============================================================
# 3) EXTRA TRUSTWORTHY SITES (appended after your Bekannte_Seiten)
# ============================================================

EXTRA_TRUSTWORTHY_SITES = [
    # NGOs / intergov
    "humanrightswatch.org",
    "amnesty.org",
    "ilo.org",
    "oecd.org",
    "ohchr.org",
    "un.org",
    "unep.org",
    "worldbank.org",

    # journalism
    "reuters.com",
    "apnews.com",
    "bbc.co.uk",
    "dw.com",
    "theguardian.com",
    "ft.com",

    # regulators
    "epa.gov",
    "ec.europa.eu",
]


# ============================================================
# 4) READ SETTINGS
# ============================================================

rohst = rawm = max_datum_str = None

def lese_einstellungen():
    global rohst, rawm, max_datum_str

    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Settings file not found: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    def safe_get(idx: int, default: str = "") -> str:
        if len(rows) > idx and rows[idx] and rows[idx][0].strip():
            return rows[idx][0].strip()
        return default

    rohst = safe_get(5, "Stahl")
    rawm = safe_get(8, "Steel")
    max_datum_str = safe_get(11, "20.11.2020")

    print("SETTINGS:")
    print("  rohst     =", rohst)
    print("  rawm      =", rawm)
    print("  max_datum =", max_datum_str)

    parsed = dateparser.parse(max_datum_str, languages=["de", "en"])
    return parsed.date() if parsed else None


# ============================================================
# 5) CSV LIST LOADERS
# ============================================================

def lese_einfache_liste(pfad: Path, delimiter: str = ",") -> list[str]:
    if not pfad.exists():
        print("‚ö†Ô∏è File not found:", pfad)
        return []
    items: list[str] = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                val = (cell or "").strip()
                if val:
                    items.append(val)
    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k not in seen:
            seen.add(k)
            uniq.append(it.strip())
    return uniq

def normalize_domain(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^https?://", "", s, flags=re.IGNORECASE)
    s = s.split("/")[0].strip()
    s = s.replace("www.", "").strip()
    return s

def merge_sites(user_sites: list[str]) -> list[str]:
    out: list[str] = []
    seen = set()

    def add_site(site: str):
        d = normalize_domain(site)
        if not d:
            return
        if d not in seen:
            seen.add(d)
            out.append(d)

    for s in user_sites:
        add_site(s)
    for s in EXTRA_TRUSTWORTHY_SITES:
        add_site(s)

    return out


# ============================================================
# 6) FILTERS
# ============================================================

def contains_required_material(text: str, material: str) -> bool:
    if not text or not material:
        return False
    t = text.lower()
    m = material.lower().strip()
    variants = {m, m.replace("-", " "), m.replace(" ", "-")}
    return any(v in t for v in variants if v)

def contains_risk_terms(text: str, extra_terms: list[str]) -> bool:
    t = (text or "").lower()

    for kw in RISK_TERMS_BASE:
        if kw.lower() in t:
            return True

    for sw in extra_terms:
        sw_clean = (sw or "").replace("+", " ").strip().lower()
        if not sw_clean:
            continue
        if sw_clean in t:
            return True
        for token in sw_clean.split():
            if len(token) >= 4 and token in t:
                return True

    return False

def is_bad_result(url: str, title: str, snippet: str) -> bool:
    if not url:
        return True

    ul = url.strip().lower()
    if any(ul.endswith(ext) for ext in BAD_FILE_EXTS):
        return True

    dom = urlparse(url).netloc.lower()
    if any(bad in dom for bad in BAD_DOMAIN_SUBSTRINGS):
        return True

    if any(hint in ul for hint in BAD_URL_HINTS):
        return True

    combined = f"{title} {snippet}".lower()
    if "policy" in combined and "violation" not in combined and "abuse" not in combined:
        return True

    return False


# ============================================================
# 7) SELENIUM
# ============================================================

def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return webdriver.Chrome(options=opts)


# ============================================================
# 8) DUCKDUCKGO QUERIES / SCRAPING
# ============================================================

def ddg_search_url(query: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(query) + "&kp=1&ia=web"

def build_queries_for_site(domain: str, rawm: str, schlagwoerter: list[str]) -> list[str]:
    base_queries = [
        f"site:{domain} {rawm} human rights violation",
        f"site:{domain} {rawm} forced labour",
        f"site:{domain} {rawm} child labour",
        f"site:{domain} {rawm} pollution spill contamination",
        f"site:{domain} {rawm} discrimination trade union collective bargaining",
        f"site:{domain} {rawm} unsafe working conditions hazardous work",
    ]

    for sw in schlagwoerter[:MAX_KEYWORDS_PER_SITE]:
        sw_clean = (sw or "").replace("+", " ").strip()
        if sw_clean:
            base_queries.append(f"site:{domain} {rawm} {sw_clean}")

    return [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base_queries]

def build_fallback_queries(rawm: str, schlagwoerter: list[str]) -> list[str]:
    """
    Open-web queries WITHOUT site: restriction.
    Keep strict material + risk intent, but broaden reach.
    """
    qs = [
        f"{rawm} human rights violation",
        f"{rawm} forced labour",
        f"{rawm} child labour",
        f"{rawm} union rights violation",
        f"{rawm} trade union freedoms",
        f"{rawm} discrimination workers",
        f"{rawm} pollution contamination spill",
        f"{rawm} toxic waste",
        f"{rawm} hazardous work unsafe working conditions",
        f"{rawm} lawsuit labour abuse",
    ]

    # add a few of your keywords (cleaned)
    for sw in schlagwoerter[:max(6, MAX_KEYWORDS_PER_SITE)]:
        sw_clean = (sw or "").replace("+", " ").strip()
        if sw_clean:
            qs.append(f"{rawm} {sw_clean}")

    return [re.sub(r'[\u201C\u201D\"\']', "", q) for q in qs]

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    items: list[dict] = []
    try:
        driver.get(search_url)
    except Exception:
        return items

    pages_seen = 0
    while pages_seen < max_pages:
        time.sleep(2)

        html = (driver.page_source or "").lower()
        if "filter l√∂schen" in html or "try again" in html:
            break

        try:
            WebDriverWait(driver, 8).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article[data-testid="result"]'))
            )
        except Exception:
            soup = BeautifulSoup(driver.page_source, "html.parser")
            txt = soup.get_text(" ", strip=True).lower()
            if "no results" in txt or "keine ergebnisse" in txt:
                break
            break

        articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for art in articles:
            try:
                a = art.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
                url = (a.get_attribute("href") or "").strip()
                title = (a.text or "").strip()
                if not url or "duckduckgo.com" in url:
                    continue
            except Exception:
                continue

            snippet = ""
            try:
                snippet = (art.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']").text or "").strip()
            except Exception:
                pass

            date_txt = ""
            try:
                date_txt = (art.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq").text or "").strip()
            except Exception:
                pass

            items.append({"url": url, "title": title, "snippet": snippet, "date_txt": date_txt})

        try:
            more_btn = WebDriverWait(driver, 2).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", more_btn)
            time.sleep(0.8)
            more_btn.click()
            pages_seen += 1
            time.sleep(1.2)
        except Exception:
            break

    return items


# ============================================================
# 9) DATE CHECK (permissive)
# ============================================================

def is_date_ok(date_txt: str, min_date, _url: str) -> bool:
    if min_date is None:
        return True
    if not date_txt:
        return True
    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        return True
    return parsed.date() >= min_date


# ============================================================
# 10) WRITE HELPERS
# ============================================================

def write_header(out_path: Path):
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow(
            ["gefundener link", "datum", "seite", "suche", "titel", "snippet"]
        )

def append_row(out_path: Path, url: str, date_txt: str, site: str, query: str, title: str, snippet: str):
    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow([url, date_txt, site, query, title, snippet])


# ============================================================
# 11) MAIN
# ============================================================

if __name__ == "__main__":
    min_date = lese_einstellungen()

    user_sites_raw = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    schlagwoerter  = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    sites = merge_sites(user_sites_raw)

    print("\nUser sites found:", len(user_sites_raw))
    print("Total sites used (user + extras):", len(sites))
    print("Material strict filter (rawm):", rawm)
    print("Keywords loaded:", len(schlagwoerter))

    out_path = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"
    write_header(out_path)

    global_seen_urls = set()
    total_accepted = 0

    driver = make_driver()

    try:
        # ---------------------------
        # PHASE 1: site-restricted
        # (NO per-site limit)
        # ---------------------------
        for idx_site, domain in enumerate(sites, start=1):
            print(f"\nüåê Site {idx_site}/{len(sites)}: {domain} | accepted so far = {total_accepted}")

            queries = build_queries_for_site(domain, rawm, schlagwoerter)
            for q_idx, q in enumerate(queries, start=1):
                search_url = ddg_search_url(q)
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)

                for res in results:
                    url = res["url"]
                    title = res["title"]
                    snippet = res["snippet"]
                    date_txt = res.get("date_txt", "")

                    if not url or url in global_seen_urls:
                        continue
                    if is_bad_result(url, title, snippet):
                        continue
                    if not is_date_ok(date_txt, min_date, url):
                        continue

                    combined = f"{title} {snippet}".strip()
                    if not contains_required_material(combined, rawm):
                        continue
                    if not contains_risk_terms(combined, schlagwoerter):
                        continue

                    # accept
                    global_seen_urls.add(url)
                    total_accepted += 1
                    append_row(out_path, url, date_txt, domain, q, title, snippet)

                    print(f"   ‚úÖ ACCEPTED (total={total_accepted}) | {title}")

                time.sleep(1.0)

        # ---------------------------
        # PHASE 2: fallback open web
        # Trigger: total_accepted < 5
        # Stop: when total_accepted reaches 10
        # ---------------------------
        if total_accepted < FALLBACK_TRIGGER_MIN_ACCEPTED:
            print(
                f"\n‚ö†Ô∏è Low yield after site phase: total accepted = {total_accepted} "
                f"(< {FALLBACK_TRIGGER_MIN_ACCEPTED}). Starting open-web fallback..."
            )

            fb_queries = build_fallback_queries(rawm, schlagwoerter)

            for q_idx, q in enumerate(fb_queries, start=1):
                if total_accepted >= FALLBACK_STOP_AT_TOTAL:
                    print(f"‚úÖ Fallback stop reached: total accepted = {total_accepted}")
                    break

                search_url = ddg_search_url(q)
                print(f"\nüåç Fallback query {q_idx}/{len(fb_queries)} | accepted so far = {total_accepted}")

                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                for res in results:
                    if total_accepted >= FALLBACK_STOP_AT_TOTAL:
                        break

                    url = res["url"]
                    title = res["title"]
                    snippet = res["snippet"]
                    date_txt = res.get("date_txt", "")

                    if not url or url in global_seen_urls:
                        continue
                    if is_bad_result(url, title, snippet):
                        continue
                    if not is_date_ok(date_txt, min_date, url):
                        continue

                    combined = f"{title} {snippet}".strip()
                    if not contains_required_material(combined, rawm):
                        continue
                    if not contains_risk_terms(combined, schlagwoerter):
                        continue

                    global_seen_urls.add(url)
                    total_accepted += 1
                    append_row(out_path, url, date_txt, "OPEN_WEB", q, title, snippet)

                    print(f"   ‚úÖ ACCEPTED (total={total_accepted}) | {title}")

                time.sleep(1.0)

        print(f"\n‚úÖ Done. Total accepted = {total_accepted}")
        print("Output written to:", out_path)

    finally:
        try:
            driver.quit()
        except Exception:
            pass
