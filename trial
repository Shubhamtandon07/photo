# -*- coding: utf-8 -*-
"""
Export Outlook training data from .msg files (and optionally .html/.htm files) into:
- 1 output file per input file (NO extra split files)
- subject + a cleaned/sanitized body only
- removes headers (Von/An/Cc/Gesendet/Betreff etc.), signatures, disclaimers
- redacts sensitive info (names, emails, phones, addresses, money, IDs, company/supplier)
- labels the mail as question/answer/mixed using Azure OpenAI (optional) with a safe fallback

IMPORTANT NOTES
1) .msg parsing in Python 3.14 is often unreliable depending on installed wheels.
   This script will:
   - try extract_msg if available,
   - else skip .msg (and still process .html/.htm).
2) You asked to ignore images (.png/.jpg) for now => attachments are NOT parsed here.
3) OneDrive "online-only" files can appear as 0 bytes. Ensure the folder is locally available.

RUN:
  python sanitize_emails.py
or in Jupyter/VSCode notebook:
  %run sanitize_emails.py

CONFIG:
  Set MSG_DIR and OUT_DIR below.
  Put OPENAI_API_KEY in .env (same as your bot project).
"""

import os
import re
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, List

from dotenv import load_dotenv

# =========================
# EDIT THESE
# =========================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

# Also process standalone html/htm files in the folder
PROCESS_HTML_FILES = True

# LLM labeling (recommended). If blocked by policy, set False.
USE_LLM_LABELING = True

# Azure OpenAI (same pattern as your earlier bot)
AZURE_OPENAI_ENDPOINT = "https://genai-nexus.api.corpinter.net/apikey/"
AZURE_API_VERSION = "2024-06-01"
AZURE_DEPLOYMENT_NAME = "gpt-4o"

# Output format: "txt" or "html"
OUTPUT_FORMAT = "html"  # "txt" also supported

# =========================
# LOAD ENV
# =========================
load_dotenv()


# =========================
# OPTIONAL IMPORTS
# =========================
HAVE_EXTRACT_MSG = False
try:
    import extract_msg  # type: ignore
    HAVE_EXTRACT_MSG = True
except Exception:
    HAVE_EXTRACT_MSG = False

HAVE_AZURE = False
try:
    from openai import AzureOpenAI  # type: ignore
    HAVE_AZURE = True
except Exception:
    HAVE_AZURE = False


# =========================
# UTIL
# =========================
def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def safe_hash(s: str) -> str:
    return hashlib.sha1((s or "").encode("utf-8", errors="ignore")).hexdigest()[:8]


def clean_ws(s: str) -> str:
    return re.sub(r"[ \t]+", " ", (s or "")).strip()


def normalize_newlines(s: str) -> str:
    s = (s or "").replace("\r\n", "\n").replace("\r", "\n")
    # strip weird spaced HTML like "h t m l"
    s = re.sub(r"(?:\b\w\b\s+){6,}", lambda m: m.group(0).replace(" ", ""), s)
    return s


def strip_html(html: str) -> str:
    html = html or ""
    html = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html)
    html = re.sub(r"(?is)<br\s*/?>", "\n", html)
    html = re.sub(r"(?is)</p\s*>", "\n\n", html)
    html = re.sub(r"(?is)<[^>]+>", " ", html)
    html = html.replace("&nbsp;", " ")
    html = html.replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
    html = re.sub(r"[ \t]+", " ", html)
    html = re.sub(r"\n{3,}", "\n\n", html)
    return html.strip()


# =========================
# SENSITIVE PATTERNS (broad + office-like)
# =========================
EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b[A-Z0-9.-]+\.(?:com|de|net|org|info|eu|io|co|uk)\b", re.I)

PHONE_RE = re.compile(
    r"(?<!\w)(?:\+|00)?\d{1,3}[\s\-]?(?:\(?\d{2,5}\)?[\s\-]?)?(?:\d[\s\-]?){6,}\d(?!\w)"
)

IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}[ ]?(?:[A-Z0-9][ ]?){11,30}\b", re.I)

MONEY_RE = re.compile(
    r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b|\b€\s*\d[\d\.\,\s]*\b"
)

REF_RE = re.compile(
    r"(?i)\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|Supplier|Portal|Round|ID|Ref|SP|Project|V\d{2,4})"
    r"\s*[:#]?\s*[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b"
)

ADDRESS_RE = re.compile(
    r"(?i)\b[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,3}\s+"
    r"(Straße|Strasse|Str\.|Weg|Allee|Platz|Ring|Gasse|Damm|Ufer|Road|Street|St\.|Ave|Avenue|Lane|Blvd|Boulevard)\s+"
    r"\d{1,5}[a-zA-Z]?\b"
)

TITLE_NAME_RE = re.compile(
    r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b"
)

# Greeting name cues: "Hallo X", "Hi X", "Guten Morgen X", "Dear X", "Hello X"
GREETING_NAME_RE = re.compile(
    r"(?im)^\s*(?:hallo|hi|hello|dear|guten\s+morgen|guten\s+tag|guten\s+abend|moin)\s+"
    r"([A-ZÄÖÜ][^\n,]{1,40})(?:,|\s*$)"
)

# Signature name cues: "VG X", "LG X", "BR X", "Best regards X", "Mit freundlichen Grüßen X"
SIGNOFF_NAME_RE = re.compile(
    r"(?im)^\s*(?:vg|lg|br|mfg|mit\s+freundlichen\s+grüßen|best\s+regards|kind\s+regards|with\s+best\s+regards)\s*[,/]*\s*"
    r"([A-ZÄÖÜ][^\n]{1,60})\s*$"
)

# Heuristic full names (stronger than your old one; still heuristic)
NAME_STRONG_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}(?:\s+[A-ZÄÖÜ][a-zäöüß]{2,})?\b")

# Company/supplier cues
COMPANY_CUES_RE = re.compile(
    r"(?i)\b(?:gmbh|ag|inc\.?|ltd\.?|llc|kg|se|sarl|bv|oy|spa|s\.p\.a\.|co\.?|company|lieferant|supplier)\b"
)

# Anything like "Mercedes-Benz ..." etc. You can extend
COMPANY_TERMS = [
    "Mercedes-Benz", "Mercedes Benz", "Mercedes", "Daimler", "MB", "Mercedes-Benz AG"
]


# =========================
# HEADER / DISCLAIMER REMOVAL
# =========================
HEADER_LINES_RE = re.compile(
    r"(?im)^(von|from|an|to|cc|bcc|gesendet|sent|betreff|subject|datum|date)\s*:\s*.*$"
)

FORWARD_QUOTE_SPLIT_RE = re.compile(
    r"(?im)^\s*(von|from)\s*:\s+.+$|^\s*-----\s*original message\s*-----\s*$"
)

CONFIDENTIALITY_RE = re.compile(
    r"(?is)(this message.*confidential|vertraulich|disclaimer|rechtlicher hinweis|"
    r"please consider the environment|bitte denken sie an die umwelt).{0,2000}$"
)

# typical corporate signature blocks (keep it aggressive)
SIGNATURE_BLOCK_RE = re.compile(
    r"(?is)\n(?:mit freundlichen grüßen|best regards|kind regards|with best regards|vg|lg|br|mfg)\b.*$"
)

MAILTO_REMAINS_RE = re.compile(r"(?i)<mailto:[^>]+>")


def remove_headers_and_noise(text: str) -> str:
    """
    Keep only body-like content:
    - remove header lines
    - cut off long quoted history if needed (but keep current message + some context)
    - remove disclaimers/signatures aggressively
    """
    t = normalize_newlines(text)

    # remove "mailto:" artifacts and HTML leftovers
    t = MAILTO_REMAINS_RE.sub(" ", t)
    t = re.sub(r"(?i)mailto\s*:\s*\S+", " ", t)

    # delete header lines anywhere
    t = HEADER_LINES_RE.sub("", t)

    # remove confidentiality tail
    t = CONFIDENTIALITY_RE.sub("", t)

    # remove excessive whitespace
    t = re.sub(r"\n{3,}", "\n\n", t)

    # Try to keep the newest human text:
    # If there is a clear "From:" block marking quoted chain, keep everything ABOVE it (newest part)
    m = FORWARD_QUOTE_SPLIT_RE.search(t)
    if m:
        # keep only the part above the first quoted header
        t = t[: m.start()].strip()

    # remove signature block (last)
    t = SIGNATURE_BLOCK_RE.sub("", t).strip()

    # cleanup
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t).strip()

    return t


# =========================
# REDACTION (delete, not replace with [PERSON])
# =========================
def redact_all_sensitive(text: str) -> Tuple[str, dict]:
    """
    Deletes sensitive content.
    Returns redacted text + stats.
    """
    t = normalize_newlines(text)

    stats = {
        "emails": 0, "urls": 0, "domains": 0, "phones": 0, "iban": 0,
        "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
    }

    def sub_count(regex: re.Pattern, repl: str, key: str, s: str) -> str:
        found = list(regex.finditer(s))
        if found:
            stats[key] += len(found)
        return regex.sub(repl, s)

    # remove explicit company terms
    for term in COMPANY_TERMS:
        if term:
            before = t
            t = re.sub(re.escape(term), "", t, flags=re.I)
            if t != before:
                stats["companies"] += 1

    t = sub_count(URL_RE, "", "urls", t)
    t = sub_count(WWW_RE, "", "urls", t)
    t = sub_count(EMAIL_RE, "", "emails", t)
    t = sub_count(DOMAIN_RE, "", "domains", t)
    t = sub_count(PHONE_RE, "", "phones", t)
    t = sub_count(IBAN_RE, "", "iban", t)
    t = sub_count(MONEY_RE, "", "money", t)
    t = sub_count(REF_RE, "", "refs", t)
    t = sub_count(ADDRESS_RE, "", "addresses", t)

    # remove greetings/signoffs names
    t = GREETING_NAME_RE.sub(lambda m: m.group(0).replace(m.group(1), "").rstrip() + "\n", t)
    t = SIGNOFF_NAME_RE.sub(lambda m: m.group(0).replace(m.group(1), "").rstrip(), t)

    # remove titles + names
    before = t
    t = TITLE_NAME_RE.sub("", t)
    if t != before:
        stats["names"] += 1

    # strong name heuristic
    t2 = NAME_STRONG_RE.sub("", t)
    if t2 != t:
        # approximate count (not exact after sub)
        stats["names"] += 1
        t = t2

    # company cue words: if line looks like org signature, remove line
    lines = []
    for ln in t.split("\n"):
        if COMPANY_CUES_RE.search(ln) and len(ln) < 180:
            stats["companies"] += 1
            continue
        lines.append(ln)
    t = "\n".join(lines)

    # final whitespace cleanup
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"(\n\s*){3,}", "\n\n", t)
    t = t.strip()

    # if nothing left, keep empty
    return t, stats


# =========================
# MSG / HTML READING
# =========================
def read_msg_with_extract_msg(path: Path) -> Tuple[str, str]:
    """
    Returns (subject, body_text). Raises if parse fails.
    """
    m = extract_msg.Message(str(path))
    m.process()

    subject = (getattr(m, "subject", "") or "").strip()

    # prefer htmlBody if present
    html_body = getattr(m, "htmlBody", None)
    body = getattr(m, "body", None)
    rtf = getattr(m, "rtfBody", None)

    best = ""
    if html_body and str(html_body).strip():
        best = strip_html(str(html_body))
    elif body and str(body).strip():
        best = str(body)
    elif rtf and str(rtf).strip():
        # rtf is not reliably convertible here; keep as-is
        best = str(rtf)

    best = normalize_newlines(best)
    return subject, best


def read_html_file(path: Path) -> Tuple[str, str]:
    raw = path.read_text(encoding="utf-8", errors="ignore")
    txt = strip_html(raw)
    subject = path.stem
    return subject, txt


# =========================
# LLM LABELING (question/answer/mixed/unknown)
# =========================
def build_azure_client():
    if not HAVE_AZURE:
        return None
    key = os.getenv("OPENAI_API_KEY")
    if not key:
        return None
    return AzureOpenAI(
        api_version=AZURE_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=key,
    )


def heuristic_label(text: str) -> str:
    """
    Simple fallback: classify based on cues.
    """
    s = (text or "").strip()
    if not s:
        return "unknown"

    # counts
    qmarks = s.count("?")
    req = len(re.findall(r"(?i)\b(bitte|can you|could you|kannst du|könnt ihr|please|request)\b", s))
    ans = len(re.findall(r"(?i)\b(danke|here are|unten|following|wir plädieren|we propose|we suggest|"
                         r"anmerkungen|my comments|info|in summary|next steps)\b", s))

    if ans >= 2 and (qmarks <= 1 and req <= 1):
        return "answer"
    if qmarks >= 2 or req >= 2:
        return "question"
    if ans >= 1 and (qmarks >= 1 or req >= 1):
        return "mixed"
    # default
    return "answer" if ans >= 1 else "unknown"


def llm_label_segment(client, subject: str, prev_seg: str, seg: str, next_seg: str) -> str:
    """
    Return exactly: question | answer | mixed | unknown
    """
    if not client:
        return "unknown"

    def clip(x: str, n: int = 2500) -> str:
        x = (x or "").strip()
        return (x[:n] + "…") if len(x) > n else x

    prompt = f"""
You label segments inside one email thread.
Return EXACTLY ONE token: question OR answer OR mixed OR unknown.

Definitions:
- question: mostly asking for info/action/decision/confirmation (supplier/colleague requests).
- answer: mostly providing information/assessment/decision/next steps.
  If it also contains ONE short follow-up question, still label as answer.
- mixed: balanced asking + answering, or multiple questions + multiple answers.
- unknown: too little usable content.

Important:
- Older segments are earlier in the thread; newer segments are later.
- Do not get fooled by greetings/signatures; focus on the core statements.

Thread subject:
{clip(subject, 400)}

Previous segment (older):
{clip(prev_seg)}

Current segment:
{clip(seg)}

Next segment (newer):
{clip(next_seg)}

Return exactly one token.
""".strip()

    try:
        r = client.chat.completions.create(
            model=AZURE_DEPLOYMENT_NAME,
            temperature=0.0,
            messages=[
                {"role": "system", "content": "Output only one token: question/answer/mixed/unknown."},
                {"role": "user", "content": prompt},
            ],
        )
        out = (r.choices[0].message.content or "").strip().lower()
        if out in {"question", "answer", "mixed", "unknown"}:
            return out
        return "unknown"
    except Exception:
        return "unknown"


# =========================
# THREAD SEGMENTATION (keeps 1 file output, but labels within the chain)
# =========================
REPLY_SPLIT_RE = re.compile(
    r"(?im)^\s*(?:von|from)\s*:\s+.+$|^\s*gesendet\s*:\s+.+$|^\s*-----\s*original message\s*-----\s*$"
)

def split_thread_into_segments(clean_body: str) -> List[str]:
    """
    Split a thread into segments at "From:/Von:" boundaries.
    Returns list in chronological order (oldest first).
    """
    t = normalize_newlines(clean_body).strip()
    if not t:
        return []

    parts = REPLY_SPLIT_RE.split(t)
    segs = [p.strip() for p in parts if p and p.strip()]

    # Heuristic: Outlook extraction often puts newest first; we want oldest->newest
    # If the first segment contains typical "thanks/response" and later contains "please", reverse.
    if len(segs) >= 2:
        first = segs[0].lower()
        last = segs[-1].lower()
        first_ans = bool(re.search(r"(?i)\b(danke|thanks|unten|anmerkungen|my comments)\b", first))
        last_req = bool(re.search(r"(?i)\b(bitte|please|can you|could you|kannst du|könnt ihr)\b", last))
        if first_ans and last_req:
            segs = list(reversed(segs))

    return segs


# =========================
# OUTPUT BUILDERS
# =========================
def to_html_page(source_name: str, label: str, subject: str, body: str, stats: dict) -> str:
    def esc(x: str) -> str:
        return (x or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

    meta = {
        "source": source_name,
        "label": label,
        "generated": now_str(),
        "subject": subject,
        "redaction_stats": stats,
    }
    meta_json = esc(json.dumps(meta, ensure_ascii=False, indent=2))

    body_html = "<br>".join(esc(line) for line in (body or "").split("\n"))

    return f"""<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>Sanitized - {esc(subject)[:80]}</title>
</head>
<body style="font-family:Segoe UI,Arial; font-size:13px; line-height:1.35; padding:16px;">
<h3 style="margin:0 0 8px 0;">Sanitized Email</h3>
<div style="color:#444; margin-bottom:10px;">
  <b>Source:</b> {esc(source_name)}<br>
  <b>Label:</b> {esc(label)}<br>
  <b>Generated:</b> {esc(now_str())}<br>
  <b>Subject:</b> {esc(subject)}
</div>
<details style="margin-bottom:12px;">
  <summary><b>Redaction stats</b></summary>
  <pre style="white-space:pre-wrap;">{meta_json}</pre>
</details>
<hr>
<div>{body_html if body_html.strip() else "<i>(no usable body text extracted)</i>"}</div>
</body>
</html>
"""


def to_txt(source_name: str, label: str, subject: str, body: str, stats: dict) -> str:
    return (
        f"Source: {source_name}\n"
        f"Label: {label}\n"
        f"Generated: {now_str()}\n"
        f"Subject: {subject}\n"
        f"RedactionStats: {json.dumps(stats, ensure_ascii=False)}\n"
        f"\n---BODY---\n"
        f"{body if body.strip() else '(no usable body text extracted)'}\n"
    )


def safe_out_name(src_path: Path, suffix: str) -> str:
    base = src_path.stem
    base = re.sub(r"[\\/:*?\"<>|]+", "_", base)
    base = re.sub(r"\s+", " ", base).strip()
    if len(base) > 80:
        base = base[:80]
    return f"{base}__{safe_hash(str(src_path))}.{suffix}"


# =========================
# MAIN
# =========================
def main():
    in_dir = Path(MSG_DIR)
    out_dir = Path(OUT_DIR)
    out_dir.mkdir(parents=True, exist_ok=True)

    # collect files
    msg_files = sorted(in_dir.rglob("*.msg")) + sorted(in_dir.rglob("*.MSG"))
    html_files = []
    if PROCESS_HTML_FILES:
        html_files = sorted(in_dir.rglob("*.html")) + sorted(in_dir.rglob("*.htm"))

    total_in = len(msg_files) + len(html_files)
    print(f"[{now_str()}] Input folder: {in_dir}")
    print(f"[{now_str()}] Output folder: {out_dir}")
    print(f"[{now_str()}] Found: {len(msg_files)} .msg + {len(html_files)} html/htm = {total_in}")

    if total_in == 0:
        print("No .msg/.MSG files found (and PROCESS_HTML_FILES did not find html/htm).")
        return

    # LLM client
    client = None
    if USE_LLM_LABELING:
        client = build_azure_client()
        if not client:
            print("LLM labeling enabled but Azure client not available (missing openai package or OPENAI_API_KEY). Falling back to heuristics.")

    errors = []
    written = 0

    def process_one(src_path: Path, kind: str):
        nonlocal written

        # read
        subject = ""
        raw_body = ""
        if kind == "msg":
            if not HAVE_EXTRACT_MSG:
                raise RuntimeError("extract_msg not installed; cannot parse .msg in this environment.")
            subject, raw_body = read_msg_with_extract_msg(src_path)
        else:
            subject, raw_body = read_html_file(src_path)

        # cleanup
        cleaned = remove_headers_and_noise(raw_body)

        # if still empty, try using html-strip directly (for weird VML cases)
        if not cleaned.strip() and kind == "msg":
            cleaned = strip_html(raw_body)

        # segment thread for labeling (but write ONE output file only)
        segments = split_thread_into_segments(cleaned)
        # If segmentation yields nothing, treat whole cleaned as one segment
        if not segments and cleaned.strip():
            segments = [cleaned.strip()]

        # redact each segment and keep only meaningful ones (>20 chars)
        seg_out = []
        seg_stats_agg = {
            "emails": 0, "urls": 0, "domains": 0, "phones": 0, "iban": 0,
            "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
        }

        for seg in segments:
            red, st = redact_all_sensitive(seg)
            # aggregate stats
            for k in seg_stats_agg:
                seg_stats_agg[k] += int(st.get(k, 0))
            if len(red.strip()) >= 20:
                seg_out.append(red.strip())

        # if everything got nuked, keep a very small fallback
        if not seg_out:
            body_final = ""
            label_final = "unknown"
            stats_final = seg_stats_agg
        else:
            # label each segment (optional) and build a single body
            labels = []
            if client:
                for i in range(len(seg_out)):
                    prev_seg = seg_out[i - 1] if i > 0 else ""
                    cur_seg = seg_out[i]
                    next_seg = seg_out[i + 1] if (i + 1) < len(seg_out) else ""
                    lab = llm_label_segment(client, subject, prev_seg, cur_seg, next_seg)
                    if lab == "unknown":
                        lab = heuristic_label(cur_seg)
                    labels.append(lab)
            else:
                labels = [heuristic_label(x) for x in seg_out]

            # overall label: if any answer segments exist, prefer answer unless mostly questions
            q = sum(1 for x in labels if x == "question")
            a = sum(1 for x in labels if x == "answer")
            m = sum(1 for x in labels if x == "mixed")

            if a >= max(q, m) and a >= 1:
                label_final = "answer"
            elif q >= max(a, m) and q >= 1:
                label_final = "question"
            elif m >= 1:
                label_final = "mixed"
            else:
                label_final = labels[0] if labels else "unknown"

            # build single body (chronological, oldest->newest)
            # add tags per segment inside the one file:
            blocks = []
            for lab, seg in zip(labels, seg_out):
                tag = "Q" if lab == "question" else ("A" if lab == "answer" else "M")
                blocks.append(f"[{tag}] {seg}")
            body_final = "\n\n".join(blocks).strip()
            stats_final = seg_stats_agg

        # final subject sanitization: remove sensitive terms, keep readable
        subj_clean = subject.strip() if subject else src_path.stem
        subj_clean = redact_all_sensitive(subj_clean)[0]
        subj_clean = clean_ws(subj_clean)[:160] if subj_clean else "Sanitized"

        # write
        if OUTPUT_FORMAT.lower() == "txt":
            out_name = safe_out_name(src_path, "txt")
            content = to_txt(src_path.name, label_final, subj_clean, body_final, stats_final)
        else:
            out_name = safe_out_name(src_path, "html")
            content = to_html_page(src_path.name, label_final, subj_clean, body_final, stats_final)

        (out_dir / out_name).write_text(content, encoding="utf-8", errors="ignore")
        written += 1

    # process msg
    for p in msg_files:
        try:
            process_one(p, "msg")
        except Exception as e:
            errors.append({"file": str(p), "error": str(e)})

    # process html
    for p in html_files:
        try:
            process_one(p, "html")
        except Exception as e:
            errors.append({"file": str(p), "error": str(e)})

    # error log
    if errors:
        (out_dir / "errors.json").write_text(json.dumps(errors, ensure_ascii=False, indent=2), encoding="utf-8")
        (out_dir / "errors.log").write_text("\n".join([f"{x['file']} :: {x['error']}" for x in errors]), encoding="utf-8")

    print(f"[{now_str()}] Done. Written={written}, Errors={len(errors)}")
    if not HAVE_EXTRACT_MSG:
        print("NOTE: extract_msg not installed -> .msg files will fail. Install via: pip install extract_msg")
    if USE_LLM_LABELING and (not HAVE_AZURE or not os.getenv("OPENAI_API_KEY")):
        print("NOTE: LLM labeling was requested but Azure client not available -> heuristic labeling used.")


if __name__ == "__main__":
    main()
