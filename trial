# -*- coding: utf-8 -*-
# KI Risiko ‚Äì Pr√§ziser Scraper (Top-3 pro Seite)
# ------------------------------------------------
# Liest:
#   C:\Users\SHTANDO\Desktop\KI Risko\Textdoks_f√ºr_Einstellungen_der_Suche\
#       Einstellungen_Analyse.csv
#       Bekannte_Seiten.csv
#       Schlagw√∂rter.csv
#
# Schreibt:
#   C:\Users\SHTANDO\Desktop\KI Risko\gescrapte_Artikel-Links\
#       gescrapte_links_<rohst>.csv
#
# Logik:
#   - F√ºr jede bekannte Seite (Domain) mehrere Risiko-Suchanfragen auf DuckDuckGo
#   - Pro Seite max. 3 sehr relevante Artikel (HR / Umwelt)
#   - Datum >= max_datum (falls erkennbar)
#   - Vor-Filter: Risiko-Schlagw√∂rter im Titel/Snippet
#   - GPT-Filter: strikt 0/1 relevant
# ------------------------------------------------

import os
import csv
import time
from pathlib import Path
from datetime import datetime

import dateparser
from bs4 import BeautifulSoup

import openai

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from urllib.parse import quote_plus


# =========================================================
# 1) Pfade / Konfiguration
# =========================================================

PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")

SETTINGS_DIR = PROJECT_ROOT / "Textdoks_f√ºr_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV = SETTINGS_DIR / "Schlagw√∂rter.csv"

# pro Seite max. X Artikel
MAX_ARTICLES_PER_SITE = 3

# max. DuckDuckGo-Seiten pro Query
MAX_DDG_PAGES = 2

# Browser headless oder sichtbar?
HEADLESS = False

# Risiko-Schlagw√∂rter (zus√§tzlich zu deinen aus Schlagw√∂rter.csv)
RISK_TERMS_BASE = [
    "human rights",
    "violation",
    "abuse",
    "forced labour",
    "forced labor",
    "child labour",
    "child labor",
    "pollution",
    "environmental damage",
    "contamination",
    "toxic waste",
    "spill",
    "deforestation",
    "protest",
    "conflict",
    "strike",
]


# =========================================================
# 2) OpenAI Client
# =========================================================

client = openai.AzureOpenAI(
    api_version="2024-06-01",
    azure_endpoint="https://genai-nexus.api.corpinter.net/apikey/",
    api_key=os.getenv("OPENAI_API_KEY"),
)


# =========================================================
# 3) Einstellungen lesen
# =========================================================

rohst = rawm = max_datum_str = None
seitenumfang = 3
scraperpfad = None

def lese_einstellungen():
    global rohst, rawm, max_datum_str, seitenumfang, scraperpfad

    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Einstellungsdatei nicht gefunden: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    def get_row(idx, default=""):
        if len(rows) > idx and len(rows[idx]) > 0:
            return rows[idx][0].strip()
        return default

    rohst         = get_row(5, "Stahl")
    rawm          = get_row(8, "Steel")
    max_datum_str = get_row(11, "20.11.2020")
    seitenumfang  = int(get_row(14, "3") or "3")
    scraperpfad   = get_row(17, "")

    print("SETTINGS:")
    print(f"  rohst        = {rohst}")
    print(f"  rawm         = {rawm}")
    print(f"  max_datum    = {max_datum_str}")
    print(f"  seitenumfang = {seitenumfang}")
    print(f"  scraperpfad  = {scraperpfad or '(auto)'}")

    # Datum parsen (deutsch)
    max_date = dateparser.parse(max_datum_str, languages=["de"])
    if not max_date:
        # Fallback: alles zulassen
        return None
    return max_date.date()


# =========================================================
# 4) Listen einlesen (Seiten & Schlagw√∂rter)
# =========================================================

def lese_einfache_liste(pfad: Path, delimiter=",") -> list[str]:
    if not pfad.exists():
        print(f"‚ö†Ô∏è Datei nicht gefunden: {pfad} ‚Üí leere Liste")
        return []
    werte: list[str] = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                val = cell.strip()
                if val:
                    werte.append(val)
    # Duplikate entfernen, Reihenfolge beibehalten
    seen = set()
    uniq = []
    for v in werte:
        if v not in seen:
            seen.add(v)
            uniq.append(v)
    return uniq


# =========================================================
# 5) Selenium Driver
# =========================================================

def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)

    driver = webdriver.Chrome(options=opts)
    return driver


# =========================================================
# 6) DuckDuckGo Suchanfragen bauen
# =========================================================

def build_queries_for_site(domain: str, rawm: str, schlagwoerter: list[str]) -> list[str]:
    """
    F√ºr eine Domain mehrere Queries bauen.
    - Wir benutzen KEINE Anf√ºhrungszeichen und KEIN OR,
      um 'Filter l√∂schen' zu vermeiden.
    - Schlagw√∂rter-Zeilen mit + werden zu Leerzeichen umgewandelt.
    """
    base_terms = [rawm, "mining"]
    queries: list[str] = []

    # 1) Basisrisiko-Query (nur rawm + risk terms)
    base_query_terms = base_terms + RISK_TERMS_BASE
    q1 = "site:www." + domain + " " + " ".join(base_query_terms)
    queries.append(q1)

    # 2) Noch eine Basisvariante ohne "mining"
    base_query_terms2 = [rawm] + RISK_TERMS_BASE
    q2 = "site:www." + domain + " " + " ".join(base_query_terms2)
    queries.append(q2)

    # 3) ein paar Schlagwort-Kombis hinzunehmen (z.B. die ersten 3)
    for sw in schlagwoerter[:3]:
        sw_clean = sw.replace("+", " ")
        q_sw = "site:www." + domain + " " + rawm + " " + sw_clean
        queries.append(q_sw)

    return queries


def ddg_search_url(query: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(query) + "&kp=1&ia=web"


# =========================================================
# 7) DuckDuckGo Resultate scrapen
# =========================================================

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    """
    Scrape DuckDuckGo Resultatskarten f√ºr eine Such-URL.
    Gibt Liste von Dicts zur√ºck: {url, title, snippet, date_txt}
    """
    items: list[dict] = []

    try:
        driver.get(search_url)
    except Exception as e:
        print(f"‚ùå Konnte Suchseite nicht laden: {e}")
        return items

    pages_seen = 0

    while pages_seen < max_pages:
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located(
                    (By.CSS_SELECTOR, 'article[data-testid="result"]')
                )
            )
        except Exception:
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            txt = soup.get_text(" ", strip=True)
            if "Keine Ergebnisse" in txt or "No results" in txt:
                print("   ‚Üí DuckDuckGo: keine Ergebnisse.")
                break
            else:
                print("   ‚Üí DuckDuckGo: Resultate nicht stabil erkennbar.")
                break

        articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for art in articles:
            # Link & Titel
            link_el = None
            try:
                link_el = art.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
            except Exception:
                try:
                    link_el = art.find_element(By.CSS_SELECTOR, "a.Rn_JXVtoPVAFyGkcaXyK")
                except Exception:
                    continue

            url = link_el.get_attribute("href") or ""
            title = link_el.text.strip()
            if not url or "duckduckgo.com" in url:
                continue

            # Snippet
            snippet = ""
            try:
                sn_el = art.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']")
                snippet = sn_el.text.strip()
            except Exception:
                pass

            # Datum (falls vorhanden)
            date_txt = ""
            try:
                date_el = art.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq")
                date_txt = date_el.text.strip()
            except Exception:
                pass

            items.append(
                {
                    "url": url,
                    "title": title,
                    "snippet": snippet,
                    "date_txt": date_txt,
                }
            )

        # "Mehr Ergebnisse"?
        try:
            more_btn = WebDriverWait(driver, 3).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", more_btn)
            time.sleep(1)
            more_btn.click()
            pages_seen += 1
            time.sleep(1)
        except Exception:
            break

    return items


# =========================================================
# 8) Vorfilter: lokale Risiko-Schlagw√∂rter
# =========================================================

def contains_risk_terms(text: str, extra_terms: list[str]) -> bool:
    """
    Pr√ºft, ob einer der Risiko-Begriffe im Text vorkommt.
    extra_terms: aus Schlagw√∂rter.csv (mit + schon bereinigt).
    """
    text_lower = text.lower()
    for kw in RISK_TERMS_BASE:
        if kw.lower() in text_lower:
            return True
    for sw in extra_terms:
        if not sw:
            continue
        sw_clean = sw.replace("+", " ").lower()
        # split in W√∂rter, pr√ºfe eines davon
        for w in sw_clean.split():
            if w and w in text_lower:
                return True
    return False


# =========================================================
# 9) GPT-basierte Klassifikation
# =========================================================

def gpt_link_relevance(rohstoff: str, url: str, title: str, snippet: str) -> str:
    """
    GPT-Filter:
      - 1 = Artikel ist direkt zu HR/Umwelt-Konflikten beim Rohstoff
      - 0 = irrelevant
    """
    text_for_model = f"URL: {url}\nTitle: {title}\nSnippet: {snippet}"

    prompt = f"""
Du bist Menschenrechts- und Nachhaltigkeitsexperte.

Entscheide, ob dieser Artikel direkt von konfliktbehafteten Nachhaltigkeits-Themen
bez√ºglich Umwelt oder Menschenrecht handelt, die im Zusammenhang mit der Gewinnung
oder Weiterverarbeitung des Rohstoffs "{rohstoff}" stehen.

Relevant (1) sind nur Artikel mit klaren negativen Vorkommnissen f√ºr Menschen
und/oder Umwelt, z.B.:
- Kinderarbeit, Zwangsarbeit, moderne Sklaverei
- gef√§hrliche Arbeitsbedingungen, schwere Arbeitsunf√§lle
- Landkonflikte, Vertreibungen, Gewalt gegen Communities
- Umweltverschmutzung (Luft, Wasser, Boden), giftige Abf√§lle, D√§mmeinst√ºrze,
  Entwaldung etc.

Nicht relevant (0) sind:
- rein finanzielle oder wirtschaftliche Meldungen
- neutrale technische Berichte ohne klare Verletzungen
- allgemeine Klimaberichte ohne konkreten Bezug zu dieser Rohstoffaktivit√§t

Antwort NUR mit:
1  ‚Üí relevant
0  ‚Üí nicht relevant

Hier ist der Treffer:
{text_for_model}
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Du bist ein sehr strenger Filter f√ºr Menschenrechts- und Umwelt-Risiken.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0,
        )
        ans = resp.choices[0].message.content.strip()
        return "1" if ans.startswith("1") else "0"
    except Exception as e:
        print(f"   ‚ö†Ô∏è GPT-Fehler: {e}")
        return "0"


# =========================================================
# 10) Datum pr√ºfen
# =========================================================

def is_date_ok(date_txt: str, max_date: datetime.date | None, url: str) -> bool:
    """
    date_txt: Datum/Snippet von DuckDuckGo (z.B. '2 days ago', '20.11.2024')
    max_date: fr√ºhestes erlaubtes Datum (aus Einstellungen), oder None, wenn keine Filterung.
    """
    if max_date is None:
        return True

    if not date_txt:
        # kein Datum ‚Üí zur Sicherheit trotzdem zulassen
        return True

    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        # nicht interpretierbar ‚Üí zulassen
        return True

    return parsed.date() >= max_date


# =========================================================
# 11) Main
# =========================================================

if __name__ == "__main__":
    max_date = lese_einstellungen()

    # bekannte Seiten + Schlagw√∂rter laden
    seiten = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    schlagwoerter = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    print("\nBekannte Seiten:", seiten)
    print("Schlagw√∂rter   :", schlagwoerter)

    out_path = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"

    # CSV vorbereiten
    with out_path.open("w", newline="", encoding="utf-8-sig") as f_out:
        writer = csv.writer(f_out, delimiter=";")
        writer.writerow(["gefundener link", "datum", "seite", "suche", "titel", "snippet"])

    driver = make_driver()

    try:
        for idx_site, domain in enumerate(seiten, start=1):
            print(f"\nüåê Seite {idx_site}/{len(seiten)}: {domain}")

            queries = build_queries_for_site(domain, rawm, schlagwoerter)
            print(f"   ‚Üí {len(queries)} Queries f√ºr diese Seite.")

            accepted_for_site = 0
            seen_urls_for_site: set[str] = set()

            for q_idx, q in enumerate(queries, start=1):
                if accepted_for_site >= MAX_ARTICLES_PER_SITE:
                    print(f"   ‚úÖ Limit von {MAX_ARTICLES_PER_SITE} Artikeln f√ºr {domain} erreicht.")
                    break

                url_search = ddg_search_url(q)
                print(f"   üîé Query {q_idx}/{len(queries)}")
                print(f"      {url_search}")

                results = scrape_ddg_results(driver, url_search, max_pages=MAX_DDG_PAGES)
                print(f"      ‚Üí {len(results)} Roh-Ergebnisse")

                for res in results:
                    if accepted_for_site >= MAX_ARTICLES_PER_SITE:
                        break

                    url_hit = res["url"]
                    title = res["title"]
                    snippet = res["snippet"]
                    date_txt = res["date_txt"]

                    if url_hit in seen_urls_for_site:
                        continue
                    seen_urls_for_site.add(url_hit)

                    # Datum pr√ºfen
                    if not is_date_ok(date_txt, max_date, url_hit):
                        # optional debug:
                        # print(f"      ‚úÇÔ∏è veraltet: {date_txt} ‚Üí {url_hit}")
                        continue

                    # lokaler Risiko-Filter
                    combined_text = (title + " " + snippet).strip()
                    if not contains_risk_terms(combined_text, schlagwoerter):
                        # optional debug:
                        # print(f"      ‚úÇÔ∏è keine Risiko-Schlagw√∂rter: {title}")
                        continue

                    # GPT-Filter
                    rel = gpt_link_relevance(rawm, url_hit, title, snippet)
                    if rel == "1":
                        accepted_for_site += 1
                        print(f"      ‚úÖ akzeptiert #{accepted_for_site} f√ºr {domain}")
                        print(f"         Titel: {title}")
                        print(f"         URL  : {url_hit}")

                        # direkt anh√§ngen
                        with out_path.open("a", newline="", encoding="utf-8-sig") as f_out:
                            writer = csv.writer(f_out, delimiter=";")
                            writer.writerow([url_hit, date_txt, domain, q, title, snippet])

                time.sleep(2)  # kleine Pause zwischen Queries

            if accepted_for_site == 0:
                print(f"   ‚Üí Keine relevanten Artikel f√ºr {domain} gefunden.")
            else:
                print(f"   ‚Üí Insgesamt {accepted_for_site} relevante Artikel f√ºr {domain}.")

            time.sleep(2)  # Pause zwischen Seiten

    finally:
        try:
            driver.quit()
        except Exception:
            pass

    print(f"\n‚úÖ Fertig. Ergebnisse in: {out_path}")
