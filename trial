# -*- coding: utf-8 -*-
"""
MSG Chain Sanitizer (Outlook COM) - ONE OUTPUT PER .MSG, MULTIPLE SEGMENTS INSIDE

- Reads .msg files using Outlook COM (no extract_msg).
- Extracts subject + body (prefers HTMLBody).
- Splits email chains into segments (each segment ~ one email in thread).
- Sanitizes each segment: removes names/emails/phones/IDs/addresses/money/etc (deletes, no [PERSON] tokens).
- Labels EACH SEGMENT as question/answer (heuristic).
- Optionally extracts attachments and appends extracted text to the newest segment.

Run:
  python sanitize_msg_outlook_chain.py
"""

import os
import re
import json
import shutil
import hashlib
import tempfile
from datetime import datetime
from pathlib import Path
from html import escape

# =========================
# EDIT THESE
# =========================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

RECURSIVE = True
OUTPUT_FORMAT = "html"   # "html" or "txt"
SKIP_TEMP = True         # skip "~$" files

MAX_BODY_CHARS = 50000
ATTACH_MAX_CHARS_EACH = 8000
ATTACH_MAX_FILES = 10

# If you want to REMOVE supplier words entirely:
REMOVE_SUPPLIER_WORDS = True
SUPPLIER_CUE_WORDS = [
    "lieferant", "lieferanten", "supplier",
    # add known supplier names if you prefer delete instead of anonymize
]

# =========================
# Signature / greeting cues
# =========================
SIGNATURE_CLOSINGS = [
    "best regards", "kind regards", "regards",
    "mit freundlichen grüßen", "freundliche grüße", "viele grüße",
    "vg", "lg", "br", "mfg", "thanks and regards",
    "with best regards", "üdvözlettel",
    "danke und viele grüße", "danke und freundliche grüße"
]

GREETING_CUES = [
    "hallo", "hi", "hello", "guten morgen", "guten tag", "good morning",
    "dear", "liebe", "lieber", "sehr geehrte", "sehr geehrter"
]

REPLY_PREFIXES = ("re:", "aw:", "wg:", "fw:", "fwd:")

# =========================
# Regex patterns
# =========================
WS_RE = re.compile(r"\s+")
TAG_RE = re.compile(r"<[^>]+>", re.S)

EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b(?:[a-z0-9-]+\.)+(?:[a-z]{2,})(?:/[^\s<>()]+)?\b", re.I)

PHONE_RE = re.compile(
    r"(?<!\w)(?:\+?\d{1,3}[\s\-\/]?)?(?:\(?\d{2,5}\)?[\s\-\/]?)?\d[\d\s\-\/]{6,}\d(?!\w)"
)
IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}[A-Z0-9]{11,30}\b", re.I)
MONEY_RE = re.compile(
    r"(?ix)"
    r"(?:\b(?:eur|usd|gbp|chf)\s*\d[\d\.,\s]*\b)|"
    r"(?:\b\d[\d\.,\s]*\s*(?:€|eur|usd|gbp|chf)\b)|"
    r"(?:\b€\s*\d[\d\.,\s]*\b)"
)

REF_RE = re.compile(
    r"(?i)\b(?:po|pr|ncr|ticket|case|req|request|material|part|supplier|portal|round|id|ref|sp\d+)\s*[:#]?\s*"
    r"[a-z0-9\-_\/]*\d[a-z0-9\-_\/]*\b"
)

ADDRESS_RE = re.compile(
    r"(?i)\b([A-ZÄÖÜ][a-zäöüß]+(?:\s[A-ZÄÖÜ][a-zäöüß]+){0,3})\s"
    r"(straße|strasse|str\.|weg|allee|platz|ring|gasse|damm|ufer)\s"
    r"\d{1,5}[a-z]?\b"
)

TITLE_NAME_RE = re.compile(
    r"(?i)\b(?:mr|mrs|ms|miss|dr|prof|herr|frau)\.?\s+"
    r"[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b"
)
NAME_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}\b")

# Greeting line capturing possible name after greeting
GREETING_NAME_RE = re.compile(
    r"(?im)^\s*(?:hallo|hi|hello|guten\s+morgen|guten\s+tag|good\s+morning|dear|liebe|lieber|sehr\s+geehrte)\s+([^\n,]{2,50})"
)

# Chain header detection (German + English)
HDR_FROM_RE = re.compile(r"(?im)^\s*(von|from)\s*:\s*.+$")
HDR_SENT_RE = re.compile(r"(?im)^\s*(gesendet|sent)\s*:\s*.+$")
HDR_TO_RE   = re.compile(r"(?im)^\s*(an|to)\s*:\s*.+$")
HDR_CC_RE   = re.compile(r"(?im)^\s*(cc)\s*:\s*.+$")
HDR_SUBJ_RE = re.compile(r"(?im)^\s*(betreff|subject)\s*:\s*.+$")

# A "header block" often begins with From/Von and contains Sent/To/Subject lines
HEADER_BLOCK_START = re.compile(r"(?im)^\s*(von|from)\s*:\s*.+$")

QUOTE_MARKERS_RE = re.compile(r"(?im)^\s*(?:-----original message-----|_{5,}|begin forwarded message:)\s*$")


def clean_ws(s: str) -> str:
    return WS_RE.sub(" ", s or "").strip()


def safe_output_name(source_path: Path) -> str:
    raw = source_path.stem
    cleaned = re.sub(r"[\\/:*?\"<>|]+", "_", raw)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    h = hashlib.sha1(str(source_path).encode("utf-8", errors="ignore")).hexdigest()[:8]
    if len(cleaned) > 90:
        cleaned = cleaned[:90]
    if not cleaned:
        cleaned = "mail"
    ext = ".html" if OUTPUT_FORMAT.lower() == "html" else ".txt"
    return f"{cleaned}__{h}{ext}"


def html_to_text(raw: str) -> str:
    """Basic HTML->text while keeping line breaks."""
    if not raw:
        return ""
    s = raw.replace("\r\n", "\n")
    s = re.sub(r"(?i)<\s*br\s*/?\s*>", "\n", s)
    s = re.sub(r"(?i)</\s*p\s*>", "\n\n", s)
    s = re.sub(r"(?i)</\s*div\s*>", "\n", s)
    s = TAG_RE.sub(" ", s)

    lines = [WS_RE.sub(" ", ln).strip() for ln in s.split("\n")]
    text = "\n".join(lines)
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return text


def outlook_open_msg(path: str):
    """Outlook COM open .msg via OpenSharedItem."""
    try:
        import win32com.client as win32
    except Exception as e:
        raise SystemExit("pywin32 is required. Install: pip install pywin32") from e

    app = win32.Dispatch("Outlook.Application")
    ns = app.GetNamespace("MAPI")
    try:
        return ns.OpenSharedItem(path)
    except Exception:
        return None


def get_msg_body_text(mail_item) -> str:
    """Prefer HTMLBody, fallback Body."""
    html = ""
    body = ""
    try:
        html = getattr(mail_item, "HTMLBody", "") or ""
    except Exception:
        html = ""
    try:
        body = getattr(mail_item, "Body", "") or ""
    except Exception:
        body = ""

    if html.strip():
        return html_to_text(html)
    return body or ""


def extract_attachments_to_temp(mail_item, temp_dir: Path):
    """Save attachments to temp; returns list of saved paths."""
    saved = []
    try:
        atts = mail_item.Attachments
    except Exception:
        return saved

    count = 0
    for i in range(1, getattr(atts, "Count", 0) + 1):
        if count >= ATTACH_MAX_FILES:
            break
        try:
            att = atts.Item(i)
            fname = getattr(att, "FileName", "") or f"attachment_{i}"
            safe = re.sub(r"[\\/:*?\"<>|]+", "_", fname).strip() or f"attachment_{i}"
            out_path = temp_dir / safe
            if out_path.exists():
                out_path = temp_dir / f"{out_path.stem}_{i}{out_path.suffix}"
            att.SaveAsFile(str(out_path))
            saved.append(out_path)
            count += 1
        except Exception:
            continue
    return saved


def try_extract_attachment_text(file_path: Path) -> str:
    """Best effort extraction. Missing libs => return ''."""
    ext = file_path.suffix.lower()

    try:
        if ext == ".txt":
            return file_path.read_text(encoding="utf-8", errors="ignore")

        if ext == ".pdf":
            try:
                from pypdf import PdfReader
            except Exception:
                return ""
            out = []
            r = PdfReader(str(file_path))
            for pg in r.pages:
                try:
                    out.append(pg.extract_text() or "")
                except Exception:
                    pass
            return "\n".join(out)

        if ext == ".docx":
            try:
                from docx import Document
            except Exception:
                return ""
            doc = Document(str(file_path))
            paras = []
            for p in doc.paragraphs:
                t = clean_ws(p.text)
                if t:
                    paras.append(t)
            return "\n".join(paras)

        if ext == ".xlsx":
            try:
                import openpyxl
            except Exception:
                return ""
            wb = openpyxl.load_workbook(str(file_path), data_only=True, read_only=True)
            out = []
            for ws in wb.worksheets:
                out.append(f"Sheet: {ws.title}")
                max_rows = min(ws.max_row or 0, 200)
                max_cols = min(ws.max_column or 0, 30)
                for r in range(1, max_rows + 1):
                    row = []
                    for c in range(1, max_cols + 1):
                        v = ws.cell(row=r, column=c).value
                        row.append("" if v is None else str(v))
                    if any(x.strip() for x in row):
                        out.append(" | ".join(clean_ws(x) for x in row))
            return "\n".join(out)

        if ext in (".png", ".jpg", ".jpeg"):
            try:
                from PIL import Image
            except Exception:
                return ""
            try:
                import pytesseract
            except Exception:
                return ""
            try:
                img = Image.open(str(file_path))
                return pytesseract.image_to_string(img) or ""
            except Exception:
                return ""

    except Exception:
        return ""

    return ""


def redact_text(text: str):
    """Delete sensitive info entirely (no [PERSON] tokens). Returns (redacted_text, stats)."""
    stats = {
        "emails": 0, "phones": 0, "urls": 0, "domains": 0, "iban": 0,
        "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
    }
    if not text:
        return "", stats

    t = text

    if REMOVE_SUPPLIER_WORDS:
        for w in SUPPLIER_CUE_WORDS:
            if not w:
                continue
            t2, n = re.subn(rf"(?i)\b{re.escape(w)}\b", "", t)
            if n:
                stats["companies"] += n
                t = t2

    def sub_count(pattern, key):
        nonlocal t
        t2, n = pattern.subn("", t)
        if n:
            stats[key] += n
            t = t2

    sub_count(EMAIL_RE, "emails")
    sub_count(URL_RE, "urls")
    sub_count(WWW_RE, "urls")
    sub_count(IBAN_RE, "iban")
    sub_count(MONEY_RE, "money")
    sub_count(REF_RE, "refs")
    sub_count(ADDRESS_RE, "addresses")
    sub_count(PHONE_RE, "phones")
    sub_count(DOMAIN_RE, "domains")

    t2, n = TITLE_NAME_RE.subn("", t)
    if n:
        stats["names"] += n
        t = t2

    def _strip_greeting_name(m):
        stats["names"] += 1
        return m.group(0).replace(m.group(1), "").strip()

    t = GREETING_NAME_RE.sub(_strip_greeting_name, t)

    t2, n = NAME_RE.subn("", t)
    if n:
        stats["names"] += n
        t = t2

    t2, n = re.subn(r"(?i)/o=exch[^ \n]+", "", t)
    if n:
        stats["names"] += n
        t = t2

    t = re.sub(r"\n{3,}", "\n\n", t).strip()
    t = re.sub(r"\s+([,.;:])", r"\1", t)
    t = re.sub(r"([,.;:]){2,}", r"\1", t)
    t = re.sub(r"\(\s*\)", "", t)
    return t.strip(), stats


def extract_useful_body(text: str) -> str:
    """
    For a segment: prefer from greeting until signature.
    If no greeting, keep trimmed segment excluding obvious header lines.
    """
    if not text:
        return ""

    t = text.strip()
    if len(t) > MAX_BODY_CHARS:
        t = t[:MAX_BODY_CHARS]

    lines = t.split("\n")

    # Drop pure header lines inside segment
    filtered = []
    for ln in lines:
        if HDR_FROM_RE.match(ln) or HDR_SENT_RE.match(ln) or HDR_TO_RE.match(ln) or HDR_CC_RE.match(ln) or HDR_SUBJ_RE.match(ln):
            continue
        filtered.append(ln)
    lines = filtered

    # Try find greeting start
    start_idx = 0
    for i, ln in enumerate(lines):
        l = ln.strip().lower()
        if any(l.startswith(gc) for gc in GREETING_CUES):
            start_idx = i
            break

    # Find signature
    end_idx = len(lines)
    for i in range(len(lines)):
        l = lines[i].strip().lower()
        if any(l == sc or l.startswith(sc + " ") for sc in SIGNATURE_CLOSINGS) or any(sc in l for sc in SIGNATURE_CLOSINGS):
            end_idx = i
            break

    body = "\n".join(lines[start_idx:end_idx]).strip()
    body = re.sub(r"\n{3,}", "\n\n", body).strip()

    # If too short, fallback to whole segment (minus header lines)
    if len(clean_ws(body)) < 30:
        fallback = "\n".join(lines).strip()
        return fallback if len(clean_ws(fallback)) >= 30 else ""
    return body


def label_question_answer(subject: str, body: str) -> str:
    s = (subject or "").strip().lower()
    b = (body or "").strip().lower()

    # Strong indicators
    if "?" in (body or ""):
        return "question"

    ask_cues = ["kannst du", "könnt ihr", "kann einer", "please", "bitte", "could you", "can you", "draufschauen", "check", "prüfen", "request", "anfrage"]
    if any(x in b for x in ask_cues):
        return "question"

    answer_cues = ["danke", "unten", "anmerkungen", "hier", "folgende", "we suggest", "we propose", "we recommend", "as discussed", "as agreed"]
    if any(x in b for x in answer_cues):
        return "answer"

    if s.startswith(REPLY_PREFIXES):
        return "answer"

    return "unknown"


def split_chain_into_segments(full_text: str):
    """
    Split thread into segments.
    We treat each block starting with From/Von as the start of an older email.
    The newest email is the text before the first header block.
    Returns list of segments in chronological order (oldest -> newest).
    """
    if not full_text:
        return []

    t = full_text.replace("\r\n", "\n")
    # Common forward marker cut (optional)
    m = QUOTE_MARKERS_RE.search(t)
    # do NOT cut here; just keep; splitting uses header blocks.

    # Find all "From/Von:" start lines
    starts = [m.start() for m in HEADER_BLOCK_START.finditer(t)]
    segments = []

    if not starts:
        # whole body is one segment
        segments.append({"raw": t, "meta": {}})
        return segments

    # Newest segment: before first "From/Von"
    newest = t[:starts[0]].strip()
    if newest:
        segments.append({"raw": newest, "meta": {"kind": "newest"}})

    # Older segments: between From/Von markers
    for i, st in enumerate(starts):
        en = starts[i + 1] if i + 1 < len(starts) else len(t)
        block = t[st:en].strip()
        if block:
            segments.append({"raw": block, "meta": {"kind": "older"}})

    # The list right now is [newest, older1, older2, ...] (newest first)
    # Convert to chronological order (oldest->newest) for readability/training:
    segments = list(reversed(segments))
    return segments


def format_output_html(source_name: str, orig_subject: str, segments_out: list, global_stats: dict) -> str:
    gen = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    subj_red, _ = redact_text(orig_subject)

    parts = []
    parts.append(f"<div><b>Source:</b> {escape(source_name)}</div>")
    parts.append(f"<div><b>Generated:</b> {escape(gen)}</div>")
    parts.append(f"<div><b>ThreadSubject:</b> {escape(subj_red) if subj_red else '(empty subject)'}</div>")
    parts.append("<hr>")

    for idx, seg in enumerate(segments_out, start=1):
        label = seg["label"]
        body = seg["body"] or "(empty after sanitization)"
        stats = seg["stats"]

        body_html = escape(body).replace("\n\n", "<br><br>").replace("\n", "<br>")
        parts.append(f"<h3 style='margin-top:18px;'>Segment {idx} — Label: {escape(label)}</h3>")
        parts.append(f"<div style='margin:8px 0 10px 0;'>{body_html}</div>")
        parts.append("<details style='margin-bottom:12px;'><summary><b>RedactionStats (segment)</b></summary>")
        parts.append(f"<pre style='background:#f6f6f6; padding:10px; border-radius:8px;'>{escape(json.dumps(stats, ensure_ascii=False))}</pre>")
        parts.append("</details>")
        parts.append("<hr>")

    parts.append("<h3>RedactionStats (total)</h3>")
    parts.append(f"<pre style='background:#f6f6f6; padding:10px; border-radius:8px;'>{escape(json.dumps(global_stats, ensure_ascii=False))}</pre>")

    return f"""<html>
<head><meta charset="utf-8"><title>Sanitized Mail Thread</title></head>
<body style="font-family:Segoe UI,Arial; font-size:13px; line-height:1.35; padding:16px;">
{''.join(parts)}
</body>
</html>"""


def format_output_txt(source_name: str, orig_subject: str, segments_out: list, global_stats: dict) -> str:
    gen = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    subj_red, _ = redact_text(orig_subject)

    out = []
    out.append(f"Source: {source_name}")
    out.append(f"Generated: {gen}")
    out.append(f"ThreadSubject: {subj_red}")
    out.append("")

    for idx, seg in enumerate(segments_out, start=1):
        out.append(f"--- Segment {idx} | Label: {seg['label']} ---")
        out.append(seg["body"] or "(empty after sanitization)")
        out.append(f"RedactionStats(segment): {json.dumps(seg['stats'], ensure_ascii=False)}")
        out.append("")

    out.append(f"RedactionStats(total): {json.dumps(global_stats, ensure_ascii=False)}")
    return "\n".join(out)


def collect_msg_files(base_dir: str):
    base = Path(base_dir)
    if not base.exists():
        raise SystemExit(f"Input folder not found: {base}")

    files = []
    it = base.rglob("*.msg") if RECURSIVE else base.glob("*.msg")
    for p in it:
        if not p.is_file():
            continue
        name = (p.name or "").lower().strip()
        if SKIP_TEMP and name.startswith("~$"):
            continue
        files.append(p)
    return sorted(files)


def main():
    in_dir = Path(MSG_DIR)
    out_dir = Path(OUT_DIR)
    out_dir.mkdir(parents=True, exist_ok=True)

    msg_files = collect_msg_files(MSG_DIR)
    print("Input folder:", in_dir)
    print(".msg found:", len(msg_files))

    if not msg_files:
        print("No .msg files found. Verify MSG_DIR and RECURSIVE.")
        return

    written = 0
    skipped = 0

    temp_root = Path(tempfile.mkdtemp(prefix="msg_chain_sanitize_"))

    try:
        for p in msg_files:
            try:
                item = None
                try:
                    item = outlook_open_msg(str(p))
                except Exception:
                    item = None
                if item is None:
                    skipped += 1
                    continue

                orig_subject = ""
                try:
                    orig_subject = getattr(item, "Subject", "") or ""
                except Exception:
                    orig_subject = ""

                raw_text = get_msg_body_text(item).replace("\r\n", "\n").strip()
                if not raw_text:
                    raw_text = ""

                # Attachments -> append into newest segment (after split)
                temp_dir = temp_root / (p.stem[:60] + "__" + hashlib.sha1(str(p).encode("utf-8", errors="ignore")).hexdigest()[:8])
                temp_dir.mkdir(parents=True, exist_ok=True)

                attach_paths = extract_attachments_to_temp(item, temp_dir)
                attach_blocks = []
                for ap in attach_paths:
                    txt = try_extract_attachment_text(ap)
                    txt = (txt or "").strip()
                    if txt:
                        if len(txt) > ATTACH_MAX_CHARS_EACH:
                            txt = txt[:ATTACH_MAX_CHARS_EACH] + "…"
                        attach_blocks.append(f"[Attachment: {ap.name}]\n{txt}")

                # Split chain into segments
                segments = split_chain_into_segments(raw_text)

                # If we have attachments text, append it to the newest segment (last in chronological order)
                if attach_blocks:
                    if segments:
                        segments[-1]["raw"] = (segments[-1]["raw"].strip() + "\n\n" + "\n\n".join(attach_blocks)).strip()
                    else:
                        segments = [{"raw": "\n\n".join(attach_blocks), "meta": {"kind": "newest"}}]

                segments_out = []
                global_stats = {
                    "emails": 0, "phones": 0, "urls": 0, "domains": 0, "iban": 0,
                    "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
                }

                for seg in segments:
                    seg_raw = seg.get("raw", "") or ""
                    useful = extract_useful_body(seg_raw)
                    red, stats = redact_text(useful)

                    red = (red or "").strip()
                    label = label_question_answer(orig_subject, red)

                    # Keep only meaningful segments
                    if len(clean_ws(red)) < 20:
                        continue

                    for k in global_stats:
                        global_stats[k] += int(stats.get(k, 0) or 0)

                    segments_out.append({"label": label, "body": red, "stats": stats})

                # If everything got filtered out, still write file (so you can see that it failed)
                if not segments_out:
                    segments_out = [{"label": "unknown", "body": "(no usable content extracted from chain)", "stats": global_stats}]

                out_name = safe_output_name(p)
                out_path = out_dir / out_name

                if OUTPUT_FORMAT.lower() == "txt":
                    out_txt = format_output_txt(p.name, orig_subject, segments_out, global_stats)
                    out_path.write_text(out_txt, encoding="utf-8", errors="ignore")
                else:
                    out_html = format_output_html(p.name, orig_subject, segments_out, global_stats)
                    out_path.write_text(out_html, encoding="utf-8", errors="ignore")

                written += 1

            except Exception as e:
                skipped += 1
                print("SKIP:", p.name, "-", e)

    finally:
        try:
            shutil.rmtree(temp_root, ignore_errors=True)
        except Exception:
            pass

    print("Done.")
    print("Written:", written)
    print("Skipped:", skipped)
    print("Output folder:", out_dir.resolve())


if __name__ == "__main__":
    main()
