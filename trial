# -*- coding: utf-8 -*-
"""
ANALYSE-SKRIPT (NO LLM) mit Risikokategorien (A–M)
- Deterministic, no Azure/OpenAI calls.
- Output CSV identical columns (13 fields + link).
- Analyses ALL links.
- NEW gates:
  (1) Year gate: require any year >= 2021 (if years exist in text)
  (2) Negative-incidence gate: require negative HR/ENV signal not merely denial/compliance

Reads:  C:/Users/SHTANDO/Desktop/KI Risko/gescrapte_Artikel-Links/gescrapte_links_<rohst>.csv
Writes: C:/Users/SHTANDO/Desktop/KI Risko/Analyse_Tabelle/analyse_tabelle_<rohst>.csv
"""

import csv
import re
import time
from pathlib import Path
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options


# --------------------------------------------------
# 0) Paths
# --------------------------------------------------
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_für_Einstellungen_der_Suche"
SCRAPED_DIR = PROJECT_ROOT / "gescrapte_Artikel-Links"
OUTPUT_DIR = PROJECT_ROOT / "Analyse_Tabelle"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

WRITE_DEBUG = True  # keep rows even if all “Not in Text”

# Gates
MIN_YEAR = 2021  # "year should be more than 2020"

# --------------------------------------------------
# 1) Read settings
# --------------------------------------------------
rohst = rawm = max_datum = None
seitenumfang = 3  # kept for compatibility

def lese_einstellungen():
    global rohst, rawm, max_datum, seitenumfang
    settings_path = SETTINGS_DIR / "Einstellungen_Analyse.csv"
    with settings_path.open("r", encoding="utf-8-sig") as f:
        rows = list(csv.reader(f, delimiter=","))

    def get_row(idx, default=""):
        return rows[idx][0].strip() if len(rows) > idx and len(rows[idx]) > 0 else default

    rohst        = get_row(5, "Stahl")
    rawm         = get_row(8, "Steel")
    max_datum    = get_row(11, "20.11.2020")
    seitenumfang = int(get_row(14, "3") or "3")


# --------------------------------------------------
# 2) Selenium driver (only if needed)
# --------------------------------------------------
HEADLESS = True

def make_driver():
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--window-size=1400,2200")
    try:
        return webdriver.Chrome(options=opts)
    except Exception:
        opts.add_argument("--headless")
        return webdriver.Chrome(options=opts)


# --------------------------------------------------
# 3) Fetch + extract article text (requests first, selenium fallback)
# --------------------------------------------------
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/121.0 Safari/537.36")

BLOCK_MARKERS = [
    "captcha", "verify you are human", "access denied", "unusual traffic",
    "cloudflare", "blocked", "bot detection"
]

def looks_blocked(text: str) -> bool:
    t = (text or "").lower()
    return any(m in t for m in BLOCK_MARKERS)

def clean_visible_text(html: str) -> str:
    soup = BeautifulSoup(html or "", "html.parser")

    for tag in soup(["script", "style", "noscript", "svg", "canvas", "header", "footer", "nav", "aside"]):
        tag.decompose()

    art = soup.find("article")
    if art:
        return art.get_text(separator="\n", strip=True)

    main = soup.find("main")
    if main:
        return main.get_text(separator="\n", strip=True)

    body = soup.find("body")
    if body:
        return body.get_text(separator="\n", strip=True)

    return soup.get_text(separator="\n", strip=True)

def fetch_html_requests(url: str, timeout_sec: int = 18) -> str:
    try:
        r = requests.get(
            url,
            headers={"User-Agent": UA, "Accept-Language": "en,de;q=0.9"},
            timeout=timeout_sec,
            allow_redirects=True
        )
        return r.text or ""
    except Exception:
        return ""

def fetch_html_selenium(url: str, timeout_sec: int = 18) -> str:
    d = make_driver()
    try:
        d.set_page_load_timeout(timeout_sec)
        d.get(url)
        time.sleep(2.5)
        return d.page_source or ""
    finally:
        try:
            d.quit()
        except Exception:
            pass

def url_zu_artikeltext(url: str) -> str:
    html = fetch_html_requests(url)
    txt = clean_visible_text(html)
    if txt and not looks_blocked(txt):
        return txt

    html2 = fetch_html_selenium(url)
    txt2 = clean_visible_text(html2)
    if txt2:
        if looks_blocked(txt2):
            raise RuntimeError("blocked_or_captcha")
        return txt2

    raise RuntimeError("empty_text")


# --------------------------------------------------
# 4) Deterministic extraction / classification
# --------------------------------------------------
RISIKOKATEGORIEN = {
    "A": "Working conditions, including occupational health and safety",
    "B": "Child labour",
    "C": "Modern slavery, including forced labour",
    "D": "Community and indigenous peoples’ rights",
    "E": "Excessive violence by private and public security forces",
    "F": "Environmental risks with impact on human rights",
    "G": "Business conduct in Conflict and High Risk Areas",
    "H": "Serious human rights abuses",
    "I": "Biodiversity",
    "J": "Water",
    "K": "Air",
    "L": "Soil",
    "M": "Waste, hazardous substances, and plant safety",
}

CAT_PATTERNS = {
    "A": [r"\bunsafe\b", r"\bsafety\b", r"\binjur(y|ies)\b", r"\bfatalit(y|ies)\b", r"\baccident(s)?\b",
          r"\bworking conditions?\b", r"\boccupational\b", r"\bhealth and safety\b", r"\bwage(s)?\b", r"\bovertime\b"],
    "B": [r"\bchild labor\b", r"\bchild labour\b", r"\bunderage\b", r"\bminor(s)?\b"],
    "C": [r"\bforced labor\b", r"\bforced labour\b", r"\bmodern slavery\b", r"\btraffick(ing|ed)\b",
          r"\bdebt bondage\b", r"\bcoerc(ed|ion)\b"],
    "D": [r"\bindigenous\b", r"\bcommunity\b", r"\bdisplacement\b", r"\bland rights?\b", r"\bfree prior informed consent\b",
          r"\bFPIC\b", r"\bresettlement\b"],
    "E": [r"\bsecurity forces?\b", r"\bpolice\b", r"\bmilitar(y|ized)\b", r"\bshoot(ing|ings)\b", r"\bexcessive force\b"],
    "F": [r"\benvironment(al)?\b", r"\bimpact\b", r"\bdamage\b", r"\bemissions?\b", r"\bclimate\b", r"\bpollut"],
    "G": [r"\bconflict\b", r"\barmed\b", r"\bmilitia\b", r"\bhigh[- ]risk\b", r"\bsanction(s)?\b", r"\bwar\b"],
    "H": [r"\btorture\b", r"\bkilling(s)?\b", r"\bextrajudicial\b", r"\bcrimes against humanity\b"],
    "I": [r"\bbiodiversity\b", r"\bhabitat\b", r"\bendangered\b", r"\bwildlife\b", r"\becosystem(s)?\b"],
    "J": [r"\bwater\b", r"\briver(s)?\b", r"\bgroundwater\b", r"\baquifer\b", r"\bdrinking water\b"],
    "K": [r"\bair\b", r"\bdust\b", r"\bparticulate\b", r"\bsmog\b", r"\bfume(s)?\b"],
    "L": [r"\bsoil\b", r"\bland contamination\b", r"\btopsoil\b", r"\bheavy metal(s)?\b"],
    "M": [r"\bwaste\b", r"\btoxic\b", r"\bhazard(ous)?\b", r"\btailing(s)?\b", r"\btailings dam\b",
          r"\bspill(s|ed)?\b", r"\bcyanide\b", r"\bmercury\b", r"\barsenic\b", r"\bleak(s|ed)?\b"],
}

HARM_PATTERNS = [
    (r"\bchild labou?r\b|\bunderage\b", "Child labour"),
    (r"\bforced labou?r\b|\bmodern slavery\b|\btraffick(ing|ed)\b", "Forced labour / modern slavery"),
    (r"\bdiscriminat(ion|e|ed)\b|\brace\b|\breligion\b|\bage\b", "Discrimination (age/race/religion etc.)"),
    (r"\btrade union\b|\bunion\b|\bcollective bargaining\b|\bunion[- ]bust", "Trade union freedoms / collective bargaining"),
    (r"\bpollut(ion|ed|ing)\b|\bcontaminat(e|ed|ion)\b|\btoxic\b|\bspill\b|\btailings?\b", "Pollution / contamination / toxic release"),
    (r"\bdeforest(ation|ed|ing)\b", "Deforestation"),
    (r"\bdisplacement\b|\bresettlement\b|\bland rights?\b", "Displacement / land-rights impacts"),
    (r"\bviolence\b|\bshoot(ing|ings)\b|\bassault\b|\bexcessive force\b", "Violence / security-force abuse"),
]

YEAR_RE = re.compile(r"\b(19\d{2}|20\d{2})\b")

# --- NEW: Negative-incidence detection (heuristics) ---
NEGATIVE_EVENT_PATTERNS = [
    # hard negative signals
    r"\bviolat(e|ed|ion|ions)\b",
    r"\babuse(s|d)?\b",
    r"\balleg(e|ed|ations?)\b",
    r"\baccus(e|ed|ation|ations)\b",
    r"\binvestigat(e|ed|ion|ions)\b",
    r"\bcomplaint(s)?\b",
    r"\blawsuit(s)?\b|\bsued\b",
    r"\bspill(s|ed)?\b",
    r"\bcontaminat(e|ed|ion)\b",
    r"\btoxic\b",
    r"\btailing(s)?\b|\btailings dam\b",
    r"\bforced\s+labou?r\b",
    r"\bchild\s+labou?r\b",
    r"\bmodern\s+slavery\b",
    r"\bunion[-\s]?bust(ing|er|ed)?\b",
    r"\bdeforest(ation|ed|ing)\b",
    r"\bdeath(s)?\b|\bkilled\b|\bfatalit(y|ies)\b",
    r"\bdisplacement\b|\beviction(s)?\b",
]

# phrases that often *negate* the risk mention
NEGATION_CUES = [
    r"\bno\b", r"\bnot\b", r"\bnever\b", r"\bwithout\b",
    r"\bden(y|ies|ied)\b", r"\bdenial\b",
    r"\bfalse\b", r"\bunfounded\b",
    r"\bdoes\s+not\b", r"\bdid\s+not\b", r"\bhas\s+not\b", r"\bhave\s+not\b",
    r"\bno\s+evidence\b", r"\bno\s+indication\b",
    r"\bcomplies?\b", r"\bcompliance\b",
    r"\bzero[-\s]?tolerance\b",
    r"\bpolicy\b", r"\bpolicies\b",
    r"\bprevent(s|ed|ing)\b",
]

NEG_EVENT_RE = re.compile("|".join(NEGATIVE_EVENT_PATTERNS), flags=re.IGNORECASE)

def expand_categories(raw_cats: str) -> str:
    if not raw_cats or raw_cats.lower().startswith("not in text"):
        return raw_cats
    parts = [p.strip().upper() for p in raw_cats.replace(";", ",").split(",") if p.strip()]
    seen, expanded = set(), []
    for p in parts:
        if p not in seen:
            seen.add(p)
            label = RISIKOKATEGORIEN.get(p, "")
            expanded.append(f"{p} ({label})" if label else p)
    return ", ".join(expanded)

def mandatory_material_present(text: str, material: str) -> bool:
    if not material or not text:
        return False
    return material.lower() in text.lower()

def extract_years_list(text: str) -> list[int]:
    ys = YEAR_RE.findall(text or "")
    out = []
    for y in ys:
        try:
            out.append(int(y))
        except Exception:
            pass
    return sorted(set(out))

def year_gate_pass(text: str) -> tuple[bool, str]:
    """
    Requirement: year > 2020 (>= 2021).
    Heuristic: if we find years and the max year < MIN_YEAR => fail.
    If no year is found, we do NOT hard-fail (to avoid dropping short pages),
    but we record "Not in Text" in output year field.
    """
    years = extract_years_list(text)
    if not years:
        return True, "no_year_found"
    if max(years) >= MIN_YEAR:
        return True, "ok"
    return False, f"max_year={max(years)}"

def is_negated_context(text: str, match_start: int, window: int = 80) -> bool:
    """
    Check for negation cues near a negative-event match.
    We look ~80 chars before the match.
    """
    lo = max(0, match_start - window)
    ctx = (text[lo:match_start] or "").lower()
    for cue in NEGATION_CUES:
        if re.search(cue, ctx):
            return True
    return False

def negative_incidence_gate_pass(text: str) -> tuple[bool, str]:
    """
    Require at least one 'negative event' mention that is not primarily negated/denied.
    """
    if not text:
        return False, "empty_text"

    # find all negative-event matches; accept if any are not negated
    for m in NEG_EVENT_RE.finditer(text):
        if not is_negated_context(text, m.start(), window=90):
            return True, f"matched={m.group(0)[:30]}"
    return False, "only_denial_or_no_negative_event"

def pick_categories(text: str) -> str:
    t = text.lower()
    found = []
    for code, pats in CAT_PATTERNS.items():
        for p in pats:
            if re.search(p, t, flags=re.IGNORECASE):
                found.append(code)
                break
    if not found:
        return "Not in Text"
    found_sorted = [c for c in "ABCDEFGHIJKLM" if c in set(found)]
    return expand_categories(", ".join(found_sorted))

def extract_harms(text: str) -> str:
    t = text.lower()
    harms = []
    for pat, label in HARM_PATTERNS:
        if re.search(pat, t, flags=re.IGNORECASE):
            harms.append(label)
    return "; ".join(dict.fromkeys(harms)) if harms else "Not in Text"

def extract_years(text: str) -> str:
    years = extract_years_list(text)
    return ", ".join(str(y) for y in years) if years else "Not in Text"

def short_reason(text: str, max_sentences: int = 3) -> str:
    if not text:
        return "Not in Text"
    parts = re.split(r"(?<=[.!?])\s+", text.strip())
    parts = [p.strip() for p in parts if p.strip()]
    if not parts:
        return "Not in Text"
    out = " ".join(parts[:max_sentences])
    return out[:900]

def analyse_artikel_deterministisch(rohstoff_name: str, artikeltext: str) -> list[str]:
    cats = pick_categories(artikeltext)
    harms = extract_harms(artikeltext)
    begr = short_reason(artikeltext, 3)

    years = extract_years(artikeltext)

    # Keep the rest conservative; you can enrich later
    countries = "Not in Text"
    locations = "Not in Text"
    stage = "Not in Text"
    companies = "Not in Text"
    groups = "Not in Text"
    min_people = "Not in Text"
    min_reason = "Not in Text"

    # severity heuristic
    sev = "moderate" if harms != "Not in Text" else "minor"
    sev_reason = "Derived from presence/absence of harm keywords"

    return [
        cats,
        harms,
        begr,
        years,
        countries,
        locations,
        stage,
        companies,
        groups,
        min_people,
        min_reason,
        sev,
        sev_reason
    ]


# --------------------------------------------------
# 5) CSV helpers
# --------------------------------------------------
def lese_links_aus_scrape_csv(rohstoff: str) -> list[str]:
    fpath = SCRAPED_DIR / f"gescrapte_links_{rohstoff}.csv"
    if not fpath.exists():
        raise FileNotFoundError(f"Scrape CSV not found: {fpath}")

    links = []
    with fpath.open("r", encoding="utf-8-sig") as f:
        r = csv.reader(f, delimiter=";")
        next(r, None)
        for row in r:
            if row and row[0].strip():
                links.append(row[0].strip())
    return list(dict.fromkeys(links))

def schreibe_analyse_csv(rohstoff: str, daten_zeilen: list[list[str]]):
    out_path = OUTPUT_DIR / f"analyse_tabelle_{rohstoff}.csv"
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow([
            "link",
            "Risikokategorien (A–M)",
            "Arten der Menschenrechts-/Umweltschädigungen",
            "Begründung",
            "Jahr(e)",
            "Land/Länder",
            "Ort(e)",
            "Stufe der Rohstoffgewinnung",
            "Involvierte Unternehmen",
            "Betroffene Personengruppen/Ökosysteme",
            "Mindestzahl der betroffenen Personen",
            "Begründung Mindestzahl",
            "Schwere",
            "Begründung Schwere",
        ])
        for row in daten_zeilen:
            if not WRITE_DEBUG:
                all_not = all(
                    (isinstance(x, str) and x.strip().lower() == "not in text")
                    for x in row[1:]
                )
                if all_not:
                    continue
            w.writerow(row)
    print(f"✅ Analyse geschrieben nach: {out_path}")


# --------------------------------------------------
# 6) Main process (ANALYZE ALL LINKS)
# --------------------------------------------------
if __name__ == "__main__":
    lese_einstellungen()
    print(f"SETTINGS:\n  rohst     = {rohst}\n  rawm      = {rawm}\n  max_datum = {max_datum}")

    links = lese_links_aus_scrape_csv(rohst)
    print("Links gefunden:", len(links))

    rows, failed = [], []
    analysed_count = 0
    skipped_material = 0
    skipped_year = 0
    skipped_non_negative = 0

    for i, link in enumerate(links, start=1):
        print(f"[{i}/{len(links)}] analysiere: {link}")

        try:
            artikel = url_zu_artikeltext(link)

            # 1) Mandatory material gate
            if rawm and rawm.strip() and not mandatory_material_present(artikel, rawm):
                skipped_material += 1
                print("  ⛔ skipped (required material not in body text)")
                continue

            # 2) Year gate (>2020)
            ok_year, year_reason = year_gate_pass(artikel)
            if not ok_year:
                skipped_year += 1
                print(f"  ⛔ skipped (year gate failed: {year_reason})")
                continue

            # 3) Negative-incidence gate
            ok_neg, neg_reason = negative_incidence_gate_pass(artikel)
            if not ok_neg:
                skipped_non_negative += 1
                print(f"  ⛔ skipped (no clear negative incidence: {neg_reason})")
                continue

            analysed = analyse_artikel_deterministisch(rawm, artikel)
            cleaned = [x.replace("\n", " ").replace("\r", " ").strip(" -") if isinstance(x, str) else x for x in analysed]

            rows.append([link] + cleaned)
            analysed_count += 1
            print("  ✅ done")

            time.sleep(1.2)

        except Exception as e:
            print("  ❌ error:", e)
            failed.append(link)
            time.sleep(2.0)

    schreibe_analyse_csv(rohst, rows)

    if failed:
        fail_path = OUTPUT_DIR / f"fehlgeschlagene_urls_analyse_{rohst}.csv"
        with fail_path.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f, delimiter=";")
            w.writerow(["link"])
            for u in failed:
                w.writerow([u])
        print(f"⚠️ Fehlgeschlagen: {len(failed)} → {fail_path}")

    print(
        f"\nDONE.\n"
        f"  Analysed: {analysed_count}\n"
        f"  Skipped (material missing): {skipped_material}\n"
        f"  Skipped (year gate): {skipped_year}\n"
        f"  Skipped (no negative incidence): {skipped_non_negative}\n"
        f"  Failed: {len(failed)}"
    )
