# -*- coding: utf-8 -*-
# ============================================================
# KI Risiko â€“ HIGH-RECALL Scraper (Stage A + Stage B verification)
# ============================================================
# Reads:
#   C:\Users\SHTANDO\Desktop\KI Risko\Textdoks_fÃ¼r_Einstellungen_der_Suche\
#       Einstellungen_Analyse.csv
#       Bekannte_Seiten.csv
#       SchlagwÃ¶rter.csv
#
# Writes:
#   C:\Users\SHTANDO\Desktop\KI Risko\gescrapte_Artikel-Links\
#       gescrapte_links_<rohst>.csv
#
# Core changes vs strict version:
#   - High recall in Stage A (title/snippet) with softer gates
#   - Stage B: open page and verify material + risk terms in BODY before rejecting
#   - Fixed PDF check (URL path, not domain)
#   - Blocks social/video + careers/IR/policy-heavy pages
#   - Adds trustworthy extra sites (used AFTER your sites)
#   - Shows live progress: collected so far
#   - Fallback: if after all sites < 5 hits, do web-wide search until 10 total, then stop
#
# Notes:
#   - No GPT key is used in this scraper (no OpenAI calls). Pure heuristic + page verification.
#   - You can later filter strictly in analysis.
# ============================================================

import csv
import time
import re
from pathlib import Path
from urllib.parse import quote_plus, urlparse

import dateparser
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# 1) PATHS / CONFIG
# ============================================================
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_fÃ¼r_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV   = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV      = SETTINGS_DIR / "SchlagwÃ¶rter.csv"

# Per-site scraping behavior (your list + my extra list)
MAX_ARTICLES_PER_SITE = 999999   # "no limit" effectively
MAX_DDG_PAGES         = 2
HEADLESS              = False

# Fallback behavior
FALLBACK_TRIGGER_MIN_TOTAL = 5   # if fewer than this after all sites => web-wide mode
FALLBACK_TARGET_TOTAL      = 10  # stop once total collected reaches this in fallback

# Stage B (page verification) controls
VERIFY_IN_BODY_MAX_CHARS = 25000  # limit body text to keep it light
VERIFY_TIMEOUT_SEC       = 10

# Small pauses (tune if DDG throttles you)
PAUSE_BETWEEN_QUERIES_SEC = (1.5, 2.5)
PAUSE_BETWEEN_SITES_SEC   = (1.5, 2.5)


# ============================================================
# 2) RISK TERMS (expanded + variants)
# ============================================================
# Includes your additions: Discrimination + Trade Union Freedoms (+ related phrasing)
RISK_TERMS_BASE = [
    # core HR
    "human rights", "rights abuse", "rights violation", "abuse", "violation", "allegation", "accusation",
    "forced labour", "forced labor", "modern slavery", "slavery", "trafficking",
    "child labour", "child labor", "underage", "minor workers",
    "wage theft", "exploitation", "worker exploitation", "labour exploitation", "labor exploitation",
    "unsafe working conditions", "hazardous work", "fatal accident", "deadly accident",

    # unions / freedom of association
    "trade union", "union rights", "trade union freedoms", "freedom of association",
    "collective bargaining", "union busting", "union-busting", "strike", "labour strike", "labor strike",

    # discrimination (age/race/religion etc.)
    "discrimination", "racial discrimination", "religious discrimination", "age discrimination",

    # environment linked to HR
    "pollution", "contamination", "contaminated", "toxic", "toxic waste",
    "spill", "leak", "tailings", "tailings dam", "dam failure",
    "deforestation", "illegal mining", "community conflict", "protest", "displacement", "eviction",
    "violence", "harassment", "intimidation",
]

# Regex stems for fuzzy matching (helps catch wording variants)
RISK_REGEX = [
    r"\bviolat", r"\babuse", r"\balleg", r"\baccus",
    r"\bforced\s+labou?r", r"\bchild\s+labou?r", r"\bmodern\s+slaver",
    r"\bdiscriminat", r"\brace", r"\breligio", r"\bage\s+discrimin",
    r"\btrade\s+union", r"\bunion\s+right", r"\bcollective\s+bargain", r"\bfreedom\s+of\s+association",
    r"\bunsafe\s+work", r"\bhazard", r"\bfatal", r"\bdeath", r"\bkilled",
    r"\bpollut", r"\bcontamin", r"\btoxic", r"\bspill", r"\btailing", r"\bdam\s+fail", r"\bdeforest",
    r"\bevict", r"\bdisplace", r"\bviolence", r"\bharass", r"\bintimidat",
]


# ============================================================
# 3) TRUSTWORTHY EXTRA SITES (added AFTER your Bekannte_Seiten)
# ============================================================
# These are generally high quality for HR / environment / watchdog coverage.
EXTRA_SITES = [
    "humanrightswatch.org",
    "amnesty.org",
    "business-humanrights.org",
    "ohchr.org",
    "ilo.org",
    "unep.org",
    "oecd.org",
    "transparency.org",
    "globalwitness.org",
    "en.earthjournalism.net",
    "reuters.com",
    "apnews.com",
    # you can add more later if needed
]


# ============================================================
# 4) SETTINGS / CSV READERS
# ============================================================
rohst = rawm = max_datum_str = None

def lese_einstellungen():
    global rohst, rawm, max_datum_str
    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Settings file not found: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    # keep your known indices
    rohst = rows[5][0].strip()
    rawm  = rows[8][0].strip()
    max_datum_str = rows[11][0].strip() if len(rows) > 11 and rows[11] else ""

    max_date = dateparser.parse(max_datum_str, languages=["de", "en"]) if max_datum_str else None
    return max_date.date() if max_date else None

def lese_einfache_liste(pfad: Path, delimiter=",") -> list[str]:
    if not pfad.exists():
        return []
    items = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                v = (cell or "").strip()
                if v:
                    items.append(v)
    # dedupe keep order
    seen = set()
    out = []
    for x in items:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


# ============================================================
# 5) SELENIUM DRIVER
# ============================================================
def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    driver = webdriver.Chrome(options=opts)
    return driver


# ============================================================
# 6) URL FILTERS (fixes PDF + blocks junk)
# ============================================================
BAD_DOMAINS = [
    "facebook.com", "m.facebook.com",
    "instagram.com",
    "tiktok.com",
    "youtube.com", "youtu.be",
    "twitter.com", "x.com",
    "pinterest.com",
]

BAD_PATH_TOKENS = [
    # careers / jobs
    "/careers", "/career", "/jobs", "/job", "vacancy", "stellenangebot",
    # investor / finance / IR
    "/investor", "/investors", "/ir", "investor-relations", "quarter", "earnings",
    # corporate policy/PR-heavy (often not violations)
    "code-of-conduct", "modern-slavery-statement", "supplier-code",
    "/policy", "/policies", "/compliance", "/ethics",
    "/sustainability", "/esg", "/csr",
    "annual-report", "sustainability-report",
]

def is_bad_url(url: str) -> bool:
    if not url:
        return True

    u = url.strip()
    parsed = urlparse(u)
    domain = (parsed.netloc or "").lower()
    path_q = ((parsed.path or "") + "?" + (parsed.query or "")).lower()

    # block obvious social/video
    if any(bad in domain for bad in BAD_DOMAINS):
        return True

    # block PDFs by PATH (correct)
    if (parsed.path or "").lower().endswith(".pdf") or ".pdf?" in path_q:
        return True

    # block very common non-content pages
    if any(tok in path_q for tok in BAD_PATH_TOKENS):
        return True

    return False


# ============================================================
# 7) MATCHING HELPERS
# ============================================================
def normalize_text(s: str) -> str:
    s = s or ""
    s = s.replace("\u00ad", "")  # soft hyphen
    return re.sub(r"\s+", " ", s).strip()

def contains_risk_terms(text: str) -> bool:
    t = (text or "").lower()
    # direct list
    if any(k in t for k in RISK_TERMS_BASE):
        return True
    # regex stems
    for rx in RISK_REGEX:
        if re.search(rx, t, flags=re.IGNORECASE):
            return True
    return False

def contains_material_soft(text: str, material: str, synonyms: list[str]) -> bool:
    """Stage A: allow material or synonyms in title/snippet."""
    t = (text or "").lower()
    mats = [material] + (synonyms or [])
    mats = [m.strip().lower() for m in mats if m and m.strip()]
    return any(m in t for m in mats)

def is_date_ok(date_txt: str, max_date) -> bool:
    if max_date is None:
        return True
    if not date_txt:
        return True
    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        return True
    return parsed.date() >= max_date


# ============================================================
# 8) DUCKDUCKGO SEARCH + SCRAPE
# ============================================================
def ddg_search_url(q: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(q) + "&kp=1&ia=web"

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    items = []
    try:
        driver.get(search_url)
    except Exception:
        return items

    pages_seen = 0
    while pages_seen < max_pages:
        time.sleep(2.5)

        html_low = (driver.page_source or "").lower()
        if "filter lÃ¶schen" in html_low or "try again" in html_low:
            # skip this query if DDG complains
            break

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article[data-testid="result"]'))
            )
        except Exception:
            break

        articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for art in articles:
            try:
                a = art.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
                url = a.get_attribute("href") or ""
                title = (a.text or "").strip()
                if not url or "duckduckgo.com" in url:
                    continue
            except Exception:
                continue

            snippet = ""
            try:
                snippet = (art.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']").text or "").strip()
            except Exception:
                pass

            date_txt = ""
            try:
                date_txt = (art.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq").text or "").strip()
            except Exception:
                pass

            items.append({"url": url, "title": title, "snippet": snippet, "date_txt": date_txt})

        # More results
        try:
            more_btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results")))
            driver.execute_script("arguments[0].scrollIntoView(true);", more_btn)
            time.sleep(0.8)
            more_btn.click()
            pages_seen += 1
            time.sleep(1.2)
        except Exception:
            break

    return items


# ============================================================
# 9) STAGE B: VERIFY IN ARTICLE BODY
# ============================================================
def fetch_page_text(driver: webdriver.Chrome, url: str) -> str:
    """Load page and extract visible-ish text from body."""
    try:
        driver.get(url)
    except Exception:
        return ""

    try:
        WebDriverWait(driver, VERIFY_TIMEOUT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    except Exception:
        pass

    html = driver.page_source or ""
    soup = BeautifulSoup(html, "html.parser")

    # remove obvious boilerplate elements
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "aside"]):
        try:
            tag.decompose()
        except Exception:
            pass

    body = soup.find("body")
    text = body.get_text(" ", strip=True) if body else soup.get_text(" ", strip=True)
    text = normalize_text(text)
    return text[:VERIFY_IN_BODY_MAX_CHARS]


# ============================================================
# 10) QUERY BUILDERS
# ============================================================
def build_site_queries(domain: str, material_en: str, extra_keywords: list[str]) -> list[str]:
    # Use a few combos; keep it human-ish; no quotes, no OR.
    kw = [k.replace("+", " ") for k in (extra_keywords or [])][:5]

    base = [
        f"site:{domain} {material_en} human rights violation",
        f"site:{domain} {material_en} forced labour",
        f"site:{domain} {material_en} child labour",
        f"site:{domain} {material_en} discrimination trade union",
        f"site:{domain} {material_en} pollution contamination",
        f"site:{domain} {material_en} lawsuit investigation",
    ]
    for k in kw:
        base.append(f"site:{domain} {material_en} {k}")

    # sanitize quotes if user keyword file contains them
    base = [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base]
    return list(dict.fromkeys(base))

def build_webwide_queries(material_en: str, extra_keywords: list[str]) -> list[str]:
    kw = [k.replace("+", " ") for k in (extra_keywords or [])][:8]
    base = [
        f"{material_en} human rights violation",
        f"{material_en} forced labour",
        f"{material_en} child labour",
        f"{material_en} discrimination trade union rights",
        f"{material_en} pollution contamination toxic waste",
        f"{material_en} lawsuit investigation allegations",
        f"{material_en} community conflict displacement",
    ]
    for k in kw:
        base.append(f"{material_en} {k}")
    base = [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base]
    return list(dict.fromkeys(base))


# ============================================================
# 11) MAIN
# ============================================================
if __name__ == "__main__":
    max_date = lese_einstellungen()
    print(f"SETTINGS: rohst={rohst} | rawm={rawm} | max_datum={max_datum_str or '(none)'}")

    user_sites = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    schlagwoerter = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    # material synonyms: use some from keywords file (helps catch "precious metals", etc.)
    # We do NOT force these; they help Stage A.
    MATERIAL_SYNONYMS = []
    for sw in schlagwoerter:
        s = sw.replace("+", " ").strip()
        if len(s) <= 3:
            continue
        # keep some generic metal phrases if present
        if any(x in s.lower() for x in ["precious", "rare earth", "metal", "mineral", rawm.lower()]):
            MATERIAL_SYNONYMS.append(s)
    MATERIAL_SYNONYMS = list(dict.fromkeys(MATERIAL_SYNONYMS))[:10]

    # priority: your sites first, then extra sites
    all_sites = list(dict.fromkeys(user_sites + EXTRA_SITES))

    out_path = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"

    # write header
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow(["url", "date", "site", "query", "title", "snippet", "verified_in_body"])

    # state
    global_seen = set()
    collected_total = 0

    driver = make_driver()

    try:
        # -----------------------------
        # Phase 1: site-restricted scraping (high recall)
        # -----------------------------
        for idx_site, domain in enumerate(all_sites, start=1):
            print(f"\nðŸŒ Site {idx_site}/{len(all_sites)}: {domain}")
            queries = build_site_queries(domain, rawm, schlagwoerter)
            print(f"   â†’ queries: {len(queries)}")

            for q_idx, q in enumerate(queries, start=1):
                search_url = ddg_search_url(q)
                print(f"   ðŸ”Ž {q_idx}/{len(queries)} | collected so far = {collected_total}")
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                print(f"      â†’ raw hits: {len(results)}")

                for hit in results:
                    url = (hit.get("url") or "").strip()
                    title = normalize_text(hit.get("title") or "")
                    snippet = normalize_text(hit.get("snippet") or "")
                    date_txt = normalize_text(hit.get("date_txt") or "")

                    if not url or url in global_seen:
                        continue

                    if is_bad_url(url):
                        continue

                    if not is_date_ok(date_txt, max_date):
                        continue

                    # Stage A scoring (soft gates)
                    combined = (title + " " + snippet).strip()
                    has_risk_A = contains_risk_terms(combined)
                    has_mat_A  = contains_material_soft(combined, rawm, MATERIAL_SYNONYMS)

                    # If neither risk nor material appears in snippet/title -> ignore early
                    if not has_risk_A and not has_mat_A:
                        continue

                    # Stage B: if either signal is missing, verify in body before accepting
                    verified = False
                    if not (has_risk_A and has_mat_A):
                        body_text = fetch_page_text(driver, url)
                        if not body_text:
                            continue
                        if contains_risk_terms(body_text) and contains_material_soft(body_text, rawm, MATERIAL_SYNONYMS):
                            verified = True
                        else:
                            continue
                    else:
                        verified = True

                    # Accept
                    global_seen.add(url)
                    collected_total += 1

                    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
                        csv.writer(f, delimiter=";").writerow(
                            [url, date_txt, domain, q, title, snippet, "yes" if verified else "no"]
                        )

                    # live progress
                    print(f"      âœ… accepted | total={collected_total} | {title[:80]}")

                # pause between queries
                time.sleep(random_uniform(*PAUSE_BETWEEN_QUERIES_SEC))

            time.sleep(random_uniform(*PAUSE_BETWEEN_SITES_SEC))

        # -----------------------------
        # Phase 2: fallback web-wide if too few
        # -----------------------------
        if collected_total < FALLBACK_TRIGGER_MIN_TOTAL:
            print(f"\nâš ï¸ Collected only {collected_total} after all sites. Entering web-wide fallback...")
            web_queries = build_webwide_queries(rawm, schlagwoerter)
            print(f"   â†’ fallback queries: {len(web_queries)} (stop at total={FALLBACK_TARGET_TOTAL})")

            for q_idx, q in enumerate(web_queries, start=1):
                if collected_total >= FALLBACK_TARGET_TOTAL:
                    break

                search_url = ddg_search_url(q)
                print(f"   ðŸ”Ž fallback {q_idx}/{len(web_queries)} | collected so far = {collected_total}")
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                print(f"      â†’ raw hits: {len(results)}")

                for hit in results:
                    if collected_total >= FALLBACK_TARGET_TOTAL:
                        break

                    url = (hit.get("url") or "").strip()
                    title = normalize_text(hit.get("title") or "")
                    snippet = normalize_text(hit.get("snippet") or "")
                    date_txt = normalize_text(hit.get("date_txt") or "")

                    if not url or url in global_seen:
                        continue
                    if is_bad_url(url):
                        continue
                    if not is_date_ok(date_txt, max_date):
                        continue

                    combined = (title + " " + snippet).strip()
                    has_risk_A = contains_risk_terms(combined)
                    has_mat_A  = contains_material_soft(combined, rawm, MATERIAL_SYNONYMS)

                    if not has_risk_A and not has_mat_A:
                        continue

                    verified = False
                    if not (has_risk_A and has_mat_A):
                        body_text = fetch_page_text(driver, url)
                        if not body_text:
                            continue
                        if contains_risk_terms(body_text) and contains_material_soft(body_text, rawm, MATERIAL_SYNONYMS):
                            verified = True
                        else:
                            continue
                    else:
                        verified = True

                    global_seen.add(url)
                    collected_total += 1

                    dom = urlparse(url).netloc
                    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
                        csv.writer(f, delimiter=";").writerow(
                            [url, date_txt, dom, q, title, snippet, "yes" if verified else "no"]
                        )

                    print(f"      âœ… accepted | total={collected_total} | {title[:80]}")

                time.sleep(random_uniform(*PAUSE_BETWEEN_QUERIES_SEC))

        print(f"\nâœ… Done. Total collected: {collected_total}")
        print(f"âž¡ï¸ Output: {out_path}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ============================================================
# 12) Small utility: random.uniform without importing random at top
# ============================================================
def random_uniform(a: float, b: float) -> float:
    # local import avoids clutter, safe
    import random
    return random.uniform(a, b)
