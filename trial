# -*- coding: utf-8 -*-
"""
MSG -> SANITIZED DATASET (with attachment extraction + OCR)

Input:
  MSG_DIR = folder containing .msg files (recursive)

Output:
  OUT_DIR/
    sanitized_text/         (1 .txt per .msg)
    sanitized_jsonl/dataset.jsonl
    entity_map.json         (stable pseudonyms across runs)
    errors.log
    ocr_missing.log         (only if images encountered but OCR not available)

Attachments supported:
  - .pdf   (pypdf)
  - .docx  (python-docx)
  - .xlsx/.xlsm (openpyxl)
  - .txt
  - .png/.jpg/.jpeg (OCR via pytesseract + Pillow if installed)
  - .msg (nested email, depth-limited)

Install (recommended):
  python -m pip install extract_msg pypdf python-docx openpyxl pillow pytesseract

OCR note:
  pytesseract also requires the Tesseract binary installed and on PATH.
"""

import os
import re
import json
import hashlib
import tempfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# =========================================================
# EDIT THESE (you provided)
# =========================================================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

# =========================================================
# Behavior / safety knobs
# =========================================================
# Safer: do not keep To/CC/From in the dataset (prevents identity leaks)
DROP_HEADER_FIELDS = True

# Nested .msg attachments recursion depth
MSG_ATTACHMENT_DEPTH_LIMIT = 2

# Attachment size limits (text)
ATTACH_MAX_FILES = 12
ATTACH_MAX_CHARS_PER_FILE = 8000
ATTACH_MAX_TOTAL_CHARS = 20000

# Body/subject limits
BODY_MAX_CHARS = 40000

# Optional: enforce stable replacement for known supplier/company names
SUPPLIER_COMPANY_DENYLIST = [
    # "Kirchhoff",
    # "Some Supplier GmbH",
]

# Optional: allowlist terms you never want touched (program names etc.)
ALLOWLIST_TERMS = [
    # "V530",
    # "PLS",
]

# =========================================================
# Regex patterns (tunable)
# =========================================================
EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b(?:[A-Z0-9-]+\.)+(?:[A-Z]{2,})\b", re.I)

PHONE_RE = re.compile(r"(?:(?:\+|00)\d{1,3}[\s\-]?)?(?:\(?\d{2,5}\)?[\s\-]?)?\d[\d\s\-]{6,}\d")
IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}(?:\s?[A-Z0-9]{4}){3,7}\b", re.I)
MONEY_RE = re.compile(r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b")

ADDRESS_RE = re.compile(
    r"\b([A-ZÄÖÜ][a-zäöüß]+(?:\s[A-ZÄÖÜ][a-zäöüß]+){0,3})\s"
    r"(Straße|Strasse|Str\.|Weg|Allee|Platz|Ring|Gasse|Damm|Ufer)\s"
    r"\d{1,5}[a-zA-Z]?\b"
)

REF_RE = re.compile(
    r"\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|Project|SP|ID|Ref)\s*[:#]?\s*"
    r"[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b",
    re.I,
)

TITLE_NAME_RE = re.compile(
    r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b"
)
GREET_NAME_RE = re.compile(
    r"\b(?:Hallo|Hi|Hello|Guten\s+Tag|Sehr\s+geehrte[rsn]?)\s+([A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2})\b",
    re.I,
)
NAME_HEURISTIC_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}\b")

COMPANY_SUFFIX_RE = re.compile(
    r"\b[A-ZÄÖÜ0-9][A-Za-zÄÖÜäöüß0-9&\-\.\s]{2,60}\s(?:GmbH|AG|KG|SE|S\.?A\.?|S\.?r\.?l\.?|Ltd\.?|Inc\.?|LLC|BV|NV)\b"
)
SUPPLIER_LABEL_RE = re.compile(r"(?i)\b(?:Supplier|Lieferant|Vendor|Company|Firma)\s*[:\-]\s*([A-Z0-9ÄÖÜ][^\n\r]{2,80})")

QUOTE_MARKERS = [
    r"^Von:\s.*$",
    r"^From:\s.*$",
    r"^Gesendet:\s.*$",
    r"^Sent:\s.*$",
    r"^An:\s.*$",
    r"^To:\s.*$",
    r"^Cc:\s.*$",
    r"^Betreff:\s.*$",
    r"^Subject:\s.*$",
    r"^-----Original Message-----.*$",
]
QUOTE_BLOCK_RE = re.compile("|".join(QUOTE_MARKERS), re.I | re.M)

SIGNATURE_CUTOFF_RE = re.compile(
    r"(?im)^\s*(Mit\s+freundlichen\s+Gr[üu]ßen|Best\s+regards|Kind\s+regards|"
    r"Viele\s+Gr[üu]ße|Regards|Thanks|Thank\s+you|"
    r"Telefon|Tel\.|Mobil|Mobile|E-?Mail|mailto)\b.*$"
)

# =========================================================
# Utilities
# =========================================================
def _clean_ws(text: str) -> str:
    if not text:
        return ""
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def _stable_token(prefix: str, value: str, salt: str = "v1") -> str:
    h = hashlib.sha256((salt + "||" + value).encode("utf-8", errors="ignore")).hexdigest()[:10]
    return f"[{prefix}_{h}]"

def _safe_stem(name: str, max_len: int = 80) -> str:
    stem = re.sub(r"[^A-Za-z0-9_\-]+", "_", name)[:max_len]
    return stem or "file"

def _load_json(path: Path) -> dict:
    if path.exists():
        return json.loads(path.read_text(encoding="utf-8"))
    return {}

def _save_json(path: Path, obj: dict) -> None:
    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding="utf-8")

def _strip_signature(text: str) -> str:
    lines = text.split("\n")
    if len(lines) < 6:
        return text
    for i in range(len(lines) - 1, max(-1, len(lines) - 60), -1):
        if SIGNATURE_CUTOFF_RE.match(lines[i] or ""):
            return "\n".join(lines[:i]).strip()
    return text

def _strip_quoted_history(text: str) -> str:
    m = QUOTE_BLOCK_RE.search(text)
    if m:
        return text[: m.start()].strip()
    return text

# =========================================================
# Entity mapping
# =========================================================
class EntityMapper:
    def __init__(self, map_path: Path):
        self.map_path = map_path
        self.map = _load_json(map_path)  # {"type|raw": "[TOKEN]"}

    def get(self, etype: str, raw: str) -> str:
        key = f"{etype}|{raw}"
        if key in self.map:
            return self.map[key]
        token = _stable_token(etype.upper(), raw)
        self.map[key] = token
        return token

    def save(self):
        _save_json(self.map_path, self.map)

# =========================================================
# Sanitization
# =========================================================
def sanitize_text(text: str, mapper: EntityMapper) -> Tuple[str, Dict[str, int]]:
    stats = {
        "emails": 0, "phones": 0, "urls": 0, "domains": 0, "iban": 0,
        "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
    }
    if not text:
        return "", stats

    t = text

    # Allowlist masking
    allow_masks = {}
    for term in ALLOWLIST_TERMS:
        if term and term in t:
            key = _stable_token("ALLOW", term)
            allow_masks[key] = term
            t = t.replace(term, key)

    # Forced company denylist
    for c in SUPPLIER_COMPANY_DENYLIST:
        if c and re.search(re.escape(c), t, flags=re.I):
            tok = mapper.get("company", c)
            t = re.sub(re.escape(c), tok, t, flags=re.I)
            stats["companies"] += 1

    # Company suffix
    def _rep_company(m):
        raw = m.group(0).strip()
        stats["companies"] += 1
        return mapper.get("company", raw)
    t = COMPANY_SUFFIX_RE.sub(_rep_company, t)

    # Supplier label
    def _rep_supplier_label(m):
        raw = (m.group(1) or "").strip()
        raw2 = re.split(r"[;,/|]", raw)[0].strip()
        if not raw2:
            return m.group(0)
        stats["companies"] += 1
        return re.sub(re.escape(raw), mapper.get("company", raw2), m.group(0), flags=re.I)
    t = SUPPLIER_LABEL_RE.sub(_rep_supplier_label, t)

    # Title+Name
    def _rep_title_name(m):
        raw = m.group(0).strip()
        stats["names"] += 1
        return mapper.get("name", raw)
    t = TITLE_NAME_RE.sub(_rep_title_name, t)

    # Greeting name
    def _rep_greet(m):
        raw = (m.group(1) or "").strip()
        stats["names"] += 1
        return m.group(0).replace(raw, mapper.get("name", raw))
    t = GREET_NAME_RE.sub(_rep_greet, t)

    # Heuristic names (noisy)
    def _rep_name_heur(m):
        raw = m.group(0).strip()
        stats["names"] += 1
        return mapper.get("name", raw)
    t = NAME_HEURISTIC_RE.sub(_rep_name_heur, t)

    # Emails
    def _rep_email(m):
        stats["emails"] += 1
        return mapper.get("email", m.group(0).lower())
    t = EMAIL_RE.sub(_rep_email, t)

    # URLs
    def _rep_url(_m):
        stats["urls"] += 1
        return "[URL]"
    t = URL_RE.sub(_rep_url, t)
    t = WWW_RE.sub(_rep_url, t)

    # Domains
    def _rep_domain(m):
        d = m.group(0)
        if d.startswith("[") and d.endswith("]"):
            return d
        stats["domains"] += 1
        return "[DOMAIN]"
    t = DOMAIN_RE.sub(_rep_domain, t)

    # Phones
    def _rep_phone(_m):
        stats["phones"] += 1
        return "[PHONE]"
    t = PHONE_RE.sub(_rep_phone, t)

    # IBAN
    def _rep_iban(_m):
        stats["iban"] += 1
        return "[IBAN]"
    t = IBAN_RE.sub(_rep_iban, t)

    # Money
    def _rep_money(_m):
        stats["money"] += 1
        return "[AMOUNT]"
    t = MONEY_RE.sub(_rep_money, t)

    # Refs/IDs
    def _rep_ref(m):
        stats["refs"] += 1
        raw = m.group(0).strip()
        return mapper.get("ref", raw)
    t = REF_RE.sub(_rep_ref, t)

    # Addresses
    def _rep_addr(_m):
        stats["addresses"] += 1
        return "[ADDRESS]"
    t = ADDRESS_RE.sub(_rep_addr, t)

    # Restore allowlist terms
    for key, term in allow_masks.items():
        t = t.replace(key, term)

    t = _clean_ws(t)
    return t, stats

def merge_stats(a: Dict[str, int], b: Dict[str, int]) -> Dict[str, int]:
    out = dict(a)
    for k, v in b.items():
        out[k] = int(out.get(k, 0)) + int(v)
    return out

# =========================================================
# Attachment text extraction
# =========================================================
def extract_text_pdf(path: Path) -> str:
    try:
        from pypdf import PdfReader
    except Exception:
        return ""
    out = []
    try:
        r = PdfReader(str(path))
        for pg in r.pages:
            try:
                out.append(pg.extract_text() or "")
            except Exception:
                continue
    except Exception:
        return ""
    return _clean_ws("\n".join(out))

def extract_text_docx(path: Path) -> str:
    try:
        from docx import Document
    except Exception:
        return ""
    try:
        doc = Document(str(path))
        parts = []
        for p in doc.paragraphs:
            if p.text and p.text.strip():
                parts.append(p.text.strip())
        # tables
        for table in doc.tables:
            for row in table.rows:
                cells = [c.text.strip() for c in row.cells]
                if any(cells):
                    parts.append(" | ".join(cells))
        return _clean_ws("\n".join(parts))
    except Exception:
        return ""

def extract_text_xlsx(path: Path) -> str:
    try:
        import openpyxl
    except Exception:
        return ""
    try:
        wb = openpyxl.load_workbook(str(path), data_only=True, read_only=True)
        out = []
        for ws in wb.worksheets:
            out.append(f"Sheet: {ws.title}")
            max_rows = min(ws.max_row or 0, 200)
            max_cols = min(ws.max_column or 0, 30)
            for r in range(1, max_rows + 1):
                row_vals = []
                for c in range(1, max_cols + 1):
                    v = ws.cell(row=r, column=c).value
                    row_vals.append("" if v is None else str(v))
                if any(x.strip() for x in row_vals if isinstance(x, str)):
                    out.append(" | ".join(_clean_ws(x) for x in row_vals))
        return _clean_ws("\n".join(out))
    except Exception:
        return ""

def extract_text_txt(path: Path) -> str:
    try:
        return _clean_ws(path.read_text(encoding="utf-8", errors="ignore"))
    except Exception:
        return ""

def extract_text_image_ocr(path: Path, ocr_missing_log: Path) -> str:
    """
    OCR using pytesseract + Pillow. If missing, logs once per run.
    """
    try:
        from PIL import Image
        import pytesseract
    except Exception:
        # log and skip
        with ocr_missing_log.open("a", encoding="utf-8") as f:
            f.write(f"OCR unavailable for image: {path}\n")
        return ""

    try:
        img = Image.open(str(path))
        # If you have German content, you can try lang="deu" if installed in Tesseract:
        # text = pytesseract.image_to_string(img, lang="deu")
        text = pytesseract.image_to_string(img)
        return _clean_ws(text)
    except Exception:
        return ""

def extract_text_from_attachment_file(path: Path, ocr_missing_log: Path) -> str:
    ext = path.suffix.lower()
    if ext == ".pdf":
        return extract_text_pdf(path)
    if ext == ".docx":
        return extract_text_docx(path)
    if ext in (".xlsx", ".xlsm"):
        return extract_text_xlsx(path)
    if ext == ".txt":
        return extract_text_txt(path)
    if ext in (".png", ".jpg", ".jpeg"):
        return extract_text_image_ocr(path, ocr_missing_log)
    return ""

# =========================================================
# MSG parsing (extract_msg) with attachment saving
# =========================================================
def parse_msg_with_extract_msg(path: Path, tmp_dir: Path) -> Tuple[Dict[str, str], List[Path]]:
    """
    Returns:
      - fields dict: subject/date/from/to/cc/body
      - list of attachment file paths saved to tmp_dir
    """
    import extract_msg  # pip install extract_msg

    msg = extract_msg.Message(str(path))
    msg.process()

    subject = (msg.subject or "").strip()
    date = (msg.date or "").strip()
    sender = (msg.sender or "").strip()
    to = (msg.to or "").strip()
    cc = (msg.cc or "").strip()
    body = (msg.body or "").strip()

    # Save attachments to disk
    saved = []
    try:
        atts = msg.attachments or []
        for a in atts:
            try:
                a.save(customPath=str(tmp_dir))
                # extract_msg usually saves to filename under tmp_dir
                # The object often has .longFilename or .shortFilename
                name = None
                for attr in ("longFilename", "shortFilename", "filename"):
                    v = getattr(a, attr, None)
                    if v:
                        name = str(v)
                        break
                if name:
                    cand = tmp_dir / name
                    if cand.exists():
                        saved.append(cand)
                else:
                    # fallback: list newest file
                    pass
            except Exception:
                continue
    except Exception:
        pass

    # If name detection failed, just take all files in tmp_dir
    if not saved:
        for p in tmp_dir.glob("*"):
            if p.is_file():
                saved.append(p)

    return {
        "subject": subject,
        "date": date,
        "from": sender,
        "to": to,
        "cc": cc,
        "body": body,
    }, saved

def parse_msg(path: Path, tmp_root: Path) -> Tuple[Dict[str, str], List[Path]]:
    """
    Currently uses extract_msg.
    If you must fallback to Outlook COM later, tell me and I’ll add it in this exact pipeline.
    """
    tmp_dir = tmp_root / f"msg_{hashlib.sha1(str(path).encode('utf-8', errors='ignore')).hexdigest()[:10]}"
    tmp_dir.mkdir(parents=True, exist_ok=True)
    fields, attachments = parse_msg_with_extract_msg(path, tmp_dir)
    return fields, attachments

# =========================================================
# Nested .msg attachments support
# =========================================================
def extract_attachment_texts(
    att_paths: List[Path],
    tmp_root: Path,
    ocr_missing_log: Path,
    depth: int = 0,
) -> List[Tuple[str, str]]:
    """
    Returns list of (attachment_name, extracted_text).
    Handles nested .msg attachments by parsing and extracting their body + attachments recursively.
    """
    out: List[Tuple[str, str]] = []
    if not att_paths:
        return out

    total_chars = 0
    file_count = 0

    for ap in att_paths:
        if file_count >= ATTACH_MAX_FILES:
            break
        if not ap.exists() or not ap.is_file():
            continue

        ext = ap.suffix.lower()
        name = ap.name

        # Nested msg
        if ext == ".msg" and depth < MSG_ATTACHMENT_DEPTH_LIMIT:
            try:
                nested_fields, nested_atts = parse_msg(ap, tmp_root)
                nested_body = _clean_ws(nested_fields.get("body", ""))[:ATTACH_MAX_CHARS_PER_FILE]
                if nested_body:
                    out.append((f"AttachedEmail:{name}", nested_body))
                    total_chars += len(nested_body)
                    file_count += 1
                    if total_chars >= ATTACH_MAX_TOTAL_CHARS:
                        break
                # recurse into nested attachments
                nested_texts = extract_attachment_texts(nested_atts, tmp_root, ocr_missing_log, depth=depth + 1)
                for n, t in nested_texts:
                    if file_count >= ATTACH_MAX_FILES or total_chars >= ATTACH_MAX_TOTAL_CHARS:
                        break
                    if t:
                        t2 = t[:ATTACH_MAX_CHARS_PER_FILE]
                        out.append((n, t2))
                        total_chars += len(t2)
                        file_count += 1
            except Exception:
                continue
            continue

        # Normal files (pdf/docx/xlsx/txt/images)
        txt = extract_text_from_attachment_file(ap, ocr_missing_log)
        if not txt:
            continue

        txt = txt[:ATTACH_MAX_CHARS_PER_FILE]
        if total_chars + len(txt) > ATTACH_MAX_TOTAL_CHARS:
            txt = txt[: max(0, ATTACH_MAX_TOTAL_CHARS - total_chars)]
        if txt.strip():
            out.append((name, txt))
            total_chars += len(txt)
            file_count += 1

        if total_chars >= ATTACH_MAX_TOTAL_CHARS:
            break

    return out

# =========================================================
# Per-message pipeline
# =========================================================
def sanitize_one_msg(msg_path: Path, mapper: EntityMapper, tmp_root: Path, ocr_missing_log: Path) -> Dict:
    fields, att_paths = parse_msg(msg_path, tmp_root)

    subject = _clean_ws(fields.get("subject", ""))
    date = _clean_ws(fields.get("date", ""))
    frm = _clean_ws(fields.get("from", ""))
    to = _clean_ws(fields.get("to", ""))
    cc = _clean_ws(fields.get("cc", ""))

    body = _clean_ws(fields.get("body", ""))[:BODY_MAX_CHARS]
    body = _strip_quoted_history(body)
    body = _strip_signature(body)

    # Extract attachment texts (including OCR + nested msg)
    att_texts = extract_attachment_texts(att_paths, tmp_root, ocr_missing_log, depth=0)

    # Build combined text (work-safe structure)
    combined_parts = []
    combined_parts.append("=== MAIN MESSAGE ===")
    combined_parts.append(body)

    if att_texts:
        combined_parts.append("")
        combined_parts.append("=== ATTACHMENTS (EXTRACTED TEXT) ===")
        for name, txt in att_texts:
            combined_parts.append(f"[Attachment: {name}]")
            combined_parts.append(txt)
            combined_parts.append("")

    combined = _clean_ws("\n".join(combined_parts))

    # Sanitize text
    sanitized_subject, st_sub = sanitize_text(subject, mapper)
    sanitized_body, st_body = sanitize_text(combined, mapper)
    stats = merge_stats(st_sub, st_body)

    if DROP_HEADER_FIELDS:
        sanitized_from = ""
        sanitized_to = ""
        sanitized_cc = ""
    else:
        sanitized_from, st_f = sanitize_text(frm, mapper)
        sanitized_to, st_t = sanitize_text(to, mapper)
        sanitized_cc, st_c = sanitize_text(cc, mapper)
        stats = merge_stats(stats, merge_stats(st_f, merge_stats(st_t, st_c)))

    return {
        "source_file": msg_path.name,
        "subject": sanitized_subject,
        "date": date,
        "from": sanitized_from,
        "to": sanitized_to,
        "cc": sanitized_cc,
        "text": sanitized_body,
        "stats": stats,
        "attachments_extracted": [n for n, _ in att_texts],
    }

# =========================================================
# Runner
# =========================================================
def run():
    in_path = Path(MSG_DIR)
    out_path = Path(OUT_DIR)
    out_text_dir = out_path / "sanitized_text"
    out_jsonl_dir = out_path / "sanitized_jsonl"
    out_text_dir.mkdir(parents=True, exist_ok=True)
    out_jsonl_dir.mkdir(parents=True, exist_ok=True)

    errors_log = out_path / "errors.log"
    ocr_missing_log = out_path / "ocr_missing.log"

    map_path = out_path / "entity_map.json"
    mapper = EntityMapper(map_path)

    msg_files = sorted(in_path.rglob("*.msg"))
    if not msg_files:
        raise SystemExit(f"No .msg files found under: {MSG_DIR}")

    tmp_root = Path(tempfile.gettempdir()) / "msg_sanitize_tmp"
    tmp_root.mkdir(parents=True, exist_ok=True)

    jsonl_path = out_jsonl_dir / "dataset.jsonl"

    written = 0
    with jsonl_path.open("w", encoding="utf-8") as jf:
        for p in msg_files:
            try:
                rec = sanitize_one_msg(p, mapper, tmp_root, ocr_missing_log)

                # One output .txt per .msg
                stem = _safe_stem(p.stem, 80)
                h = hashlib.sha1(str(p).encode("utf-8", errors="ignore")).hexdigest()[:8]
                out_txt = out_text_dir / f"{stem}__{h}.txt"

                # Layout
                parts = []
                parts.append(f"Source: {rec['source_file']}")
                parts.append(f"Subject: {rec['subject']}")
                if rec["date"]:
                    parts.append(f"Date: {rec['date']}")
                if not DROP_HEADER_FIELDS:
                    if rec["from"]:
                        parts.append(f"From: {rec['from']}")
                    if rec["to"]:
                        parts.append(f"To: {rec['to']}")
                    if rec["cc"]:
                        parts.append(f"CC: {rec['cc']}")
                if rec["attachments_extracted"]:
                    parts.append(f"AttachmentsExtracted: {', '.join(rec['attachments_extracted'])}")
                parts.append("")
                parts.append(rec["text"])
                parts.append("")
                parts.append(f"RedactionStats: {json.dumps(rec['stats'], ensure_ascii=False)}")

                out_txt.write_text("\n".join(parts).strip() + "\n", encoding="utf-8")

                # JSONL
                jf.write(json.dumps(rec, ensure_ascii=False) + "\n")
                written += 1

            except Exception as e:
                with errors_log.open("a", encoding="utf-8") as ef:
                    ef.write(f"{p}\n{e}\n\n")

    mapper.save()
    print(f"Done. Sanitized={written}")
    print(f"Output: {out_path.resolve()}")
    print(f"Dataset: {jsonl_path.resolve()}")
    print(f"Entity map: {map_path.resolve()}")
    if ocr_missing_log.exists():
        print(f"OCR warnings: {ocr_missing_log.resolve()}")

if __name__ == "__main__":
    run()
