# -*- coding: utf-8 -*-
"""
KI Risiko ‚Äì HIGH RECALL Scraper (Step 1)
---------------------------------------
Reads:
  C:\Users\SHTANDO\Desktop\KI Risko\Textdoks_f√ºr_Einstellungen_der_Suche\
      Einstellungen_Analyse.csv
      Bekannte_Seiten.csv
      Schlagw√∂rter.csv

Writes:
  C:\Users\SHTANDO\Desktop\KI Risko\gescrapte_Artikel-Links\
      gescrapte_links_<rohst>.csv

Goal:
  - Collect MANY candidate links (high recall).
  - NO GPT filtering here.
  - Basic junk filters only (pdf/social/jobs/shopping).
  - First search your known sites + extra trusted sites.
  - If too few results, fallback to whole-web search until TARGET_TOTAL_LINKS.
"""

import csv
import time
import re
from pathlib import Path
from urllib.parse import quote_plus, urlparse

import dateparser
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# 1) PATHS / CONFIG
# ============================================================
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")

SETTINGS_DIR = PROJECT_ROOT / "Textdoks_f√ºr_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV   = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV      = SETTINGS_DIR / "Schlagw√∂rter.csv"

OUTPUT_CSV = None  # set after reading settings

HEADLESS = False

# How many DDG "More results" clicks per query (increase for more recall)
MAX_DDG_PAGES_SITE = 4
MAX_DDG_PAGES_WEB  = 6

# Per-site collection guard (still high recall)
MAX_HITS_PER_SITE = 80

# Overall target
TARGET_TOTAL_LINKS = 800

# If after site-limited pass you have fewer than this, do whole-web fallback
MIN_TOTAL_BEFORE_FALLBACK = 80

# Sleep to reduce DDG throttling
SLEEP_BETWEEN_QUERIES = (1.5, 3.5)


# ============================================================
# 2) EXTRA TRUSTED SOURCES (added to your known sites)
#    (Use domains without scheme, no trailing slash)
# ============================================================
EXTRA_TRUSTED_SITES = [
    "hrw.org",
    "amnesty.org",
    "ilo.org",
    "oecd.org",
    "ohchr.org",
    "undp.org",
    "unep.org",
    "transparency.org",
    "globalwitness.org",
    "rightsandresources.org",
    "raid-uk.org",
    "business-humanrights.org",
]


# ============================================================
# 3) HIGH-RECALL QUERY TERMS
# ============================================================
RISK_QUERY_TERMS = [
    "human rights",
    "abuse",
    "violation",
    "forced labor",
    "forced labour",
    "child labor",
    "child labour",
    "modern slavery",
    "unsafe working conditions",
    "hazardous work",
    "worker exploitation",
    "discrimination",
    "race discrimination",
    "religion discrimination",
    "age discrimination",
    "trade union",
    "union rights",
    "union busting",
    "collective bargaining",
    "strike",
    "pollution",
    "contamination",
    "toxic waste",
    "spill",
    "tailings",
    "deforestation",
    "community conflict",
    "displacement",
    "lawsuit",
    "allegations",
    "investigation",
]


# ============================================================
# 4) READ SETTINGS
# ============================================================
rohst = rawm = max_datum_str = None

def lese_einstellungen():
    global rohst, rawm, max_datum_str, OUTPUT_CSV

    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Settings file not found: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    def get_row(idx, default=""):
        return rows[idx][0].strip() if len(rows) > idx and rows[idx] else default

    rohst        = get_row(5, "Stahl")
    rawm         = get_row(8, "Steel")
    max_datum_str = get_row(11, "20.11.2020")

    OUTPUT_CSV = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"

    print("SETTINGS:")
    print(f"  rohst      = {rohst}")
    print(f"  rawm       = {rawm}")
    print(f"  max_datum  = {max_datum_str}")
    print(f"  output     = {OUTPUT_CSV}")

    max_date = dateparser.parse(max_datum_str, languages=["de", "en"])
    return max_date.date() if max_date else None


# ============================================================
# 5) READ LIST CSVs
# ============================================================
def lese_einfache_liste(pfad: Path, delimiter=",") -> list[str]:
    if not pfad.exists():
        print(f"‚ö†Ô∏è Missing file: {pfad} -> empty list")
        return []
    items = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                v = (cell or "").strip()
                if v:
                    items.append(v)
    # de-dup keep order
    seen = set()
    out = []
    for v in items:
        if v not in seen:
            seen.add(v)
            out.append(v)
    return out


def normalize_domain(x: str) -> str:
    x = x.strip()
    x = x.replace("https://", "").replace("http://", "")
    x = x.split("/")[0]
    x = x.replace("www.", "")
    return x.lower()


# ============================================================
# 6) BASIC JUNK FILTERS (keep light!)
# ============================================================
BAD_DOMAINS = [
    "facebook.com", "instagram.com", "tiktok.com", "youtube.com",
    "twitter.com", "x.com", "pinterest.com",
]

BAD_PATH_HINTS = [
    "/jobs", "/career", "/careers", "/stellenangebote", "/karriere",
    "/shop", "/product", "/products", "/kaufen", "/preis",
    "/investor", "/investors", "/ir", "/investor-relations",
]

def looks_like_pdf(url: str) -> bool:
    u = url.lower()
    return u.endswith(".pdf") or ".pdf?" in u or "/pdf" in u

def is_junk_url(url: str) -> bool:
    try:
        p = urlparse(url)
    except Exception:
        return True

    domain = (p.netloc or "").lower()
    path = (p.path or "").lower()

    if not domain:
        return True
    if any(b in domain for b in BAD_DOMAINS):
        return True
    if looks_like_pdf(url):
        return True
    if any(h in path for h in BAD_PATH_HINTS):
        return True

    return False


# ============================================================
# 7) SELENIUM
# ============================================================
def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return webdriver.Chrome(options=opts)


# ============================================================
# 8) DUCKDUCKGO SCRAPE
# ============================================================
def ddg_search_url(query: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(query) + "&kp=1&ia=web"

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    """
    Returns list of dicts: {url, title, snippet, date_txt}
    """
    items = []
    try:
        driver.get(search_url)
    except Exception as e:
        print(f"‚ùå Cannot load: {e}")
        return items

    pages_seen = 0
    while pages_seen < max_pages:
        time.sleep(2.5)

        html = driver.page_source
        # if DDG shows try again / filter noise
        low = html.lower()
        if "filter l√∂schen" in low or "try again" in low:
            break

        try:
            WebDriverWait(driver, 8).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article[data-testid="result"]'))
            )
        except Exception:
            # No stable results
            break

        cards = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for c in cards:
            try:
                a = c.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
                url = a.get_attribute("href") or ""
                title = a.text.strip()
                if not url or "duckduckgo.com" in url:
                    continue
            except Exception:
                continue

            snippet = ""
            try:
                snippet = c.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']").text.strip()
            except Exception:
                pass

            date_txt = ""
            try:
                date_txt = c.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq").text.strip()
            except Exception:
                pass

            items.append({"url": url, "title": title, "snippet": snippet, "date_txt": date_txt})

        # click more results
        try:
            more = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results")))
            driver.execute_script("arguments[0].scrollIntoView(true);", more)
            time.sleep(0.8)
            more.click()
            pages_seen += 1
            time.sleep(1.2)
        except Exception:
            break

    return items


def is_date_ok(date_txt: str, min_date, url: str) -> bool:
    if min_date is None:
        return True
    if not date_txt:
        return True
    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        return True
    return parsed.date() >= min_date


# ============================================================
# 9) QUERY BUILDING (high recall)
# ============================================================
def clean_keyword(k: str) -> str:
    # Your Schlagw√∂rter sometimes contain "+" between words
    return (k or "").replace("+", " ").strip()

def build_site_queries(domain: str, material: str, keywords: list[str]) -> list[str]:
    dom = normalize_domain(domain)
    # Use site: filter properly
    base = [
        f"site:{dom} {material} {t}"
        for t in [
            "human rights",
            "forced labor",
            "child labour",
            "pollution",
            "environmental damage",
            "discrimination trade union",
            "lawsuit allegations",
            "unsafe working conditions",
        ]
    ]
    # add some keyword-driven queries (first 8)
    kw = [clean_keyword(x) for x in keywords if clean_keyword(x)]
    for k in kw[:8]:
        base.append(f"site:{dom} {material} {k}")
    # de-dup keep order
    seen, out = set(), []
    for q in base:
        q2 = re.sub(r"\s+", " ", q).strip()
        if q2 and q2 not in seen:
            seen.add(q2)
            out.append(q2)
    return out

def build_web_queries(material: str, keywords: list[str]) -> list[str]:
    kw = [clean_keyword(x) for x in keywords if clean_keyword(x)]
    web = []
    # broad risk combos
    for t in ["human rights", "forced labor", "child labour", "pollution", "toxic waste", "union rights", "discrimination"]:
        web.append(f"{material} {t} mining refining smelting")
    # keyword combos
    for k in kw[:12]:
        web.append(f"{material} {k} mining refining smelting")
    # also use some of the baked-in risk terms
    for t in RISK_QUERY_TERMS[:18]:
        web.append(f"{material} {t}")
    # de-dup
    seen, out = set(), []
    for q in web:
        q2 = re.sub(r"\s+", " ", q).strip()
        if q2 and q2 not in seen:
            seen.add(q2)
            out.append(q2)
    return out


# ============================================================
# 10) OUTPUT WRITING
# ============================================================
def write_header(path: Path) -> None:
    with path.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow(["url", "date", "source_site", "query", "title", "snippet"])

def append_row(path: Path, row: list[str]) -> None:
    with path.open("a", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow(row)


# ============================================================
# 11) MAIN
# ============================================================
if __name__ == "__main__":
    min_date = lese_einstellungen()

    user_sites = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    keywords   = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    # Combine: your sites first, then extra trusted sites (but avoid duplicates)
    combined_sites = []
    seen_sites = set()
    for s in user_sites + EXTRA_TRUSTED_SITES:
        d = normalize_domain(s)
        if d and d not in seen_sites:
            seen_sites.add(d)
            combined_sites.append(d)

    print(f"\nKnown sites from CSV: {len(user_sites)}")
    print(f"Extra trusted sites : {len(EXTRA_TRUSTED_SITES)}")
    print(f"Total sites used    : {len(combined_sites)}")

    write_header(OUTPUT_CSV)

    driver = make_driver()

    global_seen_urls = set()
    total_collected = 0

    try:
        # -----------------------
        # PASS 1: Site-limited
        # -----------------------
        for idx, dom in enumerate(combined_sites, start=1):
            if total_collected >= TARGET_TOTAL_LINKS:
                break

            print(f"\nüåê [{idx}/{len(combined_sites)}] site:{dom} | collected so far: {total_collected}")

            site_hits = 0
            queries = build_site_queries(dom, rawm, keywords)

            for q_idx, q in enumerate(queries, start=1):
                if total_collected >= TARGET_TOTAL_LINKS:
                    break
                if site_hits >= MAX_HITS_PER_SITE:
                    break

                search_url = ddg_search_url(q)
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES_SITE)
                print(f"   üîé {q_idx}/{len(queries)} -> {len(results)} cards | total: {total_collected}")

                for res in results:
                    if total_collected >= TARGET_TOTAL_LINKS or site_hits >= MAX_HITS_PER_SITE:
                        break

                    url = res["url"]
                    if not url or url in global_seen_urls:
                        continue
                    if is_junk_url(url):
                        continue
                    if not is_date_ok(res.get("date_txt", ""), min_date, url):
                        continue

                    global_seen_urls.add(url)
                    total_collected += 1
                    site_hits += 1

                    append_row(
                        OUTPUT_CSV,
                        [url, res.get("date_txt",""), dom, q, res.get("title",""), res.get("snippet","")]
                    )

                time.sleep(2.0)

        print(f"\n‚úÖ Site-limited pass done. Total collected: {total_collected}")

        # -----------------------
        # PASS 2: Whole-web fallback (if too few)
        # -----------------------
        if total_collected < MIN_TOTAL_BEFORE_FALLBACK:
            print(f"\n‚ö†Ô∏è Only {total_collected} links after site pass (<{MIN_TOTAL_BEFORE_FALLBACK}).")
            print(f"‚û°Ô∏è Fallback: whole-web search until {min(TARGET_TOTAL_LINKS, 10_000)} or at least 10 links.\n")

            web_queries = build_web_queries(rawm, keywords)

            for q_idx, q in enumerate(web_queries, start=1):
                if total_collected >= max(10, MIN_TOTAL_BEFORE_FALLBACK):
                    # stop early once you have a useful minimum
                    break

                search_url = ddg_search_url(q)
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES_WEB)
                print(f"   üåç web {q_idx}/{len(web_queries)} -> {len(results)} cards | total: {total_collected}")

                for res in results:
                    if total_collected >= max(10, MIN_TOTAL_BEFORE_FALLBACK):
                        break

                    url = res["url"]
                    if not url or url in global_seen_urls:
                        continue
                    if is_junk_url(url):
                        continue
                    if not is_date_ok(res.get("date_txt",""), min_date, url):
                        continue

                    global_seen_urls.add(url)
                    total_collected += 1

                    append_row(
                        OUTPUT_CSV,
                        [url, res.get("date_txt",""), "WHOLE_WEB", q, res.get("title",""), res.get("snippet","")]
                    )

                time.sleep(2.0)

            print(f"\n‚úÖ Whole-web fallback done. Total collected: {total_collected}")

        print(f"\nüéâ Finished. Links written to:\n{OUTPUT_CSV}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
