# -*- coding: utf-8 -*-
# ============================================================
# KI Risiko – Per-article executive summaries + ONE overall summary sheet
# Reads:  C:\Users\SHTANDO\Desktop\KI Risko\Analyse_Tabelle\analyse_tabelle_<rohst>.csv
# Writes: C:\Users\SHTANDO\Desktop\KI Risko\Executive_Summary_<rohst>\Executive_Summary_Artikelweise_<rohst>.xlsx
#
# What it does:
#  - Creates 1 Excel workbook
#  - Sheet "Case_1..Case_n": per-article (100–200 words) summary using ONLY analysis table fields
#  - Sheet "Overall_Summary": 200–400 words overall summary that MUST mention top countries/years/etc.
#    computed from the analysis table (no hallucination)
# ============================================================

import os
import csv
import re
from pathlib import Path
from collections import Counter

import openai
from openpyxl import Workbook


# -----------------------------
# 1) Paths and OpenAI client
# -----------------------------
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
ANALYSE_DIR = PROJECT_ROOT / "Analyse_Tabelle"
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_für_Einstellungen_der_Suche"

EINSTELLUNGEN_CSV = SETTINGS_DIR / "Einstellungen_Analyse.csv"

client = openai.AzureOpenAI(
    api_version="2024-06-01",
    azure_endpoint="https://genai-nexus.api.corpinter.net/apikey/",
    api_key=os.getenv("OPENAI_API_KEY"),
)

# -----------------------------
# 2) Read settings (rohst/rawm)
# -----------------------------
rohst = rawm = None


def lese_einstellungen():
    global rohst, rawm
    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    # keep same indices as your working pipeline
    rohst = rows[5][0].strip()
    rawm = rows[8][0].strip()
    print(f"Loaded settings → rohst={rohst}, rawm={rawm}")


# -----------------------------
# 3) Load analyse_tabelle_<rohst>.csv
# -----------------------------
def lade_analyse_tabelle() -> tuple[list[str], list[list[str]]]:
    analyse_path = ANALYSE_DIR / f"analyse_tabelle_{rohst}.csv"
    if not analyse_path.exists():
        raise FileNotFoundError(f"Analysis file not found: {analyse_path}")

    with analyse_path.open("r", encoding="utf-8-sig", newline="") as f:
        reader = csv.reader(f, delimiter=";")
        rows = list(reader)

    if not rows:
        raise ValueError(f"Analysis file is empty: {analyse_path}")

    header = rows[0]
    data_rows = rows[1:]
    print(f"Loaded {len(data_rows)} analysed rows from: {analyse_path}")
    return header, data_rows


# -----------------------------
# 4) GPT summary for one article
# -----------------------------
def baue_prompt_aus_zeile(header: list[str], row: list[str]) -> str:
    def get(col_name: str, default: str = "Not in Text") -> str:
        if col_name in header:
            idx = header.index(col_name)
            if idx < len(row) and row[idx].strip():
                return row[idx].strip()
        return default

    link = get("link", "")
    risk_categories = get("Risikokategorien (A–M)", "")
    harms = get("Arten der Menschenrechts-/Umweltschädigungen", "")
    begruendung = get("Begründung", "")
    years = get("Jahr(e)", "")
    countries = get("Land/Länder", "")
    locations = get("Ort(e)", "")
    stage = get("Stufe der Rohstoffgewinnung", "")
    companies = get("Involvierte Unternehmen", "")
    groups = get("Betroffene Personengruppen/Ökosysteme", "")
    min_people = get("Mindestzahl der betroffenen Personen", "")
    min_reason = get("Begründung Mindestzahl", "")
    severity = get("Schwere", "")
    severity_reason = get("Begründung Schwere", "")

    context = f"""
Link: {link}
Risk categories: {risk_categories}
Types of harm: {harms}
Reason / evidence: {begruendung}
Years: {years}
Countries: {countries}
Locations: {locations}
Supply-chain stage: {stage}
Companies involved: {companies}
Affected groups / ecosystems: {groups}
Minimum number of affected persons: {min_people}
Reason for this minimum: {min_reason}
Severity level: {severity}
Reason for severity: {severity_reason}
""".strip()

    prompt = f"""
You are a human-rights and environmental-risk analyst.

You receive the structured analysis of ONE article about the raw material "{rawm}".
Your task is to write a concise, well-structured executive summary of this SINGLE case.

Very important rules:
- Use ONLY the information given in the structured data below.
- Do NOT invent any new numbers, companies, places, or years.
- If something is "Not in Text" or missing, do not make it up; simply omit it or describe it more generally.
- Focus on human-rights and environmental impacts, not financial details.
- Length: about 120–200 words.
- Tone: neutral, factual, clear.
- Structure: 1–3 short paragraphs, no bullet points, no headings.

At the end of the summary, include a simple reference like:
[Article link: {link}]

Here is the structured data you must use and stay inside:

{context}
"""
    return prompt


def gpt_summary_for_row(header: list[str], row: list[str]) -> str:
    prompt = baue_prompt_aus_zeile(header, row)
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You are a careful analyst. You never invent facts that are not supported by the input.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"⚠️ GPT error for one article: {e}")
        return "Error: could not generate summary for this article."


# -----------------------------
# 5) Overall-summary helpers (facts from analyse table)
# -----------------------------
def _split_values(cell: str) -> list[str]:
    if not cell or cell.strip().lower() == "not in text":
        return []
    parts = re.split(r"[;,/|]+", cell)
    return [p.strip() for p in parts if p.strip() and p.strip().lower() != "not in text"]


def compute_overall_facts(header: list[str], rows: list[list[str]]) -> dict:
    def get(row: list[str], col: str) -> str:
        if col in header:
            idx = header.index(col)
            if idx < len(row):
                return (row[idx] or "").strip()
        return ""

    countries = Counter()
    years = Counter()
    companies = Counter()
    severities = Counter()
    categories = Counter()

    for row in rows:
        for c in _split_values(get(row, "Land/Länder")):
            countries[c] += 1

        y_raw = get(row, "Jahr(e)")
        for y in re.findall(r"\b(19\d{2}|20\d{2})\b", y_raw or ""):
            years[y] += 1

        for comp in _split_values(get(row, "Involvierte Unternehmen")):
            companies[comp] += 1

        sev = get(row, "Schwere").lower()
        if sev and sev != "not in text":
            severities[sev] += 1

        cat = get(row, "Risikokategorien (A–M)")
        if cat and cat.strip().lower() != "not in text":
            for token in [t.strip() for t in cat.split(",") if t.strip()]:
                categories[token] += 1

    def top_n(counter: Counter, n: int) -> list[tuple[str, int]]:
        return counter.most_common(n)

    return {
        "total_articles": len(rows),
        "top_countries": top_n(countries, 6),
        "top_years": top_n(years, 8),
        "top_companies": top_n(companies, 8),
        "severity_counts": dict(severities),
        "top_categories": top_n(categories, 8),
    }


def gpt_overall_summary(raw_material: str, facts: dict) -> str:
    prompt = f"""
You are a careful human-rights and environmental-risk analyst.

Write ONE overall executive summary (200–400 words) for the raw material "{raw_material}".
You MUST use ONLY the factual aggregates provided below (they are computed from the analysed rows).
Do NOT invent new places, years, companies, numbers, or incidents.

Hard requirements:
- Explicitly mention the most common country/countries (from Top countries).
- Mention the most frequent years if provided.
- Mention the severity distribution exactly as given (minor/moderate/major/critical counts).
- Mention the most frequent companies only if they appear in the list.
- If a list is empty, omit it (do not guess).
- Tone: factual, concise, readable prose. No bullets, no headings.

Aggregated facts (from analysis table):
Total analysed articles: {facts.get("total_articles")}

Top countries (count): {facts.get("top_countries")}
Top years (count): {facts.get("top_years")}
Top companies (count): {facts.get("top_companies")}
Top risk category tokens (count): {facts.get("top_categories")}
Severity counts: {facts.get("severity_counts")}

Write the summary now.
"""
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You never add facts not supported by the input."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.25,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"⚠️ GPT error for overall summary: {e}")
        return "Error: could not generate overall summary."


# -----------------------------
# 6) Write Excel with 1 sheet per article + Overall sheet
# -----------------------------
def schreibe_excel_pro_artikel(header: list[str], rows: list[list[str]], rohstoff_name: str) -> Path:
    exec_dir = PROJECT_ROOT / f"Executive_Summary_{rohstoff_name}"
    exec_dir.mkdir(parents=True, exist_ok=True)

    out_path = exec_dir / f"Executive_Summary_Artikelweise_{rohstoff_name}.xlsx"

    wb = Workbook()
    default_sheet = wb.active
    wb.remove(default_sheet)

    # --- per-article sheets ---
    for idx, row in enumerate(rows, start=1):
        sheet_name = f"Case_{idx}"
        ws = wb.create_sheet(title=sheet_name[:31])

        ws["A1"] = "Row index in analyse_tabelle"
        ws["B1"] = idx + 1  # +1 because CSV header is row 1

        excel_row = 3
        for col_idx, col_name in enumerate(header, start=1):
            value = row[col_idx - 1] if col_idx - 1 < len(row) else ""
            ws.cell(row=excel_row, column=1).value = col_name
            ws.cell(row=excel_row, column=2).value = value
            excel_row += 1

        print(f"Generating summary for article {idx}/{len(rows)}...")
        summary_text = gpt_summary_for_row(header, row)

        excel_row += 1
        ws.cell(row=excel_row, column=1).value = "Executive Summary"
        ws.cell(row=excel_row, column=2).value = summary_text

        ws.column_dimensions["A"].width = 45
        ws.column_dimensions["B"].width = 120

    # --- overall summary sheet (NEW) ---
    facts = compute_overall_facts(header, rows)
    overall_text = gpt_overall_summary(rawm, facts)

    ws_overall = wb.create_sheet(title="Overall_Summary")
    ws_overall["A1"] = "Overall Executive Summary (200–400 words)"
    ws_overall["A3"] = overall_text

    # Traceability block: computed facts from analysis table
    ws_overall["A5"] = "Computed facts (from analyse_tabelle)"
    ws_overall["A6"] = "Total analysed articles"
    ws_overall["B6"] = facts.get("total_articles")

    ws_overall["A8"] = "Top countries (count)"
    r = 9
    for name, cnt in facts.get("top_countries", []):
        ws_overall.cell(row=r, column=1).value = name
        ws_overall.cell(row=r, column=2).value = cnt
        r += 1

    r += 1
    ws_overall.cell(row=r, column=1).value = "Top years (count)"
    r += 1
    for name, cnt in facts.get("top_years", []):
        ws_overall.cell(row=r, column=1).value = name
        ws_overall.cell(row=r, column=2).value = cnt
        r += 1

    r += 1
    ws_overall.cell(row=r, column=1).value = "Severity counts"
    r += 1
    for sev, cnt in (facts.get("severity_counts") or {}).items():
        ws_overall.cell(row=r, column=1).value = sev
        ws_overall.cell(row=r, column=2).value = cnt
        r += 1

    r += 1
    ws_overall.cell(row=r, column=1).value = "Top companies (count)"
    r += 1
    for name, cnt in facts.get("top_companies", []):
        ws_overall.cell(row=r, column=1).value = name
        ws_overall.cell(row=r, column=2).value = cnt
        r += 1

    ws_overall.column_dimensions["A"].width = 55
    ws_overall.column_dimensions["B"].width = 40

    wb.save(out_path)
    return out_path


# -----------------------------
# 7) Main
# -----------------------------
if __name__ == "__main__":
    lese_einstellungen()
    header, data_rows = lade_analyse_tabelle()

    if not data_rows:
        print("No data rows found in analysis table. Nothing to summarize.")
    else:
        out_file = schreibe_excel_pro_artikel(header, data_rows, rohst)
        print(f"\n✅ Excel file written to:\n{out_file}")
