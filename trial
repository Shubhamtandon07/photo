# -*- coding: utf-8 -*-
# ============================================================
# KI Risiko ‚Äì HIGH-RECALL Scraper (Stage A + Stage B verification)
# ============================================================
# Reads:
#   C:\Users\SHTANDO\Desktop\KI Risko\Textdoks_f√ºr_Einstellungen_der_Suche\
#       Einstellungen_Analyse.csv
#       Bekannte_Seiten.csv
#       Schlagw√∂rter.csv
#
# Writes:
#   C:\Users\SHTANDO\Desktop\KI Risko\gescrapte_Artikel-Links\
#       gescrapte_links_<rohst>.csv
#
# Core:
#   - High recall in Stage A (title/snippet) with softer gates
#   - Stage B: open page and verify material + risk terms in BODY before rejecting
#   - Blocks PDFs, social/video, jobs/careers, IR/finance, PR/policy-heavy pages
#   - Adds trustworthy extra sites AFTER your sites
#   - Live progress: collected so far
#   - Fallback: if after all sites < 5 hits, web-wide search until 10 total, then stop
#
# Note: No OpenAI/GPT used in this scraper.
# ============================================================

import csv
import time
import re
from pathlib import Path
from urllib.parse import quote_plus, urlparse

from datetime import datetime

import dateparser
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# 0) Utility (MUST be defined before use)
# ============================================================
def random_uniform(a: float, b: float) -> float:
    import random
    return random.uniform(a, b)


# ============================================================
# 1) PATHS / CONFIG
# ============================================================
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_f√ºr_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV   = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV      = SETTINGS_DIR / "Schlagw√∂rter.csv"

MAX_ARTICLES_PER_SITE = 999999   # effectively no limit
MAX_DDG_PAGES         = 2
HEADLESS              = False

FALLBACK_TRIGGER_MIN_TOTAL = 5
FALLBACK_TARGET_TOTAL      = 10

VERIFY_IN_BODY_MAX_CHARS = 25000
VERIFY_TIMEOUT_SEC       = 10

PAUSE_BETWEEN_QUERIES_SEC = (1.5, 2.5)
PAUSE_BETWEEN_SITES_SEC   = (1.5, 2.5)


# ============================================================
# 2) RISK TERMS (expanded + variants)
# ============================================================
RISK_TERMS_BASE = [
    "human rights", "rights abuse", "rights violation", "abuse", "violation", "allegation", "accusation",
    "forced labour", "forced labor", "modern slavery", "slavery", "trafficking",
    "child labour", "child labor", "underage", "minor workers",
    "wage theft", "exploitation", "worker exploitation", "labour exploitation", "labor exploitation",
    "unsafe working conditions", "hazardous work", "fatal accident", "deadly accident",
    "trade union", "union rights", "trade union freedoms", "freedom of association",
    "collective bargaining", "union busting", "union-busting", "strike", "labour strike", "labor strike",
    "discrimination", "racial discrimination", "religious discrimination", "age discrimination",
    "pollution", "contamination", "contaminated", "toxic", "toxic waste",
    "spill", "leak", "tailings", "tailings dam", "dam failure",
    "deforestation", "illegal mining", "community conflict", "protest", "displacement", "eviction",
    "violence", "harassment", "intimidation",
]

RISK_REGEX = [
    r"\bviolat", r"\babuse", r"\balleg", r"\baccus",
    r"\bforced\s+labou?r", r"\bchild\s+labou?r", r"\bmodern\s+slaver",
    r"\bdiscriminat", r"\brace", r"\breligio", r"\bage\s+discrimin",
    r"\btrade\s+union", r"\bunion\s+right", r"\bcollective\s+bargain", r"\bfreedom\s+of\s+association",
    r"\bunsafe\s+work", r"\bhazard", r"\bfatal", r"\bdeath", r"\bkilled",
    r"\bpollut", r"\bcontamin", r"\btoxic", r"\bspill", r"\btailing", r"\bdam\s+fail", r"\bdeforest",
    r"\bevict", r"\bdisplace", r"\bviolence", r"\bharass", r"\bintimidat",
]


# ============================================================
# 3) TRUSTWORTHY EXTRA SITES (after your sites)
# ============================================================
EXTRA_SITES = [
    "humanrightswatch.org",
    "amnesty.org",
    "business-humanrights.org",
    "ohchr.org",
    "ilo.org",
    "unep.org",
    "oecd.org",
    "transparency.org",
    "globalwitness.org",
    "en.earthjournalism.net",
    "reuters.com",
    "apnews.com",
]


# ============================================================
# 4) SETTINGS / CSV READERS
# ============================================================
rohst = rawm = max_datum_str = None

def lese_einstellungen():
    global rohst, rawm, max_datum_str
    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Settings file not found: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    rohst = rows[5][0].strip()
    rawm  = rows[8][0].strip()
    max_datum_str = rows[11][0].strip() if len(rows) > 11 and rows[11] else ""

    # Avoid DeprecationWarning by parsing explicitly dd.mm.yyyy if that is your format
    if max_datum_str:
        try:
            # e.g. "20.11.2020"
            dt = datetime.strptime(max_datum_str, "%d.%m.%Y")
            return dt.date()
        except Exception:
            # fallback to dateparser if user changes format
            parsed = dateparser.parse(max_datum_str, languages=["de", "en"])
            return parsed.date() if parsed else None

    return None

def lese_einfache_liste(pfad: Path, delimiter=",") -> list[str]:
    if not pfad.exists():
        return []
    items = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                v = (cell or "").strip()
                if v:
                    items.append(v)
    seen = set()
    out = []
    for x in items:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


# ============================================================
# 5) SELENIUM DRIVER
# ============================================================
def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return webdriver.Chrome(options=opts)


# ============================================================
# 6) URL FILTERS
# ============================================================
BAD_DOMAINS = [
    "facebook.com", "m.facebook.com",
    "instagram.com",
    "tiktok.com",
    "youtube.com", "youtu.be",
    "twitter.com", "x.com",
    "pinterest.com",
]

BAD_PATH_TOKENS = [
    "/careers", "/career", "/jobs", "/job", "vacancy", "stellenangebot",
    "/investor", "/investors", "/ir", "investor-relations", "quarter", "earnings",
    "code-of-conduct", "modern-slavery-statement", "supplier-code",
    "/policy", "/policies", "/compliance", "/ethics",
    "/sustainability", "/esg", "/csr",
    "annual-report", "sustainability-report",
]

def is_bad_url(url: str) -> bool:
    if not url:
        return True

    u = url.strip()
    parsed = urlparse(u)
    domain = (parsed.netloc or "").lower()
    path_q = ((parsed.path or "") + "?" + (parsed.query or "")).lower()

    if any(bad in domain for bad in BAD_DOMAINS):
        return True

    # Correct PDF check (path, not domain)
    if (parsed.path or "").lower().endswith(".pdf") or ".pdf?" in path_q:
        return True

    if any(tok in path_q for tok in BAD_PATH_TOKENS):
        return True

    return False


# ============================================================
# 7) MATCHING HELPERS
# ============================================================
def normalize_text(s: str) -> str:
    s = s or ""
    s = s.replace("\u00ad", "")
    return re.sub(r"\s+", " ", s).strip()

def contains_risk_terms(text: str) -> bool:
    t = (text or "").lower()
    if any(k in t for k in RISK_TERMS_BASE):
        return True
    for rx in RISK_REGEX:
        if re.search(rx, t, flags=re.IGNORECASE):
            return True
    return False

def contains_material_soft(text: str, material: str, synonyms: list[str]) -> bool:
    t = (text or "").lower()
    mats = [material] + (synonyms or [])
    mats = [m.strip().lower() for m in mats if m and m.strip()]
    return any(m in t for m in mats)

def is_date_ok(date_txt: str, max_date) -> bool:
    if max_date is None:
        return True
    if not date_txt:
        return True
    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        return True
    return parsed.date() >= max_date


# ============================================================
# 8) DUCKDUCKGO SEARCH + SCRAPE
# ============================================================
def ddg_search_url(q: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(q) + "&kp=1&ia=web"

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    items = []
    try:
        driver.get(search_url)
    except Exception:
        return items

    pages_seen = 0
    while pages_seen < max_pages:
        time.sleep(2.5)

        html_low = (driver.page_source or "").lower()
        if "filter l√∂schen" in html_low or "try again" in html_low:
            break

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article[data-testid="result"]'))
            )
        except Exception:
            break

        articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for art in articles:
            try:
                a = art.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
                url = a.get_attribute("href") or ""
                title = (a.text or "").strip()
                if not url or "duckduckgo.com" in url:
                    continue
            except Exception:
                continue

            snippet = ""
            try:
                snippet = (art.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']").text or "").strip()
            except Exception:
                pass

            date_txt = ""
            try:
                date_txt = (art.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq").text or "").strip()
            except Exception:
                pass

            items.append({"url": url, "title": title, "snippet": snippet, "date_txt": date_txt})

        try:
            more_btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results")))
            driver.execute_script("arguments[0].scrollIntoView(true);", more_btn)
            time.sleep(0.8)
            more_btn.click()
            pages_seen += 1
            time.sleep(1.2)
        except Exception:
            break

    return items


# ============================================================
# 9) STAGE B: VERIFY IN ARTICLE BODY
# ============================================================
def fetch_page_text(driver: webdriver.Chrome, url: str) -> str:
    try:
        driver.get(url)
    except Exception:
        return ""

    try:
        WebDriverWait(driver, VERIFY_TIMEOUT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    except Exception:
        pass

    html = driver.page_source or ""
    soup = BeautifulSoup(html, "html.parser")

    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "aside"]):
        try:
            tag.decompose()
        except Exception:
            pass

    body = soup.find("body")
    text = body.get_text(" ", strip=True) if body else soup.get_text(" ", strip=True)
    text = normalize_text(text)
    return text[:VERIFY_IN_BODY_MAX_CHARS]


# ============================================================
# 10) QUERY BUILDERS
# ============================================================
def build_site_queries(domain: str, material_en: str, extra_keywords: list[str]) -> list[str]:
    kw = [k.replace("+", " ") for k in (extra_keywords or [])][:5]

    base = [
        f"site:{domain} {material_en} human rights violation",
        f"site:{domain} {material_en} forced labour",
        f"site:{domain} {material_en} child labour",
        f"site:{domain} {material_en} discrimination trade union",
        f"site:{domain} {material_en} pollution contamination",
        f"site:{domain} {material_en} lawsuit investigation",
    ]
    for k in kw:
        base.append(f"site:{domain} {material_en} {k}")

    base = [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base]
    return list(dict.fromkeys(base))

def build_webwide_queries(material_en: str, extra_keywords: list[str]) -> list[str]:
    kw = [k.replace("+", " ") for k in (extra_keywords or [])][:8]
    base = [
        f"{material_en} human rights violation",
        f"{material_en} forced labour",
        f"{material_en} child labour",
        f"{material_en} discrimination trade union rights",
        f"{material_en} pollution contamination toxic waste",
        f"{material_en} lawsuit investigation allegations",
        f"{material_en} community conflict displacement",
    ]
    for k in kw:
        base.append(f"{material_en} {k}")
    base = [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base]
    return list(dict.fromkeys(base))


# ============================================================
# 11) MAIN
# ============================================================
if __name__ == "__main__":
    max_date = lese_einstellungen()
    print(f"SETTINGS: rohst={rohst} | rawm={rawm} | max_datum={max_datum_str or '(none)'}")

    user_sites = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    schlagwoerter = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    MATERIAL_SYNONYMS = []
    for sw in schlagwoerter:
        s = sw.replace("+", " ").strip()
        if len(s) <= 3:
            continue
        if any(x in s.lower() for x in ["precious", "rare earth", "metal", "mineral", rawm.lower()]):
            MATERIAL_SYNONYMS.append(s)
    MATERIAL_SYNONYMS = list(dict.fromkeys(MATERIAL_SYNONYMS))[:10]

    all_sites = list(dict.fromkeys(user_sites + EXTRA_SITES))

    out_path = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"

    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow(["url", "date", "site", "query", "title", "snippet", "verified_in_body"])

    global_seen = set()
    collected_total = 0

    driver = make_driver()

    try:
        # Phase 1: site-restricted
        for idx_site, domain in enumerate(all_sites, start=1):
            print(f"\nüåê Site {idx_site}/{len(all_sites)}: {domain}")
            queries = build_site_queries(domain, rawm, schlagwoerter)
            print(f"   ‚Üí queries: {len(queries)}")

            for q_idx, q in enumerate(queries, start=1):
                search_url = ddg_search_url(q)
                print(f"   üîé {q_idx}/{len(queries)} | collected so far = {collected_total}")
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                print(f"      ‚Üí raw hits: {len(results)}")

                for hit in results:
                    url = (hit.get("url") or "").strip()
                    title = normalize_text(hit.get("title") or "")
                    snippet = normalize_text(hit.get("snippet") or "")
                    date_txt = normalize_text(hit.get("date_txt") or "")

                    if not url or url in global_seen:
                        continue
                    if is_bad_url(url):
                        continue
                    if not is_date_ok(date_txt, max_date):
                        continue

                    combined = (title + " " + snippet).strip()
                    has_risk_A = contains_risk_terms(combined)
                    has_mat_A  = contains_material_soft(combined, rawm, MATERIAL_SYNONYMS)

                    if not has_risk_A and not has_mat_A:
                        continue

                    verified = False
                    if not (has_risk_A and has_mat_A):
                        body_text = fetch_page_text(driver, url)
                        if not body_text:
                            continue
                        if contains_risk_terms(body_text) and contains_material_soft(body_text, rawm, MATERIAL_SYNONYMS):
                            verified = True
                        else:
                            continue
                    else:
                        verified = True

                    global_seen.add(url)
                    collected_total += 1

                    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
                        csv.writer(f, delimiter=";").writerow(
                            [url, date_txt, domain, q, title, snippet, "yes" if verified else "no"]
                        )

                    print(f"      ‚úÖ accepted | total={collected_total} | {title[:80]}")

                time.sleep(random_uniform(*PAUSE_BETWEEN_QUERIES_SEC))

            time.sleep(random_uniform(*PAUSE_BETWEEN_SITES_SEC))

        # Phase 2: fallback web-wide
        if collected_total < FALLBACK_TRIGGER_MIN_TOTAL:
            print(f"\n‚ö†Ô∏è Collected only {collected_total} after all sites. Entering web-wide fallback...")
            web_queries = build_webwide_queries(rawm, schlagwoerter)
            print(f"   ‚Üí fallback queries: {len(web_queries)} (stop at total={FALLBACK_TARGET_TOTAL})")

            for q_idx, q in enumerate(web_queries, start=1):
                if collected_total >= FALLBACK_TARGET_TOTAL:
                    break

                search_url = ddg_search_url(q)
                print(f"   üîé fallback {q_idx}/{len(web_queries)} | collected so far = {collected_total}")
                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                print(f"      ‚Üí raw hits: {len(results)}")

                for hit in results:
                    if collected_total >= FALLBACK_TARGET_TOTAL:
                        break

                    url = (hit.get("url") or "").strip()
                    title = normalize_text(hit.get("title") or "")
                    snippet = normalize_text(hit.get("snippet") or "")
                    date_txt = normalize_text(hit.get("date_txt") or "")

                    if not url or url in global_seen:
                        continue
                    if is_bad_url(url):
                        continue
                    if not is_date_ok(date_txt, max_date):
                        continue

                    combined = (title + " " + snippet).strip()
                    has_risk_A = contains_risk_terms(combined)
                    has_mat_A  = contains_material_soft(combined, rawm, MATERIAL_SYNONYMS)

                    if not has_risk_A and not has_mat_A:
                        continue

                    verified = False
                    if not (has_risk_A and has_mat_A):
                        body_text = fetch_page_text(driver, url)
                        if not body_text:
                            continue
                        if contains_risk_terms(body_text) and contains_material_soft(body_text, rawm, MATERIAL_SYNONYMS):
                            verified = True
                        else:
                            continue
                    else:
                        verified = True

                    global_seen.add(url)
                    collected_total += 1

                    dom = urlparse(url).netloc
                    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
                        csv.writer(f, delimiter=";").writerow(
                            [url, date_txt, dom, q, title, snippet, "yes" if verified else "no"]
                        )

                    print(f"      ‚úÖ accepted | total={collected_total} | {title[:80]}")

                time.sleep(random_uniform(*PAUSE_BETWEEN_QUERIES_SEC))

        print(f"\n‚úÖ Done. Total collected: {collected_total}")
        print(f"‚û°Ô∏è Output: {out_path}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass
