# -*- coding: utf-8 -*-
# ============================================================
# KI Risiko â€“ STRICT material-based HR / ENV scraper (v2)
# ------------------------------------------------------------
# Reads (from: C:\Users\SHTANDO\Desktop\KI Risko\Textdoks_fÃ¼r_Einstellungen_der_Suche\):
#   1) Einstellungen_Analyse.csv
#   2) Bekannte_Seiten.csv
#   3) SchlagwÃ¶rter.csv
#
# Writes:
#   C:\Users\SHTANDO\Desktop\KI Risko\gescrapte_Artikel-Links\gescrapte_links_<rohst>.csv
#
# Goal:
#   - Prioritize YOUR known sites, but also add extra trustworthy sites (built-in list)
#   - Only keep hits that mention the required material (rawm) + risk terms
#   - Avoid noisy sources: social media, jobs/careers, finance/IR pages, PDFs
#   - De-duplicate across all sites/queries
#
# Notes:
#   - If you get PermissionError: close the output CSV in Excel before running.
# ============================================================

import os
import csv
import time
import re
from pathlib import Path
from urllib.parse import quote_plus, urlparse

import dateparser
from bs4 import BeautifulSoup  # kept (sometimes useful when DDG markup changes)

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ============================================================
# 1) PATHS / CONFIG
# ============================================================

PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")

SETTINGS_DIR = PROJECT_ROOT / "Textdoks_fÃ¼r_Einstellungen_der_Suche"
SCRAPED_DIR  = PROJECT_ROOT / "gescrapte_Artikel-Links"
SCRAPED_DIR.mkdir(parents=True, exist_ok=True)

EINSTELLUNGEN_CSV   = SETTINGS_DIR / "Einstellungen_Analyse.csv"
BEKANNTE_SEITEN_CSV = SETTINGS_DIR / "Bekannte_Seiten.csv"
SCHLAGWORT_CSV      = SETTINGS_DIR / "SchlagwÃ¶rter.csv"

MAX_ARTICLES_PER_SITE = 3
MAX_DDG_PAGES = 2
HEADLESS = False

# How many keywords from SchlagwÃ¶rter.csv to use per site (higher = more recall, slower)
MAX_KEYWORDS_PER_SITE = 6

# ============================================================
# 2) RISK TERMS (EXTENDED)
# ============================================================

RISK_TERMS_BASE = [
    # general HR
    "human rights", "violation", "abuse",
    "forced labour", "forced labor",
    "child labour", "child labor",
    "underage", "modern slavery",
    "unsafe working conditions", "hazardous work", "worker exploitation",
    "wage theft", "trafficking",

    # discrimination (your ask)
    "discrimination", "age discrimination", "racial discrimination", "religious discrimination",
    "race", "religion", "age",

    # trade union freedoms (your ask)
    "trade union", "union rights", "union busting", "collective bargaining", "right to strike",

    # environment / env-human-rights intersection
    "pollution", "environmental damage", "contamination", "toxic waste", "spill",
    "tailings", "tailings dam", "deforestation",
    "community conflict", "protest", "displacement",
]

# Exclude obvious noise (even if they contain risk words)
BAD_DOMAIN_SUBSTRINGS = [
    "facebook.com", "m.facebook.com",
    "youtube.com", "youtu.be",
    "instagram.com",
    "tiktok.com",
    "twitter.com", "x.com",
    "pinterest.",

    # often noisy
    "reddit.com",
]

BAD_URL_HINTS = [
    "/jobs", "/job", "/careers", "/career", "/karriere", "/stellen", "/vacancies",
    "/investor", "/investors", "/ir", "/investor-relations",
    "/stock", "/shares", "/quote", "/pricing",
    "/privacy", "/cookie", "/cookies", "/terms",
]

BAD_FILE_EXTS = (".pdf", ".doc", ".docx", ".ppt", ".pptx")


# ============================================================
# 3) EXTRA TRUSTWORTHY SITES (added on top of your Bekannte_Seiten.csv)
#    These are domains to be used with site: queries.
#    Your sites remain PRIORITY because we keep their order first.
# ============================================================

EXTRA_TRUSTWORTHY_SITES = [
    # NGOs / intergov
    "humanrightswatch.org",
    "amnesty.org",
    "ilo.org",
    "oecd.org",
    "ohchr.org",
    "un.org",
    "unep.org",
    "worldbank.org",

    # investigative / journalism
    "reuters.com",
    "apnews.com",
    "bbc.co.uk",
    "dw.com",
    "theguardian.com",
    "ft.com",

    # environment / regulation (sometimes very good for incidents)
    "epa.gov",
    "ec.europa.eu",
]


# ============================================================
# 4) READ SETTINGS
# ============================================================

rohst = rawm = max_datum_str = None

def lese_einstellungen():
    """
    Uses the same indices youâ€™ve been using:
      rows[5]  = rohst (German)
      rows[8]  = rawm  (English)
      rows[11] = max_datum (e.g., 20.11.2020)
    """
    global rohst, rawm, max_datum_str

    if not EINSTELLUNGEN_CSV.exists():
        raise FileNotFoundError(f"Settings file not found: {EINSTELLUNGEN_CSV}")

    with EINSTELLUNGEN_CSV.open("r", encoding="utf-8-sig", newline="") as f:
        rows = list(csv.reader(f, delimiter=","))

    def safe_get(idx: int, default: str = "") -> str:
        if len(rows) > idx and rows[idx] and rows[idx][0].strip():
            return rows[idx][0].strip()
        return default

    rohst = safe_get(5, "Stahl")
    rawm = safe_get(8, "Steel")
    max_datum_str = safe_get(11, "20.11.2020")

    print("SETTINGS:")
    print("  rohst     =", rohst)
    print("  rawm      =", rawm)
    print("  max_datum =", max_datum_str)

    # parse date (DE/EN)
    parsed = dateparser.parse(max_datum_str, languages=["de", "en"])
    return parsed.date() if parsed else None


# ============================================================
# 5) CSV LIST LOADERS
# ============================================================

def lese_einfache_liste(pfad: Path, delimiter: str = ",") -> list[str]:
    if not pfad.exists():
        print("âš ï¸ File not found:", pfad)
        return []
    items: list[str] = []
    with pfad.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.reader(f, delimiter=delimiter)
        for row in r:
            for cell in row:
                val = (cell or "").strip()
                if val:
                    items.append(val)
    # dedupe keep order
    seen = set()
    uniq = []
    for it in items:
        k = it.strip().lower()
        if k not in seen:
            seen.add(k)
            uniq.append(it.strip())
    return uniq

def normalize_domain(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^https?://", "", s, flags=re.IGNORECASE)
    s = s.split("/")[0].strip()
    s = s.replace("www.", "").strip()
    return s

def merge_sites(user_sites: list[str]) -> list[str]:
    # user sites first (priority), then extras not already present
    out: list[str] = []
    seen = set()

    def add_site(site: str):
        d = normalize_domain(site)
        if not d:
            return
        if d not in seen:
            seen.add(d)
            out.append(d)

    for s in user_sites:
        add_site(s)
    for s in EXTRA_TRUSTWORTHY_SITES:
        add_site(s)

    return out


# ============================================================
# 6) FILTERS
# ============================================================

def contains_required_material(text: str, material: str) -> bool:
    """
    Strict: the required material (rawm) must appear in title/snippet.
    This prevents random cobalt/steel articles when you want gold, etc.
    """
    if not text or not material:
        return False
    t = text.lower()
    m = material.lower().strip()
    # allow small variations (very conservative)
    variants = {
        m,
        m.replace("-", " "),
        m.replace(" ", "-"),
    }
    return any(v in t for v in variants if v)

def contains_risk_terms(text: str, extra_terms: list[str]) -> bool:
    """
    Risk term must appear in title/snippet to reduce noise.
    Also uses SchlagwÃ¶rter.csv (where you may have 'child + labour' etc).
    """
    t = (text or "").lower()

    for kw in RISK_TERMS_BASE:
        if kw.lower() in t:
            return True

    for sw in extra_terms:
        sw_clean = (sw or "").replace("+", " ").strip().lower()
        if not sw_clean:
            continue
        # check either full phrase or any token (token check gives more recall)
        if sw_clean in t:
            return True
        for token in sw_clean.split():
            if len(token) >= 4 and token in t:
                return True

    return False

def is_bad_result(url: str, title: str, snippet: str) -> bool:
    if not url:
        return True

    u = url.strip()
    ul = u.lower()

    # file types
    if any(ul.endswith(ext) for ext in BAD_FILE_EXTS):
        return True

    # bad domains
    dom = urlparse(u).netloc.lower()
    if any(bad in dom for bad in BAD_DOMAIN_SUBSTRINGS):
        return True

    # bad url hints (jobs/finance/IR etc.)
    if any(hint in ul for hint in BAD_URL_HINTS):
        return True

    # extra noise heuristics
    combined = f"{title} {snippet}".lower()
    if "policy" in combined and "violation" not in combined and "abuse" not in combined:
        # policies are usually PR; keep only if explicitly about violations/abuse
        return True

    return False


# ============================================================
# 7) SELENIUM
# ============================================================

def make_driver() -> webdriver.Chrome:
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--start-maximized")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return webdriver.Chrome(options=opts)


# ============================================================
# 8) DUCKDUCKGO QUERY BUILDING / SCRAPING
# ============================================================

def ddg_search_url(query: str) -> str:
    return "https://duckduckgo.com/?q=" + quote_plus(query) + "&kp=1&ia=web"

def build_queries_for_site(domain: str, rawm: str, schlagwoerter: list[str]) -> list[str]:
    """
    No quotes, no OR. Also removes any accidental quotes.
    Keep it recall-friendly but still material-focused.
    """
    # base HR/env combos
    base_queries = [
        f"site:{domain} {rawm} human rights violation",
        f"site:{domain} {rawm} forced labour",
        f"site:{domain} {rawm} child labour",
        f"site:{domain} {rawm} pollution spill contamination",
        f"site:{domain} {rawm} discrimination trade union collective bargaining",
        f"site:{domain} {rawm} unsafe working conditions hazardous work",
    ]

    # add keyword queries
    kws = schlagwoerter[:MAX_KEYWORDS_PER_SITE]
    for sw in kws:
        sw_clean = (sw or "").replace("+", " ").strip()
        if sw_clean:
            base_queries.append(f"site:{domain} {rawm} {sw_clean}")

    # sanitize quotes (ASCII + smart quotes)
    sanitized = [re.sub(r'[\u201C\u201D\"\']', "", q) for q in base_queries]
    return sanitized

def scrape_ddg_results(driver: webdriver.Chrome, search_url: str, max_pages: int) -> list[dict]:
    """
    Returns list of dicts: {url, title, snippet, date_txt}
    """
    items: list[dict] = []

    try:
        driver.get(search_url)
    except Exception:
        return items

    pages_seen = 0
    while pages_seen < max_pages:
        time.sleep(2)

        html = (driver.page_source or "").lower()
        if "filter lÃ¶schen" in html or "try again" in html:
            # DDG sometimes shows this. Skip query quickly.
            break

        try:
            WebDriverWait(driver, 8).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article[data-testid="result"]'))
            )
        except Exception:
            # If DDG changes markup, attempt a light parse:
            soup = BeautifulSoup(driver.page_source, "html.parser")
            txt = soup.get_text(" ", strip=True).lower()
            if "no results" in txt or "keine ergebnisse" in txt:
                break
            break

        articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="result"]')
        for art in articles:
            try:
                a = art.find_element(By.CSS_SELECTOR, "a[data-testid='result-title-a']")
                url = (a.get_attribute("href") or "").strip()
                title = (a.text or "").strip()
                if not url or "duckduckgo.com" in url:
                    continue
            except Exception:
                continue

            snippet = ""
            try:
                snippet = (art.find_element(By.CSS_SELECTOR, "div[data-testid='result-snippet']").text or "").strip()
            except Exception:
                pass

            date_txt = ""
            try:
                date_txt = (art.find_element(By.CSS_SELECTOR, "span.MILR5XIVy9h75WrLvKiq").text or "").strip()
            except Exception:
                pass

            items.append({"url": url, "title": title, "snippet": snippet, "date_txt": date_txt})

        # more results
        try:
            more_btn = WebDriverWait(driver, 2).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "#more-results"))
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", more_btn)
            time.sleep(0.8)
            more_btn.click()
            pages_seen += 1
            time.sleep(1.2)
        except Exception:
            break

    return items


# ============================================================
# 9) DATE CHECK (optional, permissive)
# ============================================================

def is_date_ok(date_txt: str, min_date, _url: str) -> bool:
    """
    Very permissive: if can't parse, allow.
    """
    if min_date is None:
        return True
    if not date_txt:
        return True
    parsed = dateparser.parse(date_txt, languages=["de", "en"])
    if not parsed:
        return True
    return parsed.date() >= min_date


# ============================================================
# 10) MAIN
# ============================================================

if __name__ == "__main__":
    min_date = lese_einstellungen()

    user_sites_raw = lese_einfache_liste(BEKANNTE_SEITEN_CSV, delimiter=",")
    schlagwoerter  = lese_einfache_liste(SCHLAGWORT_CSV, delimiter=",")

    sites = merge_sites(user_sites_raw)

    print("\nUser sites found:", len(user_sites_raw))
    print("Total sites used (user + extras):", len(sites))
    print("Material (rawm) strict filter:", rawm)
    print("Keywords loaded:", len(schlagwoerter))

    out_path = SCRAPED_DIR / f"gescrapte_links_{rohst}.csv"

    # global de-dup across all sites and queries
    global_seen_urls = set()

    # write header
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        csv.writer(f, delimiter=";").writerow(
            ["gefundener link", "datum", "seite", "suche", "titel", "snippet"]
        )

    driver = make_driver()

    try:
        for idx_site, domain in enumerate(sites, start=1):
            print(f"\nðŸŒ Site {idx_site}/{len(sites)}: {domain}")

            queries = build_queries_for_site(domain, rawm, schlagwoerter)
            print(f"   â†’ Queries: {len(queries)}")

            accepted_for_site = 0
            seen_urls_for_site = set()

            for q_idx, q in enumerate(queries, start=1):
                if accepted_for_site >= MAX_ARTICLES_PER_SITE:
                    print(f"   âœ… Limit reached ({MAX_ARTICLES_PER_SITE}) for {domain}")
                    break

                search_url = ddg_search_url(q)
                print(f"   ðŸ”Ž Query {q_idx}/{len(queries)}")

                results = scrape_ddg_results(driver, search_url, max_pages=MAX_DDG_PAGES)
                if not results:
                    continue

                for res in results:
                    if accepted_for_site >= MAX_ARTICLES_PER_SITE:
                        break

                    url = res["url"]
                    title = res["title"]
                    snippet = res["snippet"]
                    date_txt = res.get("date_txt", "")

                    if not url:
                        continue
                    if url in global_seen_urls or url in seen_urls_for_site:
                        continue

                    if is_bad_result(url, title, snippet):
                        continue

                    if not is_date_ok(date_txt, min_date, url):
                        continue

                    combined = f"{title} {snippet}".strip()
                    # strict material presence
                    if not contains_required_material(combined, rawm):
                        continue
                    # require risk term presence
                    if not contains_risk_terms(combined, schlagwoerter):
                        continue

                    # accept
                    global_seen_urls.add(url)
                    seen_urls_for_site.add(url)
                    accepted_for_site += 1

                    print(f"      âœ… ACCEPTED #{accepted_for_site}: {title}")

                    with out_path.open("a", newline="", encoding="utf-8-sig") as f:
                        csv.writer(f, delimiter=";").writerow(
                            [url, date_txt, domain, q, title, snippet]
                        )

                time.sleep(1.5)

            if accepted_for_site == 0:
                print("   â†’ No accepted hits for this site.")
            else:
                print(f"   â†’ Accepted for {domain}: {accepted_for_site}")

            time.sleep(1.5)

    finally:
        try:
            driver.quit()
        except Exception:
            pass

    print("\nâœ… Done. Output written to:")
    print(out_path)
