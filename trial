# -*- coding: utf-8 -*-
"""
ANALYSE-SKRIPT (NO LLM) mit Risikokategorien (A–M)
- Deterministic, no Azure/OpenAI calls.
- Keeps the SAME output CSV columns as your current pipeline.
- Reads:  C:/Users/SHTANDO/Desktop/KI Risko/gescrapte_Artikel-Links/gescrapte_links_<rohst>.csv
- Writes: C:/Users/SHTANDO/Desktop/KI Risko/Analyse_Tabelle/analyse_tabelle_<rohst>.csv

Key changes vs your LLM version:
✅ No OpenAI/Azure usage at all (no API keys needed)
✅ Analyses ALL links (no random sample)
✅ Mandatory-material gate: skips if required material not present in extracted body text
✅ Robust-ish extraction with requests + BeautifulSoup; Selenium fallback if needed
✅ Basic anti-block handling (User-Agent, timeouts)
"""

import csv
import re
import time
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

import requests
import dateparser
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By


# --------------------------------------------------
# 0) Paths
# --------------------------------------------------
PROJECT_ROOT = Path(r"C:\Users\SHTANDO\Desktop\KI Risko")
SETTINGS_DIR = PROJECT_ROOT / "Textdoks_für_Einstellungen_der_Suche"
SCRAPED_DIR = PROJECT_ROOT / "gescrapte_Artikel-Links"
OUTPUT_DIR = PROJECT_ROOT / "Analyse_Tabelle"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

WRITE_DEBUG = True  # keep rows even if all “Not in Text”

# --------------------------------------------------
# 1) Read settings
# --------------------------------------------------
rohst = rawm = max_datum = None
seitenumfang = 3  # kept for compatibility, not used here

def lese_einstellungen():
    global rohst, rawm, max_datum, seitenumfang
    settings_path = SETTINGS_DIR / "Einstellungen_Analyse.csv"
    with settings_path.open("r", encoding="utf-8-sig") as f:
        rows = list(csv.reader(f, delimiter=","))

    def get_row(idx, default=""):
        return rows[idx][0].strip() if len(rows) > idx and len(rows[idx]) > 0 else default

    rohst        = get_row(5, "Stahl")
    rawm         = get_row(8, "Steel")
    max_datum    = get_row(11, "20.11.2020")
    seitenumfang = int(get_row(14, "3") or "3")


# --------------------------------------------------
# 2) Selenium driver (only if needed)
# --------------------------------------------------
HEADLESS = True

def make_driver():
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--enable-javascript")
    opts.add_argument("--window-size=1400,2200")
    try:
        return webdriver.Chrome(options=opts)
    except Exception:
        # fallback legacy headless
        opts.add_argument("--headless")
        return webdriver.Chrome(options=opts)


# --------------------------------------------------
# 3) Fetch + extract article text (requests first, selenium fallback)
# --------------------------------------------------
UA = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
      "(KHTML, like Gecko) Chrome/121.0 Safari/537.36")

BLOCK_MARKERS = [
    "captcha", "verify you are human", "access denied", "unusual traffic",
    "cloudflare", "blocked", "bot detection"
]

def looks_blocked(text: str) -> bool:
    t = (text or "").lower()
    return any(m in t for m in BLOCK_MARKERS)

def clean_visible_text(html: str) -> str:
    soup = BeautifulSoup(html or "", "html.parser")

    # remove obvious boilerplate
    for tag in soup(["script", "style", "noscript", "svg", "canvas", "header", "footer", "nav", "aside"]):
        tag.decompose()

    # prefer <article>
    art = soup.find("article")
    if art:
        txt = art.get_text(separator="\n", strip=True)
        return txt

    # fallback: main/body
    main = soup.find("main")
    if main:
        return main.get_text(separator="\n", strip=True)

    body = soup.find("body")
    if body:
        return body.get_text(separator="\n", strip=True)

    return soup.get_text(separator="\n", strip=True)

def fetch_html_requests(url: str, timeout_sec: int = 18) -> str:
    try:
        r = requests.get(
            url,
            headers={"User-Agent": UA, "Accept-Language": "en,de;q=0.9"},
            timeout=timeout_sec,
            allow_redirects=True
        )
        # Some sites return 403/406 etc. still with HTML
        if r.text:
            return r.text
        return ""
    except Exception:
        return ""

def fetch_html_selenium(url: str, timeout_sec: int = 18) -> str:
    d = make_driver()
    try:
        d.set_page_load_timeout(timeout_sec)
        d.get(url)
        time.sleep(2.5)
        return d.page_source or ""
    finally:
        try:
            d.quit()
        except Exception:
            pass

def url_zu_artikeltext(url: str) -> str:
    # 1) requests
    html = fetch_html_requests(url)
    txt = clean_visible_text(html)
    if txt and not looks_blocked(txt):
        return txt

    # 2) selenium fallback if blocked/empty
    html2 = fetch_html_selenium(url)
    txt2 = clean_visible_text(html2)
    if txt2:
        if looks_blocked(txt2):
            raise RuntimeError("blocked_or_captcha")
        return txt2

    raise RuntimeError("empty_text")


# --------------------------------------------------
# 4) Deterministic extraction / classification
# --------------------------------------------------
RISIKOKATEGORIEN = {
    "A": "Working conditions, including occupational health and safety",
    "B": "Child labour",
    "C": "Modern slavery, including forced labour",
    "D": "Community and indigenous peoples’ rights",
    "E": "Excessive violence by private and public security forces",
    "F": "Environmental risks with impact on human rights",
    "G": "Business conduct in Conflict and High Risk Areas",
    "H": "Serious human rights abuses",
    "I": "Biodiversity",
    "J": "Water",
    "K": "Air",
    "L": "Soil",
    "M": "Waste, hazardous substances, and plant safety",
}

# Category keyword patterns (broad/stem-like)
CAT_PATTERNS = {
    "A": [r"\bunsafe\b", r"\bsafety\b", r"\binjur(y|ies)\b", r"\bfatalit(y|ies)\b", r"\baccident(s)?\b",
          r"\bworking conditions?\b", r"\boccupational\b", r"\bhealth and safety\b", r"\bwage(s)?\b", r"\bovertime\b"],
    "B": [r"\bchild labor\b", r"\bchild labour\b", r"\bunderage\b", r"\bminor(s)?\b", r"\bchildren\b.*\bwork\b"],
    "C": [r"\bforced labor\b", r"\bforced labour\b", r"\bmodern slavery\b", r"\btraffick(ing|ed)\b",
          r"\bdebt bondage\b", r"\bcoerc(ed|ion)\b"],
    "D": [r"\bindigenous\b", r"\bcommunity\b", r"\bdisplacement\b", r"\bland rights?\b", r"\bfree prior informed consent\b",
          r"\bFPIC\b", r"\bresettlement\b"],
    "E": [r"\bsecurity forces?\b", r"\bpolice\b", r"\bmilitar(y|ized)\b", r"\bshoot(ing|ings)\b", r"\bviolent\b",
          r"\bassault\b", r"\bexcessive force\b"],
    "F": [r"\benvironment(al)?\b", r"\bimpact\b", r"\bdamage\b", r"\b污染\b", r"\bemissions?\b", r"\bclimate\b"],
    "G": [r"\bconflict\b", r"\barmed\b", r"\bmilitia\b", r"\bhigh[- ]risk\b", r"\bsanction(s)?\b", r"\bwar\b"],
    "H": [r"\btorture\b", r"\bkilling(s)?\b", r"\bextrajudicial\b", r"\bgenocide\b", r"\bcrimes against humanity\b"],
    "I": [r"\bbiodiversity\b", r"\bhabitat\b", r"\bendangered\b", r"\bwildlife\b", r"\becosystem(s)?\b"],
    "J": [r"\bwater\b", r"\briver(s)?\b", r"\bgroundwater\b", r"\baquifer\b", r"\bdrinking water\b", r"\bwetland(s)?\b"],
    "K": [r"\bair\b", r"\bdust\b", r"\bparticulate\b", r"\bsmog\b", r"\bfume(s)?\b", r"\bSO2\b", r"\bNOx\b"],
    "L": [r"\bsoil\b", r"\bland contamination\b", r"\btopsoil\b", r"\bheavy metal(s)?\b"],
    "M": [r"\bwaste\b", r"\btoxic\b", r"\bhazard(ous)?\b", r"\btailing(s)?\b", r"\btailings dam\b",
          r"\bspill(s|ed)?\b", r"\bcyanide\b", r"\bmercury\b", r"\barsenic\b", r"\bleak(s|ed)?\b"],
}

# Risk “harms” extraction patterns
HARM_PATTERNS = [
    (r"\bchild labou?r\b|\bunderage\b", "Child labour"),
    (r"\bforced labou?r\b|\bmodern slavery\b|\btraffick(ing|ed)\b", "Forced labour / modern slavery"),
    (r"\bdiscriminat(ion|e|ed)\b|\brace\b|\breligion\b|\bage\b", "Discrimination (age/race/religion etc.)"),
    (r"\btrade union\b|\bunion\b|\bcollective bargaining\b|\bunion[- ]bust", "Trade union freedoms / collective bargaining"),
    (r"\bpollut(ion|ed|ing)\b|\bcontaminat(e|ed|ion)\b|\btoxic\b|\bspill\b", "Pollution / contamination / toxic release"),
    (r"\bdeforest(ation|ed|ing)\b", "Deforestation"),
    (r"\bdisplacement\b|\bresettlement\b|\bland rights?\b", "Displacement / land-rights impacts"),
    (r"\bviolence\b|\bshoot(ing|ings)\b|\bassault\b", "Violence / security-force abuse"),
]

SEVERITY_RULES = [
    ("critical", [r"\bdeath(s)?\b", r"\bkilled\b", r"\bfatalit(y|ies)\b", r"\bmassive spill\b", r"\btailings dam\b", r"\bgenocide\b"]),
    ("major",    [r"\bserious\b", r"\bsevere\b", r"\bmajor\b", r"\blarge\b.*\bspill\b", r"\bforced labou?r\b", r"\bchild labou?r\b"]),
    ("moderate", [r"\bmoderate\b", r"\balleg(ation|ed|edly)\b", r"\blawsuit\b", r"\binvestigation\b", r"\bcomplaint\b"]),
]

COUNTRY_HINTS = [
    # minimal, common set (extend if you want)
    "germany","france","italy","spain","portugal","poland","sweden","norway","finland","uk","united kingdom",
    "usa","united states","canada","mexico","brazil","argentina","chile","peru","colombia",
    "china","india","indonesia","philippines","vietnam","thailand","malaysia","australia",
    "south africa","zambia","drc","democratic republic of the congo","congo",
    "rwanda","uganda","kenya","tanzania","mozambique","madagascar",
    "russia","ukraine","turkey","iran","iraq","saudi arabia","uae"
]

STAGE_HINTS = [
    (r"\bexploration\b|\bprospecting\b", "Exploration"),
    (r"\bmine\b|\bmining\b|\bextraction\b|\bpit\b|\bunderground\b", "Mining / extraction"),
    (r"\bprocessing\b|\bbeneficiation\b|\bconcentrat(e|ion)\b", "Processing / beneficiation"),
    (r"\bsmelt(ing|er)\b|\brefin(ing|ery)\b", "Smelting / refining"),
    (r"\bmanufactur(e|ing)\b|\bfabricat(e|ion)\b|\bproduction\b", "Manufacturing / production"),
    (r"\brecycl(e|ing)\b|\bsecondary\b", "Recycling / secondary material"),
]

YEAR_RE = re.compile(r"\b(19\d{2}|20\d{2})\b")
CAPITALISED_ORG_RE = re.compile(r"\b([A-Z][A-Za-z0-9&\-.]{2,}(?:\s+[A-Z][A-Za-z0-9&\-.]{2,}){0,3})\b")

def expand_categories(raw_cats: str) -> str:
    if not raw_cats or raw_cats.lower().startswith("not in text"):
        return raw_cats
    parts = [p.strip().upper() for p in raw_cats.replace(";", ",").split(",") if p.strip()]
    seen, expanded = set(), []
    for p in parts:
        if p not in seen:
            seen.add(p)
            label = RISIKOKATEGORIEN.get(p, "")
            expanded.append(f"{p} ({label})" if label else p)
    return ", ".join(expanded)

def mandatory_material_present(text: str, material: str) -> bool:
    if not material or not text:
        return False
    return material.lower() in text.lower()

def pick_categories(text: str) -> str:
    t = text.lower()
    found = []
    for code, pats in CAT_PATTERNS.items():
        for p in pats:
            if re.search(p, t, flags=re.IGNORECASE):
                found.append(code)
                break
    if not found:
        return "Not in Text"
    # keep stable order A..M
    found_sorted = [c for c in "ABCDEFGHIJKLM" if c in set(found)]
    return expand_categories(", ".join(found_sorted))

def extract_harms(text: str) -> str:
    t = text.lower()
    harms = []
    for pat, label in HARM_PATTERNS:
        if re.search(pat, t, flags=re.IGNORECASE):
            harms.append(label)
    return "; ".join(dict.fromkeys(harms)) if harms else "Not in Text"

def extract_years(text: str) -> str:
    ys = YEAR_RE.findall(text or "")
    ys = sorted(set(ys))
    return ", ".join(ys) if ys else "Not in Text"

def extract_countries(text: str) -> str:
    t = (text or "").lower()
    hits = []
    for c in COUNTRY_HINTS:
        if re.search(r"\b" + re.escape(c) + r"\b", t):
            hits.append(c.title() if c.islower() else c)
    hits = list(dict.fromkeys(hits))
    return ", ".join(hits) if hits else "Not in Text"

def extract_stage(text: str) -> str:
    t = (text or "").lower()
    for pat, label in STAGE_HINTS:
        if re.search(pat, t):
            return label
    return "Not in Text"

def extract_companies(text: str) -> str:
    """
    Very rough: pulls capitalised sequences and filters obvious non-company words.
    You can improve later with spaCy when your Python version supports it.
    """
    if not text:
        return "Not in Text"

    blacklist = {
        "The", "This", "That", "These", "Those", "Reuters", "Mongabay", "Guardian",
        "Human", "Rights", "Environmental", "Report", "Reports", "Company", "Companies",
        "Government", "Ministry", "Court", "University", "Institute", "Commission",
        "January","February","March","April","May","June","July","August","September","October","November","December"
    }

    cands = CAPITALISED_ORG_RE.findall(text)
    cleaned = []
    for c in cands:
        c = c.strip(" -–—,.;:()[]")
        if not c:
            continue
        if c in blacklist:
            continue
        # discard if looks like a sentence start fragment
        if len(c) < 3:
            continue
        # keep if contains Corp/Inc/AG/GmbH/SA/PLC/Ltd/LLC etc OR 2+ words
        if re.search(r"\b(Inc|Corp|Corporation|Ltd|LLC|AG|GmbH|S\.A\.|SA|PLC|BV|NV)\b", c):
            cleaned.append(c)
        elif len(c.split()) >= 2:
            cleaned.append(c)

    cleaned = list(dict.fromkeys(cleaned))[:8]
    return "; ".join(cleaned) if cleaned else "Not in Text"

def extract_groups(text: str) -> str:
    t = (text or "").lower()
    groups = []
    if re.search(r"\bworker(s)?\b|\blabou?rer(s)?\b|\bemployees?\b", t):
        groups.append("Workers / employees")
    if re.search(r"\bcommunity\b|\bresident(s)?\b|\bvillager(s)?\b", t):
        groups.append("Local communities")
    if re.search(r"\bindigenous\b", t):
        groups.append("Indigenous peoples")
    if re.search(r"\bchildren\b|\bchild\b|\bminor(s)?\b", t):
        groups.append("Children")
    if re.search(r"\becosystem(s)?\b|\bwildlife\b|\bhabitat\b|\briver\b|\bforest\b", t):
        groups.append("Ecosystems / wildlife")
    return "; ".join(dict.fromkeys(groups)) if groups else "Not in Text"

def estimate_min_people(text: str) -> tuple[str, str]:
    """
    Conservative: only extract explicit integers with context.
    Otherwise Not in Text.
    """
    if not text:
        return "Not in Text", "Not in Text"

    # examples: "100 workers", "2,000 residents"
    m = re.search(r"\b(\d{1,3}(?:,\d{3})+|\d{1,7})\b\s+(workers?|employees?|people|residents?|villagers?)\b", text, flags=re.IGNORECASE)
    if m:
        num = m.group(1).replace(",", "")
        return num, f"Found in text: '{m.group(0)}'"

    return "Not in Text", "Not in Text"

def severity_from_text(text: str, harms: str) -> tuple[str, str]:
    t = (text or "").lower()

    # explicit rules first
    for sev, pats in SEVERITY_RULES:
        for p in pats:
            if re.search(p, t):
                return sev, f"Matched pattern: {p}"

    # if child/forced labour present -> at least major
    if harms and harms != "Not in Text":
        if "Child labour" in harms or "Forced labour" in harms:
            return "major", "Harms include child/forced labour"

    # otherwise moderate if allegations/lawsuit/investigation
    if re.search(r"\balleg(ation|ed|edly)\b|\blawsuit\b|\binvestigation\b|\bcomplaint\b", t):
        return "moderate", "Mentions allegations/lawsuit/investigation"

    return "minor", "No strong severity markers found"


def short_reason(text: str, max_sentences: int = 4) -> str:
    if not text:
        return "Not in Text"
    # take first few non-empty sentences
    parts = re.split(r"(?<=[.!?])\s+", text.strip())
    parts = [p.strip() for p in parts if p.strip()]
    if not parts:
        return "Not in Text"
    out = " ".join(parts[:max_sentences])
    # clamp length
    return out[:900]


def analyse_artikel_deterministisch(rohstoff_name: str, artikeltext: str) -> list[str]:
    """
    Returns 13 fields in the exact order expected by your pipeline.
    """
    cats = pick_categories(artikeltext)
    harms = extract_harms(artikeltext)
    begr = short_reason(artikeltext, 3)

    years = extract_years(artikeltext)
    countries = extract_countries(artikeltext)

    # "Ort(e)" not reliably extractable without NER -> keep Not in Text
    locations = "Not in Text"

    stage = extract_stage(artikeltext)
    companies = extract_companies(artikeltext)
    groups = extract_groups(artikeltext)

    min_people, min_reason = estimate_min_people(artikeltext)

    severity, sev_reason = severity_from_text(artikeltext, harms)

    return [
        cats,
        harms,
        begr,
        years,
        countries,
        locations,
        stage,
        companies,
        groups,
        min_people,
        min_reason,
        severity,
        sev_reason
    ]


# --------------------------------------------------
# 5) CSV helpers
# --------------------------------------------------
def lese_links_aus_scrape_csv(rohstoff: str) -> list[str]:
    fpath = SCRAPED_DIR / f"gescrapte_links_{rohstoff}.csv"
    if not fpath.exists():
        raise FileNotFoundError(f"Scrape CSV not found: {fpath}")

    links = []
    with fpath.open("r", encoding="utf-8-sig") as f:
        r = csv.reader(f, delimiter=";")
        next(r, None)
        for row in r:
            if row and row[0].strip():
                links.append(row[0].strip())
    return list(dict.fromkeys(links))

def schreibe_analyse_csv(rohstoff: str, daten_zeilen: list[list[str]]):
    out_path = OUTPUT_DIR / f"analyse_tabelle_{rohstoff}.csv"
    with out_path.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f, delimiter=";")
        w.writerow([
            "link",
            "Risikokategorien (A–M)",
            "Arten der Menschenrechts-/Umweltschädigungen",
            "Begründung",
            "Jahr(e)",
            "Land/Länder",
            "Ort(e)",
            "Stufe der Rohstoffgewinnung",
            "Involvierte Unternehmen",
            "Betroffene Personengruppen/Ökosysteme",
            "Mindestzahl der betroffenen Personen",
            "Begründung Mindestzahl",
            "Schwere",
            "Begründung Schwere",
        ])
        for row in daten_zeilen:
            if not WRITE_DEBUG:
                all_not = all(
                    (isinstance(x, str) and x.strip().lower() == "not in text")
                    for x in row[1:]
                )
                if all_not:
                    continue
            w.writerow(row)
    print(f"✅ Analyse geschrieben nach: {out_path}")


# --------------------------------------------------
# 6) Main process (ANALYZE ALL LINKS)
# --------------------------------------------------
if __name__ == "__main__":
    lese_einstellungen()
    print(f"SETTINGS:\n  rohst     = {rohst}\n  rawm      = {rawm}\n  max_datum = {max_datum}")

    links = lese_links_aus_scrape_csv(rohst)
    print("Links gefunden:", len(links))

    rows, failed = [], []
    analysed_count = 0
    skipped_material = 0

    for i, link in enumerate(links, start=1):
        print(f"[{i}/{len(links)}] analysiere: {link}")

        try:
            artikel = url_zu_artikeltext(link)

            # =========================================================
            # ✅ MANDATORY MATERIAL GATE
            # Skip if required material is NOT in the extracted body text
            # =========================================================
            if rawm and rawm.strip() and not mandatory_material_present(artikel, rawm):
                skipped_material += 1
                print("  ⛔ skipped (required material not in body text)")
                continue

            analysed = analyse_artikel_deterministisch(rawm, artikel)
            cleaned = [x.replace("\n", " ").replace("\r", " ").strip(" -") if isinstance(x, str) else x for x in analysed]

            rows.append([link] + cleaned)
            analysed_count += 1
            print("  ✅ done")

            # small pause to reduce blocks
            time.sleep(1.2)

        except Exception as e:
            print("  ❌ error:", e)
            failed.append(link)

            # backoff a bit
            time.sleep(2.0)

    schreibe_analyse_csv(rohst, rows)

    # Write failed URLs
    if failed:
        fail_path = OUTPUT_DIR / f"fehlgeschlagene_urls_analyse_{rohst}.csv"
        with fail_path.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.writer(f, delimiter=";")
            w.writerow(["link"])
            for u in failed:
                w.writerow([u])
        print(f"⚠️ Fehlgeschlagen: {len(failed)} → {fail_path}")

    print(f"\nDONE.\n  Analysed: {analysed_count}\n  Skipped (material missing): {skipped_material}\n  Failed: {len(failed)}")
