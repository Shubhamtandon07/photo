# -*- coding: utf-8 -*-
"""
INPUTS:
  - .msg/.MSG (Outlook emails)
  - .html/.htm (already-exported emails)

OUTPUTS (per input file):
  OUT_DIR/sanitized_text/<name>.txt
  OUT_DIR/sanitized_html/<name>.html

ALSO:
  OUT_DIR/sanitized_jsonl/dataset.jsonl   (one record per input)
  OUT_DIR/entity_map.json                 (stable pseudonyms)
  OUT_DIR/errors.log
  OUT_DIR/ocr_missing.log                 (only if images exist but OCR libs missing)
"""

import re
import os
import json
import hashlib
import tempfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# =========================================================
# EDIT THESE (you provided)
# =========================================================
MSG_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus"
OUT_DIR = r"C:\Users\SHTANDO\OneDrive - Mercedes-Benz (corpdir.onmicrosoft.com)\DWT_MP_RM1 - Dokumente\Project Chatbot\Available data\Mails Rasmus\_exports"

# =========================================================
# Behavior knobs
# =========================================================
DROP_HEADER_FIELDS = True
MSG_ATTACHMENT_DEPTH_LIMIT = 2

ATTACH_MAX_FILES = 12
ATTACH_MAX_CHARS_PER_FILE = 8000
ATTACH_MAX_TOTAL_CHARS = 20000
BODY_MAX_CHARS = 50000

# Which input file types to consider
INPUT_EXTS = {".msg", ".html", ".htm"}

# =========================================================
# Patterns
# =========================================================
EMAIL_RE = re.compile(r"\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b", re.I)
URL_RE = re.compile(r"\bhttps?://[^\s<>()]+\b", re.I)
WWW_RE = re.compile(r"\bwww\.[^\s<>()]+\b", re.I)
DOMAIN_RE = re.compile(r"\b(?:[A-Z0-9-]+\.)+(?:[A-Z]{2,})\b", re.I)

PHONE_RE = re.compile(r"(?:(?:\+|00)\d{1,3}[\s\-]?)?(?:\(?\d{2,5}\)?[\s\-]?)?\d[\d\s\-]{6,}\d")
IBAN_RE = re.compile(r"\b[A-Z]{2}\d{2}(?:\s?[A-Z0-9]{4}){3,7}\b", re.I)
MONEY_RE = re.compile(r"(?i)\b(?:EUR|USD|GBP|CHF)\s*\d[\d\.\,\s]*\b|\b\d[\d\.\,\s]*\s*(?:€|EUR|USD|GBP|CHF)\b")

ADDRESS_RE = re.compile(
    r"\b([A-ZÄÖÜ][a-zäöüß]+(?:\s[A-ZÄÖÜ][a-zäöüß]+){0,3})\s"
    r"(Straße|Strasse|Str\.|Weg|Allee|Platz|Ring|Gasse|Damm|Ufer)\s"
    r"\d{1,5}[a-zA-Z]?\b"
)

REF_RE = re.compile(
    r"\b(?:PO|PR|NCR|Ticket|Case|Req|Request|Material|Part|Project|SP|ID|Ref)\s*[:#]?\s*"
    r"[A-Za-z0-9\-_/]*\d[A-Za-z0-9\-_/]*\b",
    re.I,
)

TITLE_NAME_RE = re.compile(
    r"\b(?:Mr|Mrs|Ms|Miss|Dr|Prof|Herr|Frau)\.?\s+[A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2}\b"
)
GREET_NAME_RE = re.compile(
    r"\b(?:Hallo|Hi|Hello|Guten\s+Tag|Sehr\s+geehrte[rsn]?)\s+([A-ZÄÖÜ][a-zäöüß]+(?:\s+[A-ZÄÖÜ][a-zäöüß]+){0,2})\b",
    re.I,
)
NAME_HEURISTIC_RE = re.compile(r"\b[A-ZÄÖÜ][a-zäöüß]{2,}\s+[A-ZÄÖÜ][a-zäöüß]{2,}\b")

COMPANY_SUFFIX_RE = re.compile(
    r"\b[A-ZÄÖÜ0-9][A-Za-zÄÖÜäöüß0-9&\-\.\s]{2,60}\s(?:GmbH|AG|KG|SE|S\.?A\.?|S\.?r\.?l\.?|Ltd\.?|Inc\.?|LLC|BV|NV)\b"
)
SUPPLIER_LABEL_RE = re.compile(r"(?i)\b(?:Supplier|Lieferant|Vendor|Company|Firma)\s*[:\-]\s*([A-Z0-9ÄÖÜ][^\n\r]{2,80})")

QUOTE_BLOCK_RE = re.compile(
    r"^Von:\s.*$|^From:\s.*$|^Gesendet:\s.*$|^Sent:\s.*$|^An:\s.*$|^To:\s.*$|^Cc:\s.*$|"
    r"^Betreff:\s.*$|^Subject:\s.*$|^-----Original Message-----.*$",
    re.I | re.M
)

SIGNATURE_CUTOFF_RE = re.compile(
    r"(?im)^\s*(Mit\s+freundlichen\s+Gr[üu]ßen|Best\s+regards|Kind\s+regards|"
    r"Viele\s+Gr[üu]ße|Regards|Thanks|Thank\s+you|"
    r"Telefon|Tel\.|Mobil|Mobile|E-?Mail|mailto)\b.*$"
)

# =========================================================
# Helpers
# =========================================================
def clean_ws(text: str) -> str:
    if not text:
        return ""
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def strip_quoted_history(text: str) -> str:
    m = QUOTE_BLOCK_RE.search(text)
    return text[:m.start()].strip() if m else text

def strip_signature(text: str) -> str:
    lines = text.split("\n")
    if len(lines) < 6:
        return text
    for i in range(len(lines) - 1, max(-1, len(lines) - 120), -1):
        if SIGNATURE_CUTOFF_RE.match(lines[i] or ""):
            return "\n".join(lines[:i]).strip()
    return text

def safe_stem(name: str, max_len: int = 80) -> str:
    stem = re.sub(r"[^A-Za-z0-9_\-]+", "_", name)[:max_len]
    return stem or "file"

def stable_token(prefix: str, value: str, salt: str = "v1") -> str:
    h = hashlib.sha256((salt + "||" + value).encode("utf-8", errors="ignore")).hexdigest()[:10]
    return f"[{prefix}_{h}]"

def html_escape(s: str) -> str:
    return (s or "").replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

def text_to_simple_html(title: str, meta: Dict[str, str], body: str, stats: Dict[str, int]) -> str:
    meta_rows = []
    for k, v in meta.items():
        if v:
            meta_rows.append(f"<tr><td><b>{html_escape(k)}</b></td><td>{html_escape(v)}</td></tr>")
    meta_tbl = "<table>" + "".join(meta_rows) + "</table>" if meta_rows else ""
    stats_str = html_escape(json.dumps(stats, ensure_ascii=False))

    lines = body.split("\n")
    html_parts = []
    buf = []
    for ln in lines:
        if ln.strip() == "":
            if buf:
                html_parts.append("<p>" + "<br>".join(html_escape(x) for x in buf) + "</p>")
                buf = []
            continue
        buf.append(ln)
    if buf:
        html_parts.append("<p>" + "<br>".join(html_escape(x) for x in buf) + "</p>")

    return f"""<html>
<head><meta charset="utf-8"><title>{html_escape(title)}</title></head>
<body style="font-family:Segoe UI, Arial; font-size:13px; line-height:1.4; padding:16px;">
<h3>{html_escape(title)}</h3>
{meta_tbl}
<hr>
{''.join(html_parts)}
<hr>
<p><b>RedactionStats:</b> {stats_str}</p>
</body>
</html>
"""

# =========================================================
# HTML -> text (robust)
# =========================================================
def html_to_text(html: str) -> str:
    if not html:
        return ""
    # Prefer BeautifulSoup if available
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")

        # remove script/style
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        txt = soup.get_text("\n")
        return clean_ws(txt)
    except Exception:
        # fallback: remove tags
        txt = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html)
        txt = re.sub(r"(?s)<[^>]+>", " ", txt)
        txt = re.sub(r"&nbsp;", " ", txt)
        txt = re.sub(r"&amp;", "&", txt)
        txt = re.sub(r"&lt;", "<", txt)
        txt = re.sub(r"&gt;", ">", txt)
        return clean_ws(txt)

def read_html_file(path: Path, errors_log: Path) -> str:
    try:
        raw = path.read_text(encoding="utf-8", errors="ignore")
        return html_to_text(raw)
    except Exception as e:
        with errors_log.open("a", encoding="utf-8") as ef:
            ef.write(f"{path}\nHTML read failed: {e}\n\n")
        return ""

# =========================================================
# Entity mapping (stable pseudonyms across runs)
# =========================================================
class EntityMapper:
    def __init__(self, map_path: Path):
        self.map_path = map_path
        self.map = {}
        if map_path.exists():
            try:
                self.map = json.loads(map_path.read_text(encoding="utf-8"))
            except Exception:
                self.map = {}

    def get(self, etype: str, raw: str) -> str:
        key = f"{etype}|{raw}"
        if key in self.map:
            return self.map[key]
        tok = stable_token(etype.upper(), raw)
        self.map[key] = tok
        return tok

    def save(self):
        self.map_path.write_text(json.dumps(self.map, indent=2, ensure_ascii=False), encoding="utf-8")

def merge_stats(a: Dict[str, int], b: Dict[str, int]) -> Dict[str, int]:
    out = dict(a)
    for k, v in b.items():
        out[k] = int(out.get(k, 0)) + int(v)
    return out

# =========================================================
# Sanitization
# =========================================================
def sanitize_text(text: str, mapper: EntityMapper) -> Tuple[str, Dict[str, int]]:
    stats = {
        "emails": 0, "phones": 0, "urls": 0, "domains": 0, "iban": 0,
        "money": 0, "refs": 0, "addresses": 0, "names": 0, "companies": 0
    }
    if not text:
        return "", stats

    t = text

    def rep_company(m):
        stats["companies"] += 1
        return mapper.get("company", m.group(0).strip())
    t = COMPANY_SUFFIX_RE.sub(rep_company, t)

    def rep_supplier_label(m):
        raw = (m.group(1) or "").strip()
        raw2 = re.split(r"[;,/|]", raw)[0].strip()
        if not raw2:
            return m.group(0)
        stats["companies"] += 1
        return re.sub(re.escape(raw), mapper.get("company", raw2), m.group(0), flags=re.I)
    t = SUPPLIER_LABEL_RE.sub(rep_supplier_label, t)

    def rep_title_name(m):
        stats["names"] += 1
        return mapper.get("name", m.group(0).strip())
    t = TITLE_NAME_RE.sub(rep_title_name, t)

    def rep_greet(m):
        raw = (m.group(1) or "").strip()
        stats["names"] += 1
        return m.group(0).replace(raw, mapper.get("name", raw))
    t = GREET_NAME_RE.sub(rep_greet, t)

    def rep_name_heur(m):
        stats["names"] += 1
        return mapper.get("name", m.group(0).strip())
    t = NAME_HEURISTIC_RE.sub(rep_name_heur, t)

    def rep_email(m):
        stats["emails"] += 1
        return mapper.get("email", m.group(0).lower())
    t = EMAIL_RE.sub(rep_email, t)

    def rep_url(_m):
        stats["urls"] += 1
        return "[URL]"
    t = URL_RE.sub(rep_url, t)
    t = WWW_RE.sub(rep_url, t)

    def rep_domain(_m):
        stats["domains"] += 1
        return "[DOMAIN]"
    t = DOMAIN_RE.sub(rep_domain, t)

    def rep_phone(_m):
        stats["phones"] += 1
        return "[PHONE]"
    t = PHONE_RE.sub(rep_phone, t)

    def rep_iban(_m):
        stats["iban"] += 1
        return "[IBAN]"
    t = IBAN_RE.sub(rep_iban, t)

    def rep_money(_m):
        stats["money"] += 1
        return "[AMOUNT]"
    t = MONEY_RE.sub(rep_money, t)

    def rep_ref(m):
        stats["refs"] += 1
        return mapper.get("ref", m.group(0).strip())
    t = REF_RE.sub(rep_ref, t)

    def rep_addr(_m):
        stats["addresses"] += 1
        return "[ADDRESS]"
    t = ADDRESS_RE.sub(rep_addr, t)

    return clean_ws(t), stats

# =========================================================
# Attachment extractors (skip if libs missing)
# =========================================================
def extract_pdf(path: Path) -> str:
    try:
        from pypdf import PdfReader
    except Exception:
        return ""
    try:
        out = []
        r = PdfReader(str(path))
        for pg in r.pages:
            try:
                out.append(pg.extract_text() or "")
            except Exception:
                continue
        return clean_ws("\n".join(out))
    except Exception:
        return ""

def extract_docx(path: Path) -> str:
    try:
        from docx import Document
    except Exception:
        return ""
    try:
        doc = Document(str(path))
        parts = []
        for p in doc.paragraphs:
            if p.text and p.text.strip():
                parts.append(p.text.strip())
        for table in doc.tables:
            for row in table.rows:
                cells = [c.text.strip() for c in row.cells]
                if any(cells):
                    parts.append(" | ".join(cells))
        return clean_ws("\n".join(parts))
    except Exception:
        return ""

def extract_xlsx(path: Path) -> str:
    try:
        import openpyxl
    except Exception:
        return ""
    try:
        wb = openpyxl.load_workbook(str(path), data_only=True, read_only=True)
        out = []
        for ws in wb.worksheets:
            out.append(f"Sheet: {ws.title}")
            max_rows = min(ws.max_row or 0, 200)
            max_cols = min(ws.max_column or 0, 30)
            for r in range(1, max_rows + 1):
                row_vals = []
                for c in range(1, max_cols + 1):
                    v = ws.cell(row=r, column=c).value
                    row_vals.append("" if v is None else str(v))
                if any(str(x).strip() for x in row_vals):
                    out.append(" | ".join(clean_ws(str(x)) for x in row_vals))
        return clean_ws("\n".join(out))
    except Exception:
        return ""

def extract_txt(path: Path) -> str:
    try:
        return clean_ws(path.read_text(encoding="utf-8", errors="ignore"))
    except Exception:
        return ""

def extract_image_ocr(path: Path, ocr_missing_log: Path) -> str:
    try:
        from PIL import Image
        import pytesseract
    except Exception:
        with ocr_missing_log.open("a", encoding="utf-8") as f:
            f.write(f"OCR unavailable (missing libs) for: {path}\n")
        return ""
    try:
        img = Image.open(str(path))
        txt = pytesseract.image_to_string(img)
        return clean_ws(txt)
    except Exception:
        return ""

def extract_attachment_text(path: Path, ocr_missing_log: Path) -> str:
    ext = path.suffix.lower()
    if ext == ".pdf":
        return extract_pdf(path)
    if ext == ".docx":
        return extract_docx(path)
    if ext in (".xlsx", ".xlsm"):
        return extract_xlsx(path)
    if ext == ".txt":
        return extract_txt(path)
    if ext in (".png", ".jpg", ".jpeg"):
        return extract_image_ocr(path, ocr_missing_log)
    if ext in (".html", ".htm"):
        # sometimes attachments are html too
        return read_html_file(path, Path(OUT_DIR) / "errors.log")
    return ""

# =========================================================
# MSG parsing via extract_msg (skip if missing)
# =========================================================
def parse_msg(path: Path, tmp_root: Path, errors_log: Path) -> Tuple[Dict[str, str], List[Path]]:
    tmp_dir = tmp_root / f"msg_{hashlib.sha1(str(path).encode('utf-8', errors='ignore')).hexdigest()[:10]}"
    tmp_dir.mkdir(parents=True, exist_ok=True)

    try:
        import extract_msg
    except Exception as e:
        with errors_log.open("a", encoding="utf-8") as ef:
            ef.write(f"{path}\nextract_msg missing: {e}\n\n")
        return {}, []

    try:
        msg = extract_msg.Message(str(path))
        msg.process()

        fields = {
            "subject": (msg.subject or "").strip(),
            "date": (msg.date or "").strip(),
            "from": (msg.sender or "").strip(),
            "to": (msg.to or "").strip(),
            "cc": (msg.cc or "").strip(),
            "body": (msg.body or "").strip(),
        }

        saved = []
        try:
            atts = msg.attachments or []
            for a in atts:
                try:
                    a.save(customPath=str(tmp_dir))
                except Exception:
                    continue
            for p in tmp_dir.glob("*"):
                if p.is_file():
                    saved.append(p)
        except Exception:
            pass

        return fields, saved

    except Exception as e:
        with errors_log.open("a", encoding="utf-8") as ef:
            ef.write(f"{path}\nextract_msg failed: {e}\n\n")
        return {}, []

def extract_attachment_texts(
    att_paths: List[Path],
    tmp_root: Path,
    ocr_missing_log: Path,
    errors_log: Path,
    depth: int = 0,
) -> List[Tuple[str, str]]:
    out: List[Tuple[str, str]] = []
    if not att_paths:
        return out

    total_chars = 0
    file_count = 0

    for ap in att_paths:
        if file_count >= ATTACH_MAX_FILES or total_chars >= ATTACH_MAX_TOTAL_CHARS:
            break
        if not ap.exists():
            continue

        ext = ap.suffix.lower()
        name = ap.name

        if ext == ".msg" and depth < MSG_ATTACHMENT_DEPTH_LIMIT:
            nested_fields, nested_atts = parse_msg(ap, tmp_root, errors_log)
            nested_body = clean_ws(nested_fields.get("body", ""))[:ATTACH_MAX_CHARS_PER_FILE]
            if nested_body:
                out.append((f"AttachedEmail:{name}", nested_body))
                total_chars += len(nested_body)
                file_count += 1
            nested_more = extract_attachment_texts(nested_atts, tmp_root, ocr_missing_log, errors_log, depth + 1)
            for n, t in nested_more:
                if file_count >= ATTACH_MAX_FILES or total_chars >= ATTACH_MAX_TOTAL_CHARS:
                    break
                if t:
                    t2 = t[:ATTACH_MAX_CHARS_PER_FILE]
                    out.append((n, t2))
                    total_chars += len(t2)
                    file_count += 1
            continue

        txt = extract_attachment_text(ap, ocr_missing_log)
        if not txt:
            continue

        txt = txt[:ATTACH_MAX_CHARS_PER_FILE]
        if total_chars + len(txt) > ATTACH_MAX_TOTAL_CHARS:
            txt = txt[: max(0, ATTACH_MAX_TOTAL_CHARS - total_chars)]
        if txt.strip():
            out.append((name, txt))
            total_chars += len(txt)
            file_count += 1

    return out

# =========================================================
# Per-input processor (MSG or HTML)
# =========================================================
def sanitize_one_input(
    input_path: Path,
    mapper: EntityMapper,
    tmp_root: Path,
    errors_log: Path,
    ocr_missing_log: Path,
) -> Dict:
    ext = input_path.suffix.lower()

    # Common record fields
    subject = ""
    date = ""
    frm = ""
    to = ""
    cc = ""
    body = ""
    attachments_extracted: List[str] = []

    if ext == ".msg":
        fields, att_paths = parse_msg(input_path, tmp_root, errors_log)
        if not fields:
            return {
                "source_file": input_path.name,
                "source_type": "msg",
                "subject": "",
                "date": "",
                "from": "",
                "to": "",
                "cc": "",
                "text": "",
                "stats": {},
                "attachments_extracted": [],
                "status": "parse_failed",
            }

        subject = clean_ws(fields.get("subject", ""))
        date = clean_ws(fields.get("date", ""))
        frm = clean_ws(fields.get("from", ""))
        to = clean_ws(fields.get("to", ""))
        cc = clean_ws(fields.get("cc", ""))

        body = clean_ws(fields.get("body", ""))[:BODY_MAX_CHARS]
        body = strip_quoted_history(body)
        body = strip_signature(body)

        att_texts = extract_attachment_texts(att_paths, tmp_root, ocr_missing_log, errors_log, depth=0)
        attachments_extracted = [n for n, _ in att_texts]

        combined = ["=== MAIN MESSAGE ===", body]
        if att_texts:
            combined += ["", "=== ATTACHMENTS (EXTRACTED TEXT) ==="]
            for n, t in att_texts:
                combined += [f"[Attachment: {n}]", t, ""]

        combined_text = clean_ws("\n".join(combined))

    elif ext in (".html", ".htm"):
        # Treat HTML as email-like content
        raw_text = read_html_file(input_path, errors_log)
        if not raw_text.strip():
            return {
                "source_file": input_path.name,
                "source_type": "html",
                "subject": "",
                "date": "",
                "from": "",
                "to": "",
                "cc": "",
                "text": "",
                "stats": {},
                "attachments_extracted": [],
                "status": "empty_or_unreadable",
            }

        # crude header extraction if present
        # (optional: many exports have "Subject:" lines)
        subj_match = re.search(r"(?im)^\s*Subject:\s*(.+)$", raw_text)
        if subj_match:
            subject = clean_ws(subj_match.group(1))

        date_match = re.search(r"(?im)^\s*Date:\s*(.+)$", raw_text)
        if date_match:
            date = clean_ws(date_match.group(1))

        body = raw_text[:BODY_MAX_CHARS]
        body = strip_quoted_history(body)
        body = strip_signature(body)
        combined_text = clean_ws(body)

    else:
        return {
            "source_file": input_path.name,
            "source_type": ext.lstrip("."),
            "subject": "",
            "date": "",
            "from": "",
            "to": "",
            "cc": "",
            "text": "",
            "stats": {},
            "attachments_extracted": [],
            "status": "unsupported_input_ext",
        }

    # sanitize
    s_subj, st1 = sanitize_text(subject, mapper)
    s_text, st2 = sanitize_text(combined_text, mapper)
    stats = merge_stats(st1, st2)

    if DROP_HEADER_FIELDS:
        s_from = s_to = s_cc = ""
    else:
        s_from, stf = sanitize_text(frm, mapper)
        s_to, stt = sanitize_text(to, mapper)
        s_cc, stc = sanitize_text(cc, mapper)
        stats = merge_stats(stats, merge_stats(stf, merge_stats(stt, stc)))

    return {
        "source_file": input_path.name,
        "source_type": "msg" if ext == ".msg" else "html",
        "subject": s_subj,
        "date": date,
        "from": s_from,
        "to": s_to,
        "cc": s_cc,
        "text": s_text,
        "stats": stats,
        "attachments_extracted": attachments_extracted,
        "status": "ok",
    }

# =========================================================
# Runner
# =========================================================
def run():
    in_path = Path(MSG_DIR)
    out_path = Path(OUT_DIR)
    out_text_dir = out_path / "sanitized_text"
    out_html_dir = out_path / "sanitized_html"
    out_jsonl_dir = out_path / "sanitized_jsonl"
    out_text_dir.mkdir(parents=True, exist_ok=True)
    out_html_dir.mkdir(parents=True, exist_ok=True)
    out_jsonl_dir.mkdir(parents=True, exist_ok=True)

    errors_log = out_path / "errors.log"
    ocr_missing_log = out_path / "ocr_missing.log"
    map_path = out_path / "entity_map.json"
    mapper = EntityMapper(map_path)

    tmp_root = Path(tempfile.gettempdir()) / "msg_sanitize_tmp"
    tmp_root.mkdir(parents=True, exist_ok=True)

    # Scan inputs: .msg/.MSG + .html/.htm
    candidates: List[Path] = []
    for p in in_path.rglob("*"):
        if not p.is_file():
            continue
        ext = p.suffix.lower()
        if ext in INPUT_EXTS:
            candidates.append(p)
        elif p.suffix == ".MSG":
            candidates.append(p)

    candidates = sorted(set(candidates))

    if not candidates:
        raise SystemExit(f"No input files found under: {MSG_DIR} (expected .msg/.MSG/.html/.htm)")

    jsonl_path = out_jsonl_dir / "dataset.jsonl"

    written_ok = 0
    written_failed = 0

    with jsonl_path.open("w", encoding="utf-8") as jf:
        for p in candidates:
            try:
                rec = sanitize_one_input(p, mapper, tmp_root, errors_log, ocr_missing_log)

                stem = safe_stem(p.stem, 80)
                h = hashlib.sha1(str(p).encode("utf-8", errors="ignore")).hexdigest()[:8]
                base_name = f"{stem}__{h}"

                if rec.get("status") == "ok" and rec.get("text", "").strip():
                    meta = {
                        "Source": rec["source_file"],
                        "SourceType": rec["source_type"],
                        "Subject": rec["subject"],
                        "Date": rec["date"],
                        "AttachmentsExtracted": ", ".join(rec["attachments_extracted"]) if rec["attachments_extracted"] else "",
                    }
                    if not DROP_HEADER_FIELDS:
                        meta["From"] = rec.get("from", "")
                        meta["To"] = rec.get("to", "")
                        meta["CC"] = rec.get("cc", "")

                    txt_out = out_text_dir / f"{base_name}.txt"
                    html_out = out_html_dir / f"{base_name}.html"

                    txt_out.write_text(
                        f"Source: {rec['source_file']}\n"
                        f"SourceType: {rec['source_type']}\n"
                        f"Subject: {rec['subject']}\n"
                        f"Date: {rec['date']}\n"
                        f"AttachmentsExtracted: {', '.join(rec['attachments_extracted']) if rec['attachments_extracted'] else ''}\n\n"
                        f"{rec['text']}\n\n"
                        f"RedactionStats: {json.dumps(rec['stats'], ensure_ascii=False)}\n",
                        encoding="utf-8",
                    )

                    html_out.write_text(
                        text_to_simple_html(
                            title=rec["subject"] or rec["source_file"],
                            meta=meta,
                            body=rec["text"],
                            stats=rec["stats"],
                        ),
                        encoding="utf-8",
                    )

                    written_ok += 1
                else:
                    written_failed += 1

                jf.write(json.dumps(rec, ensure_ascii=False) + "\n")

            except Exception as e:
                written_failed += 1
                with errors_log.open("a", encoding="utf-8") as ef:
                    ef.write(f"{p}\nUNHANDLED ERROR: {e}\n\n")

    mapper.save()
    print(f"Done. OK={written_ok}, Failed={written_failed}")
    print(f"Outputs in: {out_path.resolve()}")
    print(f"- TXT:  {out_text_dir.resolve()}")
    print(f"- HTML: {out_html_dir.resolve()}")
    print(f"- JSONL:{jsonl_path.resolve()}")
    print(f"- Map:  {map_path.resolve()}")
    if ocr_missing_log.exists():
        print(f"- OCR log: {ocr_missing_log.resolve()}")
    print(f"- Errors: {errors_log.resolve()}")

if __name__ == "__main__":
    run()
